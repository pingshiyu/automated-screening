{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Predicting DG Errors\n",
    "\n",
    "Through the use of topological indices, we try to predict the errors of DG algorithms. Previously we have already generated the DG errors for our molecules, and saved them in $\\texttt{./output/}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dg_functions import chemspace_root, chemspace_paths\n",
    "from indices import all_indices\n",
    "from dg_processing import LogTransformer, describe_losses, avg_corr, plot_losses_together, feasibility_vector, classification_metrics\n",
    "from dg_processing import evaluate_classifier, evaluate_classifier_and_get_results, cross_validate_classifier\n",
    "\n",
    "# standard functions for data analysis\n",
    "import numpy as np, pandas as pd\n",
    "import scipy.stats\n",
    "import pickle\n",
    "import time, logging\n",
    "import pprint as pp\n",
    "from functools import reduce\n",
    "\n",
    "# Chemistry libraries \n",
    "from rdkit import Chem\n",
    "from rdkit.Chem import Draw\n",
    "from openbabel import openbabel as ob, pybel\n",
    "\n",
    "# Simple ML libraries\n",
    "import sklearn\n",
    "from sklearn import linear_model, metrics, tree, ensemble, neural_network, neighbors, svm\n",
    "from sklearn import pipeline, decomposition\n",
    "from sklearn import model_selection\n",
    "from sklearn.inspection import permutation_importance\n",
    "# enable use of ensemble.HistGradientBoosting{Classifier, Regressor}\n",
    "from sklearn.experimental import enable_hist_gradient_boosting  # noqa\n",
    "from sklearn.utils.fixes import loguniform # grid search\n",
    "\n",
    "# plotting\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib # for LaTeX display\n",
    "import graphviz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ignore sklearn warnings for now\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore') \n",
    "pd.set_option('display.max_colwidth', -1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "logging.basicConfig(filename='logs/dg-error-prediction-angles.log',\n",
    "                    level=logging.DEBUG,\n",
    "                    format='%(asctime)s %(levelname)s: %(message)s')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "chemspace_names = ['c6h6', '125_56k', '125_338k']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Loading Chemical Spaces"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "chemspace = {}\n",
    "for name in chemspace_names:\n",
    "    chemspace[name] = {'mols': Chem.SmilesMolSupplier(chemspace_paths[name], delimiter='\\t')}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Loading Conformation / Indices Data\n",
    "\n",
    "The conformations (from DG) and topological indices of the molecules has been calculated, we load them up:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "570 features are included, here are the first few: \n",
      "['MaxEStateIndex', 'MinEStateIndex', 'MaxAbsEStateIndex', 'MinAbsEStateIndex', 'qed', 'MolWt', 'HeavyAtomMolWt', 'ExactMolWt', 'NumValenceElectrons', 'NumRadicalElectrons']\n"
     ]
    }
   ],
   "source": [
    "losses_names = ['length_loss', 'angle_loss', 'energy_loss', 'mmff_loss']\n",
    "for name in chemspace_names:\n",
    "    # unpickle the saved indices and conformation data\n",
    "    with open(f'./output/descriptors_and_maccs/{name}_descriptors', 'rb') as f:        \n",
    "        feature_names, features, invalids = pickle.load(f)\n",
    "        chemspace[name]['features'] = pd.DataFrame(features[:, 1:], \n",
    "                                                   index=features[:, 0], columns=feature_names)\n",
    "        \n",
    "    with open(f'./output/avg_distance_error_bonds_avg_angle_error_by_degree_uff_mmff/{name}', 'rb') as f:\n",
    "        losses, reading_failed, conformation_failed = pickle.load(f)\n",
    "        chemspace[name]['dg_loss'] = losses\n",
    "        chemspace[name]['failed_conformation'] = conformation_failed\n",
    "        \n",
    "    # molecules that either could not be read when computing indices, or in conformations\n",
    "    chemspace[name]['invalid'] = list(set().union(invalids, reading_failed))\n",
    "    \n",
    "    # Form the losses datafame (with values np.nan or the loss for every molecule)\n",
    "    df_losses = pd.DataFrame(chemspace[name]['dg_loss'][:, 1:], \n",
    "                             index=chemspace[name]['dg_loss'][:, 0], columns=losses_names)\n",
    "    df_failures = pd.DataFrame(np.nan, index=set().union(chemspace[name]['failed_conformation'], chemspace[name]['invalid']),\n",
    "                               columns=losses_names)\n",
    "    chemspace[name]['dg_loss'] = pd.concat([df_losses, df_failures], axis=0).sort_index()\n",
    "    \n",
    "    # clear out memory\n",
    "    del chemspace[name]['failed_conformation']\n",
    "    del chemspace[name]['invalid']\n",
    "\n",
    "print(f'{len(feature_names)} features are included, here are the first few: ')\n",
    "print(feature_names[:10]) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "def nan_columns(df):\n",
    "    '''\n",
    "    Find columns of `df` where there are `NaN` entries\n",
    "    '''\n",
    "    nan_loc_row, nan_loc_col = np.where(df.isnull())\n",
    "    cols_with_nan = [df.columns[i] for i in set(nan_loc_col)]\n",
    "    return cols_with_nan"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "def constant_columns(df):\n",
    "    '''\n",
    "    Find columns of `df` where all entries are constant\n",
    "    '''\n",
    "    constant_indices = df.columns[(df == df.iloc[0]).all()]\n",
    "    return list(constant_indices)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "def collinear_columns(df):\n",
    "    '''\n",
    "    Find columns of `df` with high collinearity\n",
    "    '''\n",
    "    correlation = df.corr()\n",
    "    current_columns = df.columns\n",
    "\n",
    "    correlated_xid, correlated_yid = np.where((correlation < -0.95) | (correlation > 0.95))\n",
    "    correlated_pairs = [[row, col] for row, col in zip(correlated_xid, correlated_yid)\n",
    "                       if row < col]\n",
    "    # Remove one of the correlated pairs from the matrix:\n",
    "    correlated_indices_to_remove = []\n",
    "    while correlated_pairs:\n",
    "        index_a_id, index_b_id = correlated_pairs.pop()\n",
    "\n",
    "        # the id removed in the one that is more correlated on average\n",
    "        removed_id = index_a_id if avg_corr(index_a_id, correlation) > avg_corr(index_b_id, correlation) else index_b_id\n",
    "        correlated_pairs = [[row, col] for row, col in correlated_pairs\n",
    "                           if (row != removed_id) and (col != removed_id)]\n",
    "        correlated_indices_to_remove.append(current_columns[removed_id])\n",
    "        \n",
    "    return correlated_indices_to_remove"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cleanup of columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Clean-up of features:\n",
      "68 indices dropped due to nan values: ['AUTOCORR2D_136', 'AUTOCORR2D_137', 'MaxPartialCharge', 'MinPartialCharge', 'MaxAbsPartialCharge', 'MinAbsPartialCharge', 'AUTOCORR2D_144', 'AUTOCORR2D_145', 'AUTOCORR2D_146', 'AUTOCORR2D_147', 'AUTOCORR2D_152', 'AUTOCORR2D_153', 'AUTOCORR2D_154', 'AUTOCORR2D_155', 'AUTOCORR2D_148', 'AUTOCORR2D_156', 'AUTOCORR2D_149', 'AUTOCORR2D_157', 'AUTOCORR2D_160', 'AUTOCORR2D_161', 'AUTOCORR2D_162', 'AUTOCORR2D_163', 'AUTOCORR2D_164', 'AUTOCORR2D_165', 'AUTOCORR2D_168', 'AUTOCORR2D_169', 'AUTOCORR2D_170', 'AUTOCORR2D_171', 'AUTOCORR2D_172', 'AUTOCORR2D_173', 'AUTOCORR2D_176', 'AUTOCORR2D_177', 'AUTOCORR2D_178', 'AUTOCORR2D_179', 'AUTOCORR2D_180', 'AUTOCORR2D_181', 'AUTOCORR2D_184', 'AUTOCORR2D_185', 'AUTOCORR2D_96', 'AUTOCORR2D_97', 'AUTOCORR2D_98', 'AUTOCORR2D_99', 'AUTOCORR2D_100', 'AUTOCORR2D_101', 'AUTOCORR2D_104', 'AUTOCORR2D_105', 'AUTOCORR2D_106', 'AUTOCORR2D_107', 'AUTOCORR2D_108', 'AUTOCORR2D_109', 'AUTOCORR2D_112', 'AUTOCORR2D_113', 'AUTOCORR2D_114', 'AUTOCORR2D_115', 'AUTOCORR2D_116', 'AUTOCORR2D_117', 'AUTOCORR2D_120', 'AUTOCORR2D_121', 'AUTOCORR2D_122', 'AUTOCORR2D_123', 'AUTOCORR2D_124', 'AUTOCORR2D_125', 'AUTOCORR2D_128', 'AUTOCORR2D_129', 'AUTOCORR2D_130', 'AUTOCORR2D_131', 'AUTOCORR2D_132', 'AUTOCORR2D_133']\n",
      "45 indices dropped due to all constant: ['NumRadicalElectrons', 'SMR_VSA8', 'SlogP_VSA7', 'SlogP_VSA9', 'EState_VSA11', 'fr_alkyl_halide', 'fr_azide', 'fr_barbitur', 'fr_benzodiazepine', 'fr_diazo', 'fr_halogen', 'fr_nitro', 'fr_nitro_arom', 'fr_nitro_arom_nonortho', 'fr_phos_acid', 'fr_phos_ester', 'fr_prisulfonamd', 'fr_quatN', 'fr_sulfonamd', 'fr_sulfone', 'MACCS_0', 'MACCS_1', 'MACCS_2', 'MACCS_3', 'MACCS_4', 'MACCS_5', 'MACCS_6', 'MACCS_7', 'MACCS_9', 'MACCS_10', 'MACCS_12', 'MACCS_18', 'MACCS_20', 'MACCS_27', 'MACCS_31', 'MACCS_35', 'MACCS_42', 'MACCS_44', 'MACCS_46', 'MACCS_49', 'MACCS_87', 'MACCS_103', 'MACCS_107', 'MACCS_134', 'MACCS_166']\n",
      "61 indices dropped due to high correlation: ['AUTOCORR2D_175', 'AUTOCORR2D_174', 'AUTOCORR2D_167', 'AUTOCORR2D_158', 'AUTOCORR2D_151', 'AUTOCORR2D_150', 'AUTOCORR2D_119', 'AUTOCORR2D_118', 'AUTOCORR2D_127', 'AUTOCORR2D_110', 'AUTOCORR2D_103', 'AUTOCORR2D_102', 'AUTOCORR2D_79', 'AUTOCORR2D_62', 'AUTOCORR2D_61', 'AUTOCORR2D_55', 'AUTOCORR2D_95', 'AUTOCORR2D_23', 'AUTOCORR2D_22', 'AUTOCORR2D_21', 'AUTOCORR2D_20', 'AUTOCORR2D_19', 'AUTOCORR2D_34', 'AUTOCORR2D_33', 'AUTOCORR2D_32', 'AUTOCORR2D_39', 'AUTOCORR2D_15', 'AUTOCORR2D_38', 'AUTOCORR2D_14', 'AUTOCORR2D_37', 'AUTOCORR2D_13', 'AUTOCORR2D_36', 'AUTOCORR2D_12', 'AUTOCORR2D_35', 'AUTOCORR2D_11', 'AUTOCORR2D_26', 'MACCS_143', 'crowding_3', 'NumLipinskiHBA', 'fr_phenol_noOrthoHbond', 'MACCS_63', 'fr_nitrile', 'fr_ketone', 'NumAmideBonds', 'fr_NH2', 'MACCS_154', 'fr_Nhpyrrole', 'fr_COO', 'NumHBD', 'NumHBA', 'NumAromaticHeterocycles', 'NumAliphaticRings', 'NOCount', 'NumLipinskiHBD', 'crowding_4', 'Chi3v', 'Chi3n', 'Chi1v', 'Chi1', 'ExactMolWt', 'MaxAbsEStateIndex']\n",
      "Additional 6 columns dropped due to nan values: ['AUTOCORR2D_166', 'AUTOCORR2D_182', 'AUTOCORR2D_186', 'AUTOCORR2D_126', 'AUTOCORR2D_134', 'AUTOCORR2D_138']\n"
     ]
    }
   ],
   "source": [
    "print('Clean-up of features:')\n",
    "# Find indices to remove using the smaller df (faster execution)\n",
    "features = chemspace['125_56k']['features']\n",
    "indices_to_drop = {}\n",
    "\n",
    "# Some of the input data gives NaNs, we will need to remove these:\n",
    "cols_with_nan = nan_columns(features)\n",
    "features.drop(cols_with_nan, axis=1, inplace=True)\n",
    "indices_to_drop['nan values'] = cols_with_nan\n",
    "\n",
    "# Find constant entries\n",
    "constant_cols = constant_columns(features)\n",
    "features.drop(constant_cols, axis=1, inplace=True)\n",
    "indices_to_drop['all constant'] = constant_cols\n",
    "\n",
    "# Remove collinear indices\n",
    "collinear_cols = collinear_columns(features)\n",
    "features.drop(collinear_cols, axis=1, inplace=True)\n",
    "indices_to_drop['high correlation'] = collinear_cols\n",
    "\n",
    "for reason, indices in indices_to_drop.items():\n",
    "    print(f'{len(indices)} indices dropped due to {reason}: {indices}')\n",
    "\n",
    "# free up memory\n",
    "#del chemspace['125_56k']['features']\n",
    "\n",
    "all_indices_to_drop = [index for index_list in indices_to_drop.values() for index in index_list]\n",
    "chemspace['125_338k']['features'].drop(all_indices_to_drop, axis=1, inplace=True)\n",
    "\n",
    "# missed out additional columns with NaN valuesnot present in '125_56k'\n",
    "extra_nan_cols = nan_columns(chemspace['125_338k']['features'])\n",
    "all_indices_to_drop += extra_nan_cols\n",
    "chemspace['125_338k']['features'].drop(extra_nan_cols, axis=1, inplace=True)\n",
    "print(f'Additional {len(extra_nan_cols)} columns dropped due to nan values: {extra_nan_cols[:10]}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feasibility based on: Unrealistic losses values ($\\geq 99\\%$ of realistic molecule's loss values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "length_loss     0.011046\n",
       "angle_loss      1.530038\n",
       "energy_loss    10.557267\n",
       "dtype: float64"
      ]
     },
     "execution_count": 149,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cutoffs = pd.DataFrame(columns=losses_cols)\n",
    "for real_db in ['pubchem_45k', 'chembl_50k']:#, 'gdb_50k']:\n",
    "    real_losses = chemspace[real_db]['df'][losses_cols].astype(np.float64)\n",
    "    percentiles = real_losses.describe(percentiles=[0.99]).T\n",
    "    cutoffs.loc[real_db] = percentiles['99%']\n",
    "transformed_cutoffs = losses_scaler.transform(cutoffs).max(axis=0)\n",
    "transformed_cutoffs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Form training set\n",
    "indices = chemspace['125_338k']['features'].columns\n",
    "\n",
    "features_scaler = sklearn.preprocessing.StandardScaler()\n",
    "X = features_scaler.fit_transform(chemspace['125_338k']['features'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_training_data(chemspace_name, features_scaler=features_scaler):\n",
    "    y = {}\n",
    "    for loss_name, cutoff in zip(['length_loss', 'angle_loss', 'energy_loss'], \n",
    "                                 [0.011, 1.530, 10.557]):\n",
    "        y[loss_name] = feasibility_vector(chemspace[chemspace_name]['dg_loss'][loss_name], cutoff, loss_name)\n",
    "        print(f'{loss_name} targets successfully computed.', '\\n')\n",
    "\n",
    "    # Transforming features: standardize to mean 0 and variance 1\n",
    "    X = features_scaler.transform(chemspace[chemspace_name]['features'][indices])\n",
    "    print('Features successfully computed', '\\n')\n",
    "    \n",
    "    return X, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Taking length_loss of >= 0.011 as 'impossible'\n",
      "Unconstructible molecules:\n",
      "152773 / 337924\n",
      "length_loss targets successfully computed. \n",
      "\n",
      "Taking angle_loss of >= 1.53 as 'impossible'\n",
      "Unconstructible molecules:\n",
      "321592 / 337924\n",
      "angle_loss targets successfully computed. \n",
      "\n",
      "Taking energy_loss of >= 10.557 as 'impossible'\n",
      "Unconstructible molecules:\n",
      "276149 / 337924\n",
      "energy_loss targets successfully computed. \n",
      "\n",
      "Features successfully computed \n",
      "\n"
     ]
    }
   ],
   "source": [
    "X, y = prepare_training_data('125_338k', features_scaler=features_scaler)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hyperparameter Search and Feature Importance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_search_result(cv_results):\n",
    "    '''\n",
    "    Get the search results as a dataframe, sorted from best to worst. \n",
    "    Criterion for best is based on the mean rankings by the metrics\n",
    "    '''\n",
    "    search_result = pd.DataFrame.from_dict(cv_results)\n",
    "    search_result['overall_rank'] = np.mean(\n",
    "        search_result[[f'rank_test_{metric_name}' \n",
    "                       for metric_name in ['neg_log_loss', 'f1']]],\n",
    "        axis=1)\n",
    "    search_result.sort_values('overall_rank', inplace=True)\n",
    "    \n",
    "    columns = search_result.columns\n",
    "    return search_result[\n",
    "        [col for col in columns \n",
    "         if not (col.startswith('param_') or col.startswith('split') or col.startswith('std'))]\n",
    "    ]\n",
    "    #return search_result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Energy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Preparing energy_loss training/test split:\n",
      "Train size: 304131; test size: 33793\n"
     ]
    }
   ],
   "source": [
    "# Preparing train / test split\n",
    "loss_target = 'energy_loss'\n",
    "print(f'Preparing {loss_target} training/test split:')\n",
    "X_train, X_test, y_train, y_test = sklearn.model_selection.train_test_split(\n",
    "    X, y[loss_target], test_size=0.1)\n",
    "print(f'Train size: {len(y_train)}; test size: {len(y_test)}')\n",
    "\n",
    "loss_type = 'energy'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cross Validation of Classifiers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true,
     "source_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logistic Regression: \n",
      "Cross validation results:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done   2 out of   5 | elapsed:  5.0min remaining:  7.5min\n",
      "[Parallel(n_jobs=-1)]: Done   5 out of   5 | elapsed:  5.1min finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fit_time = 297.4447111606598 +- 3.9559887882203304\n",
      "score_time = 0.24663004875183106 +- 0.13941925984074965\n",
      "test_accuracy = 0.9490625107275825 +- 0.001660026708084527\n",
      "test_adjusted balanced accuracy = 0.8126745468262655 +- 0.007148706202862203\n",
      "test_balanced accuracy = 0.9063372734131327 +- 0.0035743531014311015\n",
      "test_average precision = 0.7652983705247094 +- 0.008899843726503789\n",
      "test_f1 score = 0.8576058904256921 +- 0.005727318175649599\n",
      "test_precision = 0.8770762529784338 +- 0.005630912735092377\n",
      "test_recall = 0.8389838775997782 +- 0.006523868489248887\n",
      "test_roc = 0.9063372734131327 +- 0.0035743531014310855\n",
      "\n",
      "Neural Network: \n",
      "Cross validation results:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done   2 out of   5 | elapsed: 12.7min remaining: 19.0min\n",
      "[Parallel(n_jobs=-1)]: Done   5 out of   5 | elapsed: 15.4min finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fit_time = 830.6275831222534 +- 151.81012869248397\n",
      "score_time = 0.9630998134613037 +- 0.3938083498267851\n",
      "test_accuracy = 0.9695789595008447 +- 0.0009682390420815513\n",
      "test_adjusted balanced accuracy = 0.895942569575511 +- 0.007439885523266306\n",
      "test_balanced accuracy = 0.9479712847877556 +- 0.003719942761633153\n",
      "test_average precision = 0.8558448550020721 +- 0.005259355373113142\n",
      "test_f1 score = 0.9165620628346502 +- 0.0034502052386232063\n",
      "test_precision = 0.9192499961943682 +- 0.0047979617351773885\n",
      "test_recall = 0.9139082856457801 +- 0.008287652326287281\n",
      "test_roc = 0.9479712847877556 +- 0.0037199427616332625\n",
      "\n",
      "Decision Tree: \n",
      "Cross validation results:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done   2 out of   5 | elapsed:   40.9s remaining:  1.0min\n",
      "[Parallel(n_jobs=-1)]: Done   5 out of   5 | elapsed:   43.2s finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fit_time = 37.47785935401917 +- 3.878303047253668\n",
      "score_time = 0.2823976516723633 +- 0.16228525305455152\n",
      "test_accuracy = 0.9507137677690803 +- 0.0035709293468180786\n",
      "test_adjusted balanced accuracy = 0.8224869997539175 +- 0.01481829611437419\n",
      "test_balanced accuracy = 0.9112434998769586 +- 0.007409148057187095\n",
      "test_average precision = 0.7725856928512793 +- 0.01640025163249854\n",
      "test_f1 score = 0.8629873653746799 +- 0.010829197608027008\n",
      "test_precision = 0.8774304492525301 +- 0.008892637177855022\n",
      "test_recall = 0.8490206737672381 +- 0.013453877044503784\n",
      "test_roc = 0.9112434998769589 +- 0.0074091480571870615\n",
      "\n",
      "Gradient Boosting 3000: \n",
      "Cross validation results:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done   2 out of   5 | elapsed: 25.7min remaining: 38.6min\n",
      "[Parallel(n_jobs=-1)]: Done   5 out of   5 | elapsed: 28.4min finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fit_time = 1606.8534162998199 +- 153.8325312004414\n",
      "score_time = 21.48797788619995 +- 2.6780727346352684\n",
      "test_accuracy = 0.9751689724313911 +- 0.00040904748459531743\n",
      "test_adjusted balanced accuracy = 0.9067933391706671 +- 0.005005487216887269\n",
      "test_balanced accuracy = 0.9533966695853335 +- 0.0025027436084436344\n",
      "test_average precision = 0.8820917295756061 +- 0.0013264959892595803\n",
      "test_f1 score = 0.931200737466933 +- 0.0012309543132608869\n",
      "test_precision = 0.9436699123313339 +- 0.004041575474678304\n",
      "test_recall = 0.9190695905319373 +- 0.005906407469234006\n",
      "test_roc = 0.9533966695853335 +- 0.00250274360844368\n",
      "\n",
      "Gradient Boosting 500: \n",
      "Cross validation results:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done   2 out of   5 | elapsed:  8.7min remaining: 13.1min\n",
      "[Parallel(n_jobs=-1)]: Done   5 out of   5 | elapsed:  8.8min finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fit_time = 509.78159537315366 +- 7.214901481104408\n",
      "score_time = 9.852004098892213 +- 0.8520186440522108\n",
      "test_accuracy = 0.9718191065795934 +- 0.000874460714363475\n",
      "test_adjusted balanced accuracy = 0.8951034828944099 +- 0.004801007516231324\n",
      "test_balanced accuracy = 0.947551741447205 +- 0.002400503758115662\n",
      "test_average precision = 0.8665979570926188 +- 0.0038944014116432664\n",
      "test_f1 score = 0.921870219009581 +- 0.0025328931713427907\n",
      "test_precision = 0.9348067445082238 +- 0.002627820401496439\n",
      "test_recall = 0.9092919066000995 +- 0.004934766107887724\n",
      "test_roc = 0.947551741447205 +- 0.002400503758115714\n",
      "\n",
      "Random Forest: \n",
      "Cross validation results:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done   2 out of   5 | elapsed: 10.5min remaining: 15.7min\n",
      "[Parallel(n_jobs=-1)]: Done   5 out of   5 | elapsed: 10.6min finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fit_time = 619.7168300151825 +- 5.388052753774512\n",
      "score_time = 4.42344560623169 +- 3.386041613407836\n",
      "test_accuracy = 0.9496957900131653 +- 0.0010069946528544922\n",
      "test_adjusted balanced accuracy = 0.8532248741635687 +- 0.003961980295172675\n",
      "test_balanced accuracy = 0.9266124370817843 +- 0.0019809901475863374\n",
      "test_average precision = 0.7708487638353146 +- 0.005466790357619534\n",
      "test_f1 score = 0.8661543457754803 +- 0.0033746717519746593\n",
      "test_precision = 0.8433580124538382 +- 0.004493886774100975\n",
      "test_recall = 0.8902207444116291 +- 0.003633419376902971\n",
      "test_roc = 0.9266124370817843 +- 0.001980990147586471\n",
      "\n"
     ]
    }
   ],
   "source": [
    "loss_target = 'energy_loss'\n",
    "energy_models = {\n",
    "    'Logistic Regression': linear_model.LogisticRegression(C=10.0, class_weight=None, max_iter=1000, verbose=1),\n",
    "    'Neural Network': neural_network.MLPClassifier(alpha=1e-2,\n",
    "                                  hidden_layer_sizes=(150,),\n",
    "                                  max_iter=1000,\n",
    "                                  learning_rate_init=1e-4,\n",
    "                                  early_stopping=True, \n",
    "                                  n_iter_no_change=25,\n",
    "                                  verbose=True),\n",
    "    'Decision Tree': tree.DecisionTreeClassifier(max_depth=None, min_samples_leaf=50, min_samples_split=20,\n",
    "                                       splitter='random', criterion='entropy', class_weight=None),\n",
    "    'Gradient Boosting 3000': ensemble.HistGradientBoostingClassifier(max_iter=3000, learning_rate=0.10, \n",
    "                                                  min_samples_leaf=50,\n",
    "                                                  warm_start=False, n_iter_no_change=100),\n",
    "    'Gradient Boosting 500': ensemble.HistGradientBoostingClassifier(max_iter=500, learning_rate=0.10, \n",
    "                                                  min_samples_leaf=50,\n",
    "                                                  warm_start=False, n_iter_no_change=100),\n",
    "    'Random Forest': ensemble.RandomForestClassifier(n_estimators=300, criterion='entropy', min_samples_split=20, min_samples_leaf=20,\n",
    "                                      class_weight={True: 2, False: 1}, max_depth=None,\n",
    "                                      n_jobs=-1)\n",
    "}\n",
    "\n",
    "for name, model in energy_models.items():\n",
    "    print(f'{name}: ')\n",
    "    cross_validate_classifier(model, X, y[loss_target], cv=sklearn.model_selection.KFold(n_splits=5, shuffle=True, random_state=42), verbose=5, n_jobs=-1)\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true,
     "source_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 3 folds for each of 10 candidates, totalling 30 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done   2 tasks      | elapsed:  1.9min\n",
      "[Parallel(n_jobs=-1)]: Done  22 out of  30 | elapsed:  9.6min remaining:  3.5min\n",
      "[Parallel(n_jobs=-1)]: Done  30 out of  30 | elapsed: 12.3min finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LRC grid search saved\n"
     ]
    }
   ],
   "source": [
    "lrc_search = model_selection.RandomizedSearchCV(\n",
    "    linear_model.LogisticRegression(), \n",
    "    param_distributions={\n",
    "        'C': [1e-2, 1e-1, 1e0, 1e1],\n",
    "        'class_weight': [{True: 2, False: 1}, {True: 3, False: 1}, 'balanced', None],\n",
    "        'max_iter': [1000]\n",
    "    }, \n",
    "    n_jobs=-1,\n",
    "    refit='f1',\n",
    "    scoring=['neg_log_loss', 'f1', 'roc_auc', 'balanced_accuracy'],\n",
    "    n_iter=10,\n",
    "    cv=3,\n",
    "    verbose=5)\n",
    "lrc_search.fit(X_train, y_train)\n",
    "\n",
    "with open(f'./model_gridsearch/{loss_type}/lrc', 'wb') as f:\n",
    "    pickle.dump(lrc_search, f)\n",
    "    print('LRC grid search saved')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true,
     "source_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "[Parallel(n_jobs=1)]: Done   1 out of   1 | elapsed:  2.0min finished\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "LogisticRegression(C=10.0, max_iter=1000, verbose=1)"
      ]
     },
     "execution_count": 136,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lrc = linear_model.LogisticRegression(C=10.0, class_weight=None, max_iter=1000, verbose=1)\n",
    "lrc.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true,
     "source_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing results:\n",
      "accuracy = 0.9501080105347262\n",
      "adjusted balanced accuracy = 0.8158375839190684\n",
      "balanced accuracy = 0.9079187919595342\n",
      "average precision = 0.7692192253082388\n",
      "f1 score = 0.8601990049751244\n",
      "precision = 0.8797489823609227\n",
      "recall = 0.8414990266061\n",
      "roc = 0.9079187919595341\n",
      "confusion matrix\n",
      " = [[26920   709]\n",
      " [  977  5187]]\n"
     ]
    }
   ],
   "source": [
    "evaluate_classifier(lrc, X_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Neural Networks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true,
     "source_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 3 folds for each of 10 candidates, totalling 30 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done   2 tasks      | elapsed:  2.8min\n",
      "[Parallel(n_jobs=-1)]: Done  22 out of  30 | elapsed: 16.9min remaining:  6.1min\n",
      "[Parallel(n_jobs=-1)]: Done  30 out of  30 | elapsed: 22.6min finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NN grid search saved\n"
     ]
    }
   ],
   "source": [
    "nn_search = model_selection.RandomizedSearchCV(\n",
    "    neural_network.MLPClassifier(), \n",
    "    param_distributions={\n",
    "        'alpha': [1e-5, 1e-4, 1e-3, 1e-2],\n",
    "        'hidden_layer_sizes': [(100,), (150,), (200,)],\n",
    "        'learning_rate_init': [1e-4, 1e-3, 1e-2, 1e-1],\n",
    "        'max_iter': [1000],\n",
    "        'early_stopping': [True],\n",
    "        'n_iter_no_change': [5, 25]\n",
    "    },\n",
    "    n_jobs=-1,\n",
    "    refit='f1',\n",
    "    scoring=['neg_log_loss', 'f1', 'roc_auc'],\n",
    "    n_iter=10,\n",
    "    cv=3,\n",
    "    verbose=5)\n",
    "nn_search.fit(X_train, y_train)\n",
    "\n",
    "with open(f'./model_gridsearch/{loss_tyoe}/nn', 'wb') as f:\n",
    "    pickle.dump(nn_search, f)\n",
    "    print('NN grid search saved')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true,
     "source_hidden": true
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'alpha': 0.01,\n",
       " 'early_stopping': True,\n",
       " 'hidden_layer_sizes': (150,),\n",
       " 'learning_rate_init': 0.0001,\n",
       " 'max_iter': 1000,\n",
       " 'n_iter_no_change': 25}"
      ]
     },
     "execution_count": 109,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_search_result(nn_search.cv_results_).iloc[0].params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NN grid search loaded\n"
     ]
    }
   ],
   "source": [
    "with open(f'./model_gridsearch/{loss_type}/nn', 'rb') as f:\n",
    "    nn_energy_search = pickle.load(f)\n",
    "    print('NN grid search loaded')\n",
    "nn_energy = nn_energy_search.best_estimator_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true,
     "source_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 1, loss = 0.14151039\n",
      "Validation score: 0.955054\n",
      "Iteration 2, loss = 0.09914545\n",
      "Validation score: 0.960479\n",
      "Iteration 3, loss = 0.08721028\n",
      "Validation score: 0.962780\n",
      "Iteration 4, loss = 0.07936347\n",
      "Validation score: 0.966233\n",
      "Iteration 5, loss = 0.07394030\n",
      "Validation score: 0.966430\n",
      "Iteration 6, loss = 0.06911645\n",
      "Validation score: 0.968041\n",
      "Iteration 7, loss = 0.06545619\n",
      "Validation score: 0.968534\n",
      "Iteration 8, loss = 0.06222842\n",
      "Validation score: 0.968633\n",
      "Iteration 9, loss = 0.05897553\n",
      "Validation score: 0.967515\n",
      "Iteration 10, loss = 0.05644990\n",
      "Validation score: 0.968041\n",
      "Iteration 11, loss = 0.05449498\n",
      "Validation score: 0.971000\n",
      "Iteration 12, loss = 0.05221023\n",
      "Validation score: 0.968764\n",
      "Iteration 13, loss = 0.05064890\n",
      "Validation score: 0.969882\n",
      "Iteration 14, loss = 0.04833035\n",
      "Validation score: 0.969553\n",
      "Iteration 15, loss = 0.04712255\n",
      "Validation score: 0.970375\n",
      "Iteration 16, loss = 0.04564293\n",
      "Validation score: 0.970277\n",
      "Iteration 17, loss = 0.04364766\n",
      "Validation score: 0.968732\n",
      "Iteration 18, loss = 0.04248003\n",
      "Validation score: 0.968896\n",
      "Iteration 19, loss = 0.04087342\n",
      "Validation score: 0.971066\n",
      "Iteration 20, loss = 0.04008150\n",
      "Validation score: 0.969882\n",
      "Iteration 21, loss = 0.03885928\n",
      "Validation score: 0.969619\n",
      "Iteration 22, loss = 0.03805882\n",
      "Validation score: 0.969488\n",
      "Iteration 23, loss = 0.03721817\n",
      "Validation score: 0.969126\n",
      "Iteration 24, loss = 0.03604549\n",
      "Validation score: 0.970869\n",
      "Iteration 25, loss = 0.03545963\n",
      "Validation score: 0.969290\n",
      "Iteration 26, loss = 0.03425734\n",
      "Validation score: 0.969981\n",
      "Iteration 27, loss = 0.03332855\n",
      "Validation score: 0.969685\n",
      "Iteration 28, loss = 0.03305671\n",
      "Validation score: 0.969422\n",
      "Iteration 29, loss = 0.03168684\n",
      "Validation score: 0.969718\n",
      "Iteration 30, loss = 0.03081008\n",
      "Validation score: 0.969619\n",
      "Iteration 31, loss = 0.03018221\n",
      "Validation score: 0.969849\n",
      "Iteration 32, loss = 0.02964020\n",
      "Validation score: 0.970474\n",
      "Iteration 33, loss = 0.02870995\n",
      "Validation score: 0.969586\n",
      "Iteration 34, loss = 0.02830602\n",
      "Validation score: 0.970441\n",
      "Iteration 35, loss = 0.02846787\n",
      "Validation score: 0.970080\n",
      "Iteration 36, loss = 0.02691410\n",
      "Validation score: 0.971066\n",
      "Iteration 37, loss = 0.02748125\n",
      "Validation score: 0.969126\n",
      "Validation score did not improve more than tol=0.000100 for 25 consecutive epochs. Stopping.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "MLPClassifier(alpha=0.00235, early_stopping=True, hidden_layer_sizes=(150,),\n",
       "              learning_rate_init=0.000512, max_iter=1000, n_iter_no_change=25,\n",
       "              verbose=True)"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# alpha and hidden layer sizes chosen from parameter search\n",
    "nn = neural_network.MLPClassifier(alpha=1e-2,\n",
    "                                  hidden_layer_sizes=(150,),\n",
    "                                  max_iter=1000,\n",
    "                                  learning_rate_init=1e-4,\n",
    "                                  early_stopping=True, \n",
    "                                  n_iter_no_change=25,\n",
    "                                  verbose=True)\n",
    "nn.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true,
     "source_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing results:\n",
      "accuracy = 0.9709111354422514\n",
      "adjusted balanced accuracy = 0.9003516845784589\n",
      "balanced accuracy = 0.9501758422892295\n",
      "average precision = 0.8607165293927408\n",
      "f1 score = 0.9196567225173682\n",
      "precision = 0.9216906946264745\n",
      "recall = 0.9176317077148916\n",
      "roc = 0.9501758422892296\n",
      "confusion matrix\n",
      " = [[27184   478]\n",
      " [  505  5626]]\n"
     ]
    }
   ],
   "source": [
    "evaluate_classifier(nn, X_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Decision Trees"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true,
     "source_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 3 folds for each of 10 candidates, totalling 30 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done   2 tasks      | elapsed:  1.1min\n",
      "[Parallel(n_jobs=-1)]: Done  22 out of  30 | elapsed:  4.2min remaining:  1.5min\n",
      "[Parallel(n_jobs=-1)]: Done  30 out of  30 | elapsed:  4.8min finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DT grid search saved\n"
     ]
    }
   ],
   "source": [
    "tree_search = model_selection.RandomizedSearchCV(\n",
    "    tree.DecisionTreeClassifier(), \n",
    "    param_distributions={\n",
    "        'criterion': ['gini', 'entropy'],\n",
    "        'splitter': ['random', 'best'],\n",
    "        'class_weight': [{True: 2, False: 1}, {True: 3, False: 1}, 'balanced', None],\n",
    "        'max_depth': [None],\n",
    "        'min_samples_leaf': [20, 50],\n",
    "        'min_samples_split': [20, 50]\n",
    "    },\n",
    "    n_jobs=-1,\n",
    "    cv=3,\n",
    "    refit='f1',\n",
    "    scoring=['neg_log_loss', 'f1', 'roc_auc'],\n",
    "    verbose=5)\n",
    "tree_search.fit(X_train, y_train)\n",
    "\n",
    "with open(f'./model_gridsearch/{loss_type}/dt', 'wb') as f:\n",
    "    pickle.dump(tree_search, f)\n",
    "    print('DT grid search saved')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true,
     "source_hidden": true
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'class_weight': None,\n",
       " 'criterion': 'entropy',\n",
       " 'max_depth': None,\n",
       " 'min_samples_leaf': 50,\n",
       " 'min_samples_split': 20,\n",
       " 'splitter': 'random'}"
      ]
     },
     "execution_count": 111,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_search_result(tree_search.cv_results_).iloc[0].params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true,
     "source_hidden": true
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DecisionTreeClassifier(criterion='entropy', min_samples_leaf=50,\n",
       "                       min_samples_split=20, splitter='random')"
      ]
     },
     "execution_count": 141,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tree_clf = tree.DecisionTreeClassifier(max_depth=None, min_samples_leaf=50, min_samples_split=20,\n",
    "                                       splitter='random', criterion='entropy', class_weight=None)\n",
    "tree_clf.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true,
     "source_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing results:\n",
      "accuracy = 0.9522386293019265\n",
      "adjusted balanced accuracy = 0.8287786953437863\n",
      "balanced accuracy = 0.9143893476718932\n",
      "average precision = 0.7786477912303768\n",
      "f1 score = 0.8671823568136933\n",
      "precision = 0.8799265197060788\n",
      "recall = 0.8548020765736535\n",
      "roc = 0.9143893476718932\n",
      "confusion matrix\n",
      " = [[26910   719]\n",
      " [  895  5269]]\n"
     ]
    }
   ],
   "source": [
    "evaluate_classifier(tree_clf, X_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Histogram-Based Boosting (Similar to LightGBM)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true,
     "source_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 3 folds for each of 10 candidates, totalling 30 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done   2 tasks      | elapsed: 12.2min\n",
      "[Parallel(n_jobs=-1)]: Done  22 out of  30 | elapsed: 80.3min remaining: 29.2min\n",
      "[Parallel(n_jobs=-1)]: Done  30 out of  30 | elapsed: 94.3min finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "HGB grid search saved\n"
     ]
    }
   ],
   "source": [
    "hgb_search = model_selection.RandomizedSearchCV(\n",
    "    ensemble.HistGradientBoostingClassifier(), \n",
    "    param_distributions={\n",
    "        'max_iter': [500, 1000, 1500, 3000],\n",
    "        'learning_rate': [0.05, 0.10],\n",
    "        'min_samples_leaf': [20, 50],\n",
    "        'warm_start': [False],\n",
    "        'n_iter_no_change': [100]\n",
    "    }, \n",
    "    n_jobs=-1,\n",
    "    cv=3,\n",
    "    refit='f1',\n",
    "    scoring=['neg_log_loss', 'f1', 'roc_auc'],\n",
    "    verbose=5)\n",
    "hgb_search.fit(X_train, y_train)\n",
    "\n",
    "with open(f'./model_gridsearch/{loss_type}/hgb', 'wb') as f:\n",
    "    pickle.dump(hgb_search, f)\n",
    "    print('HGB grid search saved')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(f'./model_gridsearch/{loss_type}/hgb', 'rb') as f:\n",
    "    hgb_energy_search = pickle.load(f)\n",
    "hgb_energy = hgb_energy_search.best_estimator_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true,
     "source_hidden": true
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>mean_fit_time</th>\n",
       "      <th>mean_score_time</th>\n",
       "      <th>params</th>\n",
       "      <th>mean_test_neg_log_loss</th>\n",
       "      <th>rank_test_neg_log_loss</th>\n",
       "      <th>mean_test_f1</th>\n",
       "      <th>rank_test_f1</th>\n",
       "      <th>mean_test_roc_auc</th>\n",
       "      <th>rank_test_roc_auc</th>\n",
       "      <th>overall_rank</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>7</td>\n",
       "      <td>3325.554394</td>\n",
       "      <td>261.517975</td>\n",
       "      <td>{'early_stopping': False, 'learning_rate': 0.05, 'max_iter': 3000, 'min_samples_leaf': 50, 'warm_start': False}</td>\n",
       "      <td>-0.064393</td>\n",
       "      <td>1</td>\n",
       "      <td>0.928377</td>\n",
       "      <td>3</td>\n",
       "      <td>0.995732</td>\n",
       "      <td>3</td>\n",
       "      <td>2.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>3292.730738</td>\n",
       "      <td>249.641262</td>\n",
       "      <td>{'early_stopping': False, 'learning_rate': 0.05, 'max_iter': 3000, 'min_samples_leaf': 20, 'warm_start': False}</td>\n",
       "      <td>-0.064403</td>\n",
       "      <td>2</td>\n",
       "      <td>0.927869</td>\n",
       "      <td>4</td>\n",
       "      <td>0.995731</td>\n",
       "      <td>4</td>\n",
       "      <td>3.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>12</td>\n",
       "      <td>1673.665161</td>\n",
       "      <td>141.176981</td>\n",
       "      <td>{'early_stopping': False, 'learning_rate': 0.1, 'max_iter': 1500, 'min_samples_leaf': 20, 'warm_start': False}</td>\n",
       "      <td>-0.065238</td>\n",
       "      <td>3</td>\n",
       "      <td>0.927384</td>\n",
       "      <td>6</td>\n",
       "      <td>0.995630</td>\n",
       "      <td>6</td>\n",
       "      <td>4.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>13</td>\n",
       "      <td>1683.157192</td>\n",
       "      <td>129.600402</td>\n",
       "      <td>{'early_stopping': False, 'learning_rate': 0.1, 'max_iter': 1500, 'min_samples_leaf': 50, 'warm_start': False}</td>\n",
       "      <td>-0.065260</td>\n",
       "      <td>4</td>\n",
       "      <td>0.927471</td>\n",
       "      <td>5</td>\n",
       "      <td>0.995634</td>\n",
       "      <td>5</td>\n",
       "      <td>4.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>14</td>\n",
       "      <td>2829.303608</td>\n",
       "      <td>196.002665</td>\n",
       "      <td>{'early_stopping': False, 'learning_rate': 0.1, 'max_iter': 3000, 'min_samples_leaf': 20, 'warm_start': False}</td>\n",
       "      <td>-0.069720</td>\n",
       "      <td>9</td>\n",
       "      <td>0.930241</td>\n",
       "      <td>2</td>\n",
       "      <td>0.995921</td>\n",
       "      <td>2</td>\n",
       "      <td>5.5</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    mean_fit_time  mean_score_time  \\\n",
       "7   3325.554394    261.517975        \n",
       "6   3292.730738    249.641262        \n",
       "12  1673.665161    141.176981        \n",
       "13  1683.157192    129.600402        \n",
       "14  2829.303608    196.002665        \n",
       "\n",
       "                                                                                                             params  \\\n",
       "7   {'early_stopping': False, 'learning_rate': 0.05, 'max_iter': 3000, 'min_samples_leaf': 50, 'warm_start': False}   \n",
       "6   {'early_stopping': False, 'learning_rate': 0.05, 'max_iter': 3000, 'min_samples_leaf': 20, 'warm_start': False}   \n",
       "12  {'early_stopping': False, 'learning_rate': 0.1, 'max_iter': 1500, 'min_samples_leaf': 20, 'warm_start': False}    \n",
       "13  {'early_stopping': False, 'learning_rate': 0.1, 'max_iter': 1500, 'min_samples_leaf': 50, 'warm_start': False}    \n",
       "14  {'early_stopping': False, 'learning_rate': 0.1, 'max_iter': 3000, 'min_samples_leaf': 20, 'warm_start': False}    \n",
       "\n",
       "    mean_test_neg_log_loss  rank_test_neg_log_loss  mean_test_f1  \\\n",
       "7  -0.064393                1                       0.928377       \n",
       "6  -0.064403                2                       0.927869       \n",
       "12 -0.065238                3                       0.927384       \n",
       "13 -0.065260                4                       0.927471       \n",
       "14 -0.069720                9                       0.930241       \n",
       "\n",
       "    rank_test_f1  mean_test_roc_auc  rank_test_roc_auc  overall_rank  \n",
       "7   3             0.995732           3                  2.0           \n",
       "6   4             0.995731           4                  3.0           \n",
       "12  6             0.995630           6                  4.5           \n",
       "13  5             0.995634           5                  4.5           \n",
       "14  2             0.995921           2                  5.5           "
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_search_result(hgb_search.cv_results_).head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true,
     "source_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Binning 0.854 GB of training data: 6.281 s\n",
      "Binning 0.095 GB of validation data: 0.083 s\n",
      "Fitting gradient boosted rounds:\n",
      "[1/3000] 1 tree, 31 leaves, max depth = 8, train loss: 0.41460, val loss: 0.41473, in 0.239s\n",
      "[2/3000] 1 tree, 31 leaves, max depth = 7, train loss: 0.37210, val loss: 0.37234, in 0.239s\n",
      "[3/3000] 1 tree, 31 leaves, max depth = 7, train loss: 0.33954, val loss: 0.33980, in 0.267s\n",
      "[4/3000] 1 tree, 31 leaves, max depth = 8, train loss: 0.31343, val loss: 0.31384, in 0.250s\n",
      "[5/3000] 1 tree, 31 leaves, max depth = 8, train loss: 0.29132, val loss: 0.29166, in 0.275s\n",
      "[6/3000] 1 tree, 31 leaves, max depth = 7, train loss: 0.27307, val loss: 0.27346, in 0.268s\n",
      "[7/3000] 1 tree, 31 leaves, max depth = 8, train loss: 0.25710, val loss: 0.25763, in 0.264s\n",
      "[8/3000] 1 tree, 31 leaves, max depth = 10, train loss: 0.24315, val loss: 0.24382, in 0.276s\n",
      "[9/3000] 1 tree, 31 leaves, max depth = 8, train loss: 0.23113, val loss: 0.23180, in 0.268s\n",
      "[10/3000] 1 tree, 31 leaves, max depth = 8, train loss: 0.22064, val loss: 0.22143, in 0.247s\n",
      "[11/3000] 1 tree, 31 leaves, max depth = 8, train loss: 0.21078, val loss: 0.21157, in 0.360s\n",
      "[12/3000] 1 tree, 31 leaves, max depth = 7, train loss: 0.20242, val loss: 0.20326, in 0.302s\n",
      "[13/3000] 1 tree, 31 leaves, max depth = 8, train loss: 0.19421, val loss: 0.19506, in 0.277s\n",
      "[14/3000] 1 tree, 31 leaves, max depth = 9, train loss: 0.18705, val loss: 0.18784, in 0.317s\n",
      "[15/3000] 1 tree, 31 leaves, max depth = 7, train loss: 0.18087, val loss: 0.18167, in 0.269s\n",
      "[16/3000] 1 tree, 31 leaves, max depth = 8, train loss: 0.17538, val loss: 0.17628, in 0.300s\n",
      "[17/3000] 1 tree, 31 leaves, max depth = 9, train loss: 0.17020, val loss: 0.17103, in 0.302s\n",
      "[18/3000] 1 tree, 31 leaves, max depth = 8, train loss: 0.16558, val loss: 0.16634, in 0.344s\n",
      "[19/3000] 1 tree, 31 leaves, max depth = 8, train loss: 0.16131, val loss: 0.16214, in 0.298s\n",
      "[20/3000] 1 tree, 31 leaves, max depth = 10, train loss: 0.15740, val loss: 0.15824, in 0.301s\n",
      "[21/3000] 1 tree, 31 leaves, max depth = 11, train loss: 0.15365, val loss: 0.15459, in 0.316s\n",
      "[22/3000] 1 tree, 31 leaves, max depth = 9, train loss: 0.15020, val loss: 0.15129, in 0.255s\n",
      "[23/3000] 1 tree, 31 leaves, max depth = 8, train loss: 0.14738, val loss: 0.14855, in 0.285s\n",
      "[24/3000] 1 tree, 31 leaves, max depth = 9, train loss: 0.14467, val loss: 0.14594, in 0.301s\n",
      "[25/3000] 1 tree, 31 leaves, max depth = 8, train loss: 0.14223, val loss: 0.14351, in 0.284s\n",
      "[26/3000] 1 tree, 31 leaves, max depth = 9, train loss: 0.13940, val loss: 0.14070, in 0.293s\n",
      "[27/3000] 1 tree, 31 leaves, max depth = 9, train loss: 0.13690, val loss: 0.13830, in 0.292s\n",
      "[28/3000] 1 tree, 31 leaves, max depth = 9, train loss: 0.13491, val loss: 0.13632, in 0.270s\n",
      "[29/3000] 1 tree, 31 leaves, max depth = 8, train loss: 0.13305, val loss: 0.13446, in 0.296s\n",
      "[30/3000] 1 tree, 31 leaves, max depth = 11, train loss: 0.13150, val loss: 0.13294, in 0.280s\n",
      "[31/3000] 1 tree, 31 leaves, max depth = 9, train loss: 0.12993, val loss: 0.13147, in 0.535s\n",
      "[32/3000] 1 tree, 31 leaves, max depth = 11, train loss: 0.12809, val loss: 0.12967, in 0.333s\n",
      "[33/3000] 1 tree, 31 leaves, max depth = 11, train loss: 0.12672, val loss: 0.12831, in 0.305s\n",
      "[34/3000] 1 tree, 31 leaves, max depth = 9, train loss: 0.12545, val loss: 0.12706, in 0.346s\n",
      "[35/3000] 1 tree, 31 leaves, max depth = 13, train loss: 0.12414, val loss: 0.12582, in 0.310s\n",
      "[36/3000] 1 tree, 31 leaves, max depth = 9, train loss: 0.12296, val loss: 0.12469, in 0.292s\n",
      "[37/3000] 1 tree, 31 leaves, max depth = 9, train loss: 0.12184, val loss: 0.12364, in 0.299s\n",
      "[38/3000] 1 tree, 31 leaves, max depth = 10, train loss: 0.12062, val loss: 0.12252, in 0.279s\n",
      "[39/3000] 1 tree, 31 leaves, max depth = 9, train loss: 0.11959, val loss: 0.12160, in 0.300s\n",
      "[40/3000] 1 tree, 31 leaves, max depth = 9, train loss: 0.11862, val loss: 0.12068, in 0.281s\n",
      "[41/3000] 1 tree, 31 leaves, max depth = 10, train loss: 0.11755, val loss: 0.11972, in 0.279s\n",
      "[42/3000] 1 tree, 31 leaves, max depth = 12, train loss: 0.11642, val loss: 0.11858, in 0.294s\n",
      "[43/3000] 1 tree, 31 leaves, max depth = 9, train loss: 0.11549, val loss: 0.11774, in 0.295s\n",
      "[44/3000] 1 tree, 31 leaves, max depth = 8, train loss: 0.11473, val loss: 0.11705, in 0.300s\n",
      "[45/3000] 1 tree, 31 leaves, max depth = 11, train loss: 0.11395, val loss: 0.11628, in 0.288s\n",
      "[46/3000] 1 tree, 31 leaves, max depth = 11, train loss: 0.11303, val loss: 0.11537, in 0.289s\n",
      "[47/3000] 1 tree, 31 leaves, max depth = 10, train loss: 0.11227, val loss: 0.11467, in 0.321s\n",
      "[48/3000] 1 tree, 31 leaves, max depth = 10, train loss: 0.11165, val loss: 0.11409, in 0.286s\n",
      "[49/3000] 1 tree, 31 leaves, max depth = 10, train loss: 0.11098, val loss: 0.11359, in 0.282s\n",
      "[50/3000] 1 tree, 31 leaves, max depth = 12, train loss: 0.11027, val loss: 0.11286, in 0.434s\n",
      "[51/3000] 1 tree, 31 leaves, max depth = 9, train loss: 0.10949, val loss: 0.11207, in 0.318s\n",
      "[52/3000] 1 tree, 31 leaves, max depth = 12, train loss: 0.10891, val loss: 0.11155, in 0.344s\n",
      "[53/3000] 1 tree, 31 leaves, max depth = 10, train loss: 0.10819, val loss: 0.11088, in 0.325s\n",
      "[54/3000] 1 tree, 31 leaves, max depth = 11, train loss: 0.10756, val loss: 0.11033, in 0.278s\n",
      "[55/3000] 1 tree, 31 leaves, max depth = 9, train loss: 0.10683, val loss: 0.10969, in 0.333s\n",
      "[56/3000] 1 tree, 31 leaves, max depth = 8, train loss: 0.10626, val loss: 0.10921, in 0.287s\n",
      "[57/3000] 1 tree, 31 leaves, max depth = 8, train loss: 0.10577, val loss: 0.10889, in 0.299s\n",
      "[58/3000] 1 tree, 31 leaves, max depth = 9, train loss: 0.10511, val loss: 0.10826, in 0.269s\n",
      "[59/3000] 1 tree, 31 leaves, max depth = 9, train loss: 0.10466, val loss: 0.10791, in 0.287s\n",
      "[60/3000] 1 tree, 31 leaves, max depth = 9, train loss: 0.10422, val loss: 0.10750, in 0.289s\n",
      "[61/3000] 1 tree, 31 leaves, max depth = 10, train loss: 0.10380, val loss: 0.10709, in 0.281s\n",
      "[62/3000] 1 tree, 31 leaves, max depth = 11, train loss: 0.10329, val loss: 0.10662, in 0.269s\n",
      "[63/3000] 1 tree, 31 leaves, max depth = 9, train loss: 0.10277, val loss: 0.10626, in 0.303s\n",
      "[64/3000] 1 tree, 31 leaves, max depth = 11, train loss: 0.10220, val loss: 0.10571, in 0.287s\n",
      "[65/3000] 1 tree, 31 leaves, max depth = 12, train loss: 0.10165, val loss: 0.10524, in 0.268s\n",
      "[66/3000] 1 tree, 31 leaves, max depth = 14, train loss: 0.10116, val loss: 0.10483, in 0.293s\n",
      "[67/3000] 1 tree, 31 leaves, max depth = 9, train loss: 0.10078, val loss: 0.10450, in 0.275s\n",
      "[68/3000] 1 tree, 31 leaves, max depth = 11, train loss: 0.10029, val loss: 0.10408, in 0.312s\n",
      "[69/3000] 1 tree, 31 leaves, max depth = 17, train loss: 0.09973, val loss: 0.10366, in 0.465s\n",
      "[70/3000] 1 tree, 31 leaves, max depth = 13, train loss: 0.09934, val loss: 0.10336, in 0.368s\n",
      "[71/3000] 1 tree, 31 leaves, max depth = 13, train loss: 0.09891, val loss: 0.10296, in 0.339s\n",
      "[72/3000] 1 tree, 31 leaves, max depth = 11, train loss: 0.09846, val loss: 0.10240, in 0.281s\n",
      "[73/3000] 1 tree, 31 leaves, max depth = 13, train loss: 0.09799, val loss: 0.10191, in 0.308s\n",
      "[74/3000] 1 tree, 31 leaves, max depth = 9, train loss: 0.09759, val loss: 0.10154, in 0.310s\n",
      "[75/3000] 1 tree, 31 leaves, max depth = 11, train loss: 0.09720, val loss: 0.10118, in 0.321s\n",
      "[76/3000] 1 tree, 31 leaves, max depth = 8, train loss: 0.09679, val loss: 0.10084, in 0.257s\n",
      "[77/3000] 1 tree, 31 leaves, max depth = 13, train loss: 0.09634, val loss: 0.10039, in 0.269s\n",
      "[78/3000] 1 tree, 31 leaves, max depth = 11, train loss: 0.09581, val loss: 0.09996, in 0.317s\n",
      "[79/3000] 1 tree, 31 leaves, max depth = 12, train loss: 0.09534, val loss: 0.09951, in 0.264s\n",
      "[80/3000] 1 tree, 31 leaves, max depth = 13, train loss: 0.09495, val loss: 0.09915, in 0.259s\n",
      "[81/3000] 1 tree, 31 leaves, max depth = 11, train loss: 0.09454, val loss: 0.09879, in 0.272s\n",
      "[82/3000] 1 tree, 31 leaves, max depth = 9, train loss: 0.09423, val loss: 0.09853, in 0.286s\n",
      "[83/3000] 1 tree, 31 leaves, max depth = 14, train loss: 0.09392, val loss: 0.09827, in 0.315s\n",
      "[84/3000] 1 tree, 31 leaves, max depth = 12, train loss: 0.09357, val loss: 0.09798, in 0.257s\n",
      "[85/3000] 1 tree, 31 leaves, max depth = 13, train loss: 0.09318, val loss: 0.09763, in 0.265s\n",
      "[86/3000] 1 tree, 31 leaves, max depth = 12, train loss: 0.09280, val loss: 0.09733, in 0.294s\n",
      "[87/3000] 1 tree, 31 leaves, max depth = 10, train loss: 0.09246, val loss: 0.09711, in 0.286s\n",
      "[88/3000] 1 tree, 31 leaves, max depth = 11, train loss: 0.09220, val loss: 0.09693, in 0.298s\n",
      "[89/3000] 1 tree, 31 leaves, max depth = 9, train loss: 0.09189, val loss: 0.09669, in 0.441s\n",
      "[90/3000] 1 tree, 31 leaves, max depth = 11, train loss: 0.09150, val loss: 0.09638, in 0.321s\n",
      "[91/3000] 1 tree, 31 leaves, max depth = 13, train loss: 0.09119, val loss: 0.09613, in 0.305s\n",
      "[92/3000] 1 tree, 31 leaves, max depth = 10, train loss: 0.09088, val loss: 0.09583, in 0.289s\n",
      "[93/3000] 1 tree, 31 leaves, max depth = 12, train loss: 0.09051, val loss: 0.09551, in 0.288s\n",
      "[94/3000] 1 tree, 31 leaves, max depth = 15, train loss: 0.09018, val loss: 0.09516, in 0.258s\n",
      "[95/3000] 1 tree, 31 leaves, max depth = 16, train loss: 0.08990, val loss: 0.09486, in 0.272s\n",
      "[96/3000] 1 tree, 31 leaves, max depth = 11, train loss: 0.08957, val loss: 0.09464, in 0.252s\n",
      "[97/3000] 1 tree, 31 leaves, max depth = 11, train loss: 0.08928, val loss: 0.09438, in 0.233s\n",
      "[98/3000] 1 tree, 31 leaves, max depth = 11, train loss: 0.08902, val loss: 0.09423, in 0.273s\n",
      "[99/3000] 1 tree, 31 leaves, max depth = 12, train loss: 0.08880, val loss: 0.09412, in 0.255s\n",
      "[100/3000] 1 tree, 31 leaves, max depth = 10, train loss: 0.08852, val loss: 0.09386, in 0.232s\n",
      "[101/3000] 1 tree, 31 leaves, max depth = 11, train loss: 0.08821, val loss: 0.09361, in 0.286s\n",
      "[102/3000] 1 tree, 31 leaves, max depth = 9, train loss: 0.08793, val loss: 0.09339, in 0.276s\n",
      "[103/3000] 1 tree, 31 leaves, max depth = 10, train loss: 0.08767, val loss: 0.09319, in 0.263s\n",
      "[104/3000] 1 tree, 31 leaves, max depth = 11, train loss: 0.08740, val loss: 0.09296, in 0.269s\n",
      "[105/3000] 1 tree, 31 leaves, max depth = 12, train loss: 0.08715, val loss: 0.09274, in 0.311s\n",
      "[106/3000] 1 tree, 31 leaves, max depth = 13, train loss: 0.08687, val loss: 0.09250, in 0.283s\n",
      "[107/3000] 1 tree, 31 leaves, max depth = 10, train loss: 0.08655, val loss: 0.09225, in 0.249s\n",
      "[108/3000] 1 tree, 31 leaves, max depth = 9, train loss: 0.08634, val loss: 0.09212, in 0.313s\n",
      "[109/3000] 1 tree, 31 leaves, max depth = 9, train loss: 0.08604, val loss: 0.09188, in 0.456s\n",
      "[110/3000] 1 tree, 31 leaves, max depth = 10, train loss: 0.08584, val loss: 0.09175, in 0.305s\n",
      "[111/3000] 1 tree, 31 leaves, max depth = 8, train loss: 0.08553, val loss: 0.09155, in 0.287s\n",
      "[112/3000] 1 tree, 31 leaves, max depth = 11, train loss: 0.08523, val loss: 0.09129, in 0.299s\n",
      "[113/3000] 1 tree, 31 leaves, max depth = 14, train loss: 0.08502, val loss: 0.09118, in 0.285s\n",
      "[114/3000] 1 tree, 31 leaves, max depth = 14, train loss: 0.08480, val loss: 0.09103, in 0.296s\n",
      "[115/3000] 1 tree, 31 leaves, max depth = 11, train loss: 0.08459, val loss: 0.09088, in 0.290s\n",
      "[116/3000] 1 tree, 31 leaves, max depth = 15, train loss: 0.08438, val loss: 0.09072, in 0.282s\n",
      "[117/3000] 1 tree, 31 leaves, max depth = 9, train loss: 0.08408, val loss: 0.09051, in 0.284s\n",
      "[118/3000] 1 tree, 31 leaves, max depth = 13, train loss: 0.08384, val loss: 0.09033, in 0.297s\n",
      "[119/3000] 1 tree, 31 leaves, max depth = 14, train loss: 0.08358, val loss: 0.09012, in 0.244s\n",
      "[120/3000] 1 tree, 31 leaves, max depth = 12, train loss: 0.08338, val loss: 0.08999, in 0.264s\n",
      "[121/3000] 1 tree, 31 leaves, max depth = 10, train loss: 0.08309, val loss: 0.08977, in 0.299s\n",
      "[122/3000] 1 tree, 31 leaves, max depth = 13, train loss: 0.08283, val loss: 0.08954, in 0.280s\n",
      "[123/3000] 1 tree, 31 leaves, max depth = 9, train loss: 0.08258, val loss: 0.08940, in 0.331s\n",
      "[124/3000] 1 tree, 31 leaves, max depth = 10, train loss: 0.08232, val loss: 0.08918, in 0.324s\n",
      "[125/3000] 1 tree, 31 leaves, max depth = 9, train loss: 0.08211, val loss: 0.08902, in 0.347s\n",
      "[126/3000] 1 tree, 31 leaves, max depth = 15, train loss: 0.08193, val loss: 0.08891, in 0.299s\n",
      "[127/3000] 1 tree, 31 leaves, max depth = 8, train loss: 0.08168, val loss: 0.08869, in 0.295s\n",
      "[128/3000] 1 tree, 31 leaves, max depth = 10, train loss: 0.08145, val loss: 0.08852, in 0.304s\n",
      "[129/3000] 1 tree, 31 leaves, max depth = 12, train loss: 0.08127, val loss: 0.08846, in 0.385s\n",
      "[130/3000] 1 tree, 31 leaves, max depth = 11, train loss: 0.08108, val loss: 0.08836, in 0.313s\n",
      "[131/3000] 1 tree, 31 leaves, max depth = 10, train loss: 0.08090, val loss: 0.08826, in 0.356s\n",
      "[132/3000] 1 tree, 31 leaves, max depth = 8, train loss: 0.08071, val loss: 0.08811, in 0.216s\n",
      "[133/3000] 1 tree, 31 leaves, max depth = 11, train loss: 0.08047, val loss: 0.08794, in 0.280s\n",
      "[134/3000] 1 tree, 31 leaves, max depth = 12, train loss: 0.08026, val loss: 0.08782, in 0.270s\n",
      "[135/3000] 1 tree, 31 leaves, max depth = 9, train loss: 0.08007, val loss: 0.08768, in 0.289s\n",
      "[136/3000] 1 tree, 31 leaves, max depth = 9, train loss: 0.07987, val loss: 0.08754, in 0.249s\n",
      "[137/3000] 1 tree, 31 leaves, max depth = 9, train loss: 0.07960, val loss: 0.08729, in 0.265s\n",
      "[138/3000] 1 tree, 31 leaves, max depth = 9, train loss: 0.07942, val loss: 0.08716, in 0.280s\n",
      "[139/3000] 1 tree, 31 leaves, max depth = 9, train loss: 0.07913, val loss: 0.08691, in 0.251s\n",
      "[140/3000] 1 tree, 31 leaves, max depth = 10, train loss: 0.07896, val loss: 0.08680, in 0.277s\n",
      "[141/3000] 1 tree, 31 leaves, max depth = 10, train loss: 0.07877, val loss: 0.08669, in 0.244s\n",
      "[142/3000] 1 tree, 31 leaves, max depth = 9, train loss: 0.07855, val loss: 0.08645, in 0.225s\n",
      "[143/3000] 1 tree, 31 leaves, max depth = 11, train loss: 0.07838, val loss: 0.08630, in 0.280s\n",
      "[144/3000] 1 tree, 31 leaves, max depth = 13, train loss: 0.07823, val loss: 0.08618, in 0.214s\n",
      "[145/3000] 1 tree, 31 leaves, max depth = 8, train loss: 0.07805, val loss: 0.08612, in 0.217s\n",
      "[146/3000] 1 tree, 31 leaves, max depth = 11, train loss: 0.07784, val loss: 0.08596, in 0.245s\n",
      "[147/3000] 1 tree, 31 leaves, max depth = 10, train loss: 0.07763, val loss: 0.08583, in 0.257s\n",
      "[148/3000] 1 tree, 31 leaves, max depth = 12, train loss: 0.07748, val loss: 0.08575, in 0.275s\n",
      "[149/3000] 1 tree, 31 leaves, max depth = 12, train loss: 0.07729, val loss: 0.08563, in 0.260s\n",
      "[150/3000] 1 tree, 31 leaves, max depth = 8, train loss: 0.07707, val loss: 0.08541, in 0.465s\n",
      "[151/3000] 1 tree, 31 leaves, max depth = 10, train loss: 0.07687, val loss: 0.08524, in 0.274s\n",
      "[152/3000] 1 tree, 31 leaves, max depth = 10, train loss: 0.07668, val loss: 0.08511, in 0.332s\n",
      "[153/3000] 1 tree, 31 leaves, max depth = 11, train loss: 0.07651, val loss: 0.08499, in 0.291s\n",
      "[154/3000] 1 tree, 31 leaves, max depth = 11, train loss: 0.07633, val loss: 0.08489, in 0.232s\n",
      "[155/3000] 1 tree, 31 leaves, max depth = 11, train loss: 0.07614, val loss: 0.08470, in 0.285s\n",
      "[156/3000] 1 tree, 31 leaves, max depth = 12, train loss: 0.07594, val loss: 0.08458, in 0.246s\n",
      "[157/3000] 1 tree, 31 leaves, max depth = 11, train loss: 0.07576, val loss: 0.08446, in 0.223s\n",
      "[158/3000] 1 tree, 31 leaves, max depth = 8, train loss: 0.07559, val loss: 0.08434, in 0.248s\n",
      "[159/3000] 1 tree, 31 leaves, max depth = 11, train loss: 0.07542, val loss: 0.08423, in 0.298s\n",
      "[160/3000] 1 tree, 31 leaves, max depth = 10, train loss: 0.07527, val loss: 0.08415, in 0.223s\n",
      "[161/3000] 1 tree, 31 leaves, max depth = 8, train loss: 0.07508, val loss: 0.08402, in 0.308s\n",
      "[162/3000] 1 tree, 31 leaves, max depth = 9, train loss: 0.07490, val loss: 0.08393, in 0.237s\n",
      "[163/3000] 1 tree, 31 leaves, max depth = 11, train loss: 0.07474, val loss: 0.08381, in 0.269s\n",
      "[164/3000] 1 tree, 31 leaves, max depth = 10, train loss: 0.07460, val loss: 0.08372, in 0.207s\n",
      "[165/3000] 1 tree, 31 leaves, max depth = 9, train loss: 0.07439, val loss: 0.08356, in 0.252s\n",
      "[166/3000] 1 tree, 31 leaves, max depth = 8, train loss: 0.07421, val loss: 0.08341, in 0.274s\n",
      "[167/3000] 1 tree, 31 leaves, max depth = 12, train loss: 0.07404, val loss: 0.08331, in 0.253s\n",
      "[168/3000] 1 tree, 31 leaves, max depth = 9, train loss: 0.07390, val loss: 0.08325, in 0.263s\n",
      "[169/3000] 1 tree, 31 leaves, max depth = 14, train loss: 0.07376, val loss: 0.08315, in 0.404s\n",
      "[170/3000] 1 tree, 31 leaves, max depth = 10, train loss: 0.07363, val loss: 0.08312, in 0.319s\n",
      "[171/3000] 1 tree, 31 leaves, max depth = 11, train loss: 0.07340, val loss: 0.08293, in 0.293s\n",
      "[172/3000] 1 tree, 31 leaves, max depth = 13, train loss: 0.07326, val loss: 0.08281, in 0.304s\n",
      "[173/3000] 1 tree, 31 leaves, max depth = 15, train loss: 0.07312, val loss: 0.08272, in 0.297s\n",
      "[174/3000] 1 tree, 31 leaves, max depth = 9, train loss: 0.07298, val loss: 0.08262, in 0.239s\n",
      "[175/3000] 1 tree, 31 leaves, max depth = 10, train loss: 0.07283, val loss: 0.08250, in 0.266s\n",
      "[176/3000] 1 tree, 31 leaves, max depth = 10, train loss: 0.07267, val loss: 0.08241, in 0.256s\n",
      "[177/3000] 1 tree, 31 leaves, max depth = 13, train loss: 0.07253, val loss: 0.08236, in 0.276s\n",
      "[178/3000] 1 tree, 31 leaves, max depth = 10, train loss: 0.07235, val loss: 0.08228, in 0.262s\n",
      "[179/3000] 1 tree, 31 leaves, max depth = 10, train loss: 0.07221, val loss: 0.08222, in 0.184s\n",
      "[180/3000] 1 tree, 31 leaves, max depth = 12, train loss: 0.07207, val loss: 0.08213, in 0.223s\n",
      "[181/3000] 1 tree, 31 leaves, max depth = 10, train loss: 0.07196, val loss: 0.08206, in 0.206s\n",
      "[182/3000] 1 tree, 31 leaves, max depth = 9, train loss: 0.07176, val loss: 0.08195, in 0.314s\n",
      "[183/3000] 1 tree, 31 leaves, max depth = 11, train loss: 0.07161, val loss: 0.08184, in 0.339s\n",
      "[184/3000] 1 tree, 31 leaves, max depth = 11, train loss: 0.07141, val loss: 0.08169, in 0.289s\n",
      "[185/3000] 1 tree, 31 leaves, max depth = 11, train loss: 0.07126, val loss: 0.08160, in 0.338s\n",
      "[186/3000] 1 tree, 31 leaves, max depth = 9, train loss: 0.07115, val loss: 0.08157, in 0.271s\n",
      "[187/3000] 1 tree, 31 leaves, max depth = 14, train loss: 0.07099, val loss: 0.08144, in 0.304s\n",
      "[188/3000] 1 tree, 31 leaves, max depth = 10, train loss: 0.07085, val loss: 0.08135, in 0.315s\n",
      "[189/3000] 1 tree, 31 leaves, max depth = 15, train loss: 0.07072, val loss: 0.08126, in 0.253s\n",
      "[190/3000] 1 tree, 31 leaves, max depth = 12, train loss: 0.07055, val loss: 0.08115, in 0.387s\n",
      "[191/3000] 1 tree, 31 leaves, max depth = 7, train loss: 0.07043, val loss: 0.08105, in 0.254s\n",
      "[192/3000] 1 tree, 31 leaves, max depth = 9, train loss: 0.07024, val loss: 0.08092, in 0.290s\n",
      "[193/3000] 1 tree, 31 leaves, max depth = 10, train loss: 0.07011, val loss: 0.08084, in 0.266s\n",
      "[194/3000] 1 tree, 31 leaves, max depth = 7, train loss: 0.06993, val loss: 0.08074, in 0.227s\n",
      "[195/3000] 1 tree, 31 leaves, max depth = 11, train loss: 0.06978, val loss: 0.08064, in 0.299s\n",
      "[196/3000] 1 tree, 31 leaves, max depth = 11, train loss: 0.06966, val loss: 0.08058, in 0.193s\n",
      "[197/3000] 1 tree, 31 leaves, max depth = 11, train loss: 0.06955, val loss: 0.08049, in 0.222s\n",
      "[198/3000] 1 tree, 31 leaves, max depth = 13, train loss: 0.06944, val loss: 0.08040, in 0.226s\n",
      "[199/3000] 1 tree, 31 leaves, max depth = 11, train loss: 0.06933, val loss: 0.08037, in 0.205s\n",
      "[200/3000] 1 tree, 31 leaves, max depth = 9, train loss: 0.06922, val loss: 0.08032, in 0.231s\n",
      "[201/3000] 1 tree, 31 leaves, max depth = 9, train loss: 0.06902, val loss: 0.08018, in 0.259s\n",
      "[202/3000] 1 tree, 31 leaves, max depth = 15, train loss: 0.06889, val loss: 0.08012, in 0.249s\n",
      "[203/3000] 1 tree, 31 leaves, max depth = 7, train loss: 0.06879, val loss: 0.08001, in 0.238s\n",
      "[204/3000] 1 tree, 31 leaves, max depth = 9, train loss: 0.06868, val loss: 0.07998, in 0.223s\n",
      "[205/3000] 1 tree, 31 leaves, max depth = 13, train loss: 0.06858, val loss: 0.07996, in 0.227s\n",
      "[206/3000] 1 tree, 31 leaves, max depth = 13, train loss: 0.06843, val loss: 0.07988, in 0.252s\n",
      "[207/3000] 1 tree, 31 leaves, max depth = 12, train loss: 0.06832, val loss: 0.07984, in 0.213s\n",
      "[208/3000] 1 tree, 31 leaves, max depth = 9, train loss: 0.06822, val loss: 0.07982, in 0.251s\n",
      "[209/3000] 1 tree, 31 leaves, max depth = 10, train loss: 0.06812, val loss: 0.07978, in 0.211s\n",
      "[210/3000] 1 tree, 31 leaves, max depth = 11, train loss: 0.06796, val loss: 0.07965, in 0.416s\n",
      "[211/3000] 1 tree, 31 leaves, max depth = 11, train loss: 0.06781, val loss: 0.07961, in 0.329s\n",
      "[212/3000] 1 tree, 31 leaves, max depth = 12, train loss: 0.06771, val loss: 0.07956, in 0.616s\n",
      "[213/3000] 1 tree, 31 leaves, max depth = 11, train loss: 0.06759, val loss: 0.07950, in 0.269s\n",
      "[214/3000] 1 tree, 31 leaves, max depth = 10, train loss: 0.06747, val loss: 0.07944, in 0.267s\n",
      "[215/3000] 1 tree, 31 leaves, max depth = 9, train loss: 0.06734, val loss: 0.07937, in 0.300s\n",
      "[216/3000] 1 tree, 31 leaves, max depth = 12, train loss: 0.06722, val loss: 0.07927, in 0.275s\n",
      "[217/3000] 1 tree, 31 leaves, max depth = 9, train loss: 0.06713, val loss: 0.07925, in 0.196s\n",
      "[218/3000] 1 tree, 31 leaves, max depth = 10, train loss: 0.06701, val loss: 0.07921, in 0.242s\n",
      "[219/3000] 1 tree, 31 leaves, max depth = 10, train loss: 0.06684, val loss: 0.07907, in 0.258s\n",
      "[220/3000] 1 tree, 31 leaves, max depth = 13, train loss: 0.06674, val loss: 0.07898, in 0.219s\n",
      "[221/3000] 1 tree, 31 leaves, max depth = 10, train loss: 0.06659, val loss: 0.07891, in 0.253s\n",
      "[222/3000] 1 tree, 31 leaves, max depth = 10, train loss: 0.06649, val loss: 0.07888, in 0.220s\n",
      "[223/3000] 1 tree, 31 leaves, max depth = 15, train loss: 0.06638, val loss: 0.07888, in 0.238s\n",
      "[224/3000] 1 tree, 31 leaves, max depth = 10, train loss: 0.06627, val loss: 0.07882, in 0.245s\n",
      "[225/3000] 1 tree, 31 leaves, max depth = 10, train loss: 0.06613, val loss: 0.07874, in 0.300s\n",
      "[226/3000] 1 tree, 31 leaves, max depth = 16, train loss: 0.06602, val loss: 0.07871, in 0.283s\n",
      "[227/3000] 1 tree, 31 leaves, max depth = 13, train loss: 0.06592, val loss: 0.07870, in 0.213s\n",
      "[228/3000] 1 tree, 31 leaves, max depth = 10, train loss: 0.06583, val loss: 0.07867, in 0.212s\n",
      "[229/3000] 1 tree, 31 leaves, max depth = 17, train loss: 0.06573, val loss: 0.07868, in 0.224s\n",
      "[230/3000] 1 tree, 31 leaves, max depth = 11, train loss: 0.06558, val loss: 0.07856, in 0.263s\n",
      "[231/3000] 1 tree, 31 leaves, max depth = 13, train loss: 0.06544, val loss: 0.07849, in 0.398s\n",
      "[232/3000] 1 tree, 31 leaves, max depth = 12, train loss: 0.06534, val loss: 0.07847, in 0.251s\n",
      "[233/3000] 1 tree, 31 leaves, max depth = 13, train loss: 0.06522, val loss: 0.07841, in 0.338s\n",
      "[234/3000] 1 tree, 31 leaves, max depth = 9, train loss: 0.06509, val loss: 0.07834, in 0.250s\n",
      "[235/3000] 1 tree, 31 leaves, max depth = 14, train loss: 0.06498, val loss: 0.07829, in 0.295s\n",
      "[236/3000] 1 tree, 31 leaves, max depth = 10, train loss: 0.06487, val loss: 0.07822, in 0.197s\n",
      "[237/3000] 1 tree, 31 leaves, max depth = 8, train loss: 0.06477, val loss: 0.07816, in 0.269s\n",
      "[238/3000] 1 tree, 31 leaves, max depth = 10, train loss: 0.06469, val loss: 0.07810, in 0.194s\n",
      "[239/3000] 1 tree, 31 leaves, max depth = 11, train loss: 0.06457, val loss: 0.07803, in 0.258s\n",
      "[240/3000] 1 tree, 31 leaves, max depth = 10, train loss: 0.06449, val loss: 0.07800, in 0.224s\n",
      "[241/3000] 1 tree, 31 leaves, max depth = 9, train loss: 0.06435, val loss: 0.07787, in 0.282s\n",
      "[242/3000] 1 tree, 31 leaves, max depth = 12, train loss: 0.06425, val loss: 0.07784, in 0.287s\n",
      "[243/3000] 1 tree, 31 leaves, max depth = 13, train loss: 0.06416, val loss: 0.07781, in 0.269s\n",
      "[244/3000] 1 tree, 31 leaves, max depth = 12, train loss: 0.06407, val loss: 0.07780, in 0.226s\n",
      "[245/3000] 1 tree, 31 leaves, max depth = 10, train loss: 0.06398, val loss: 0.07775, in 0.253s\n",
      "[246/3000] 1 tree, 31 leaves, max depth = 11, train loss: 0.06386, val loss: 0.07765, in 0.274s\n",
      "[247/3000] 1 tree, 31 leaves, max depth = 10, train loss: 0.06374, val loss: 0.07759, in 0.277s\n",
      "[248/3000] 1 tree, 31 leaves, max depth = 9, train loss: 0.06362, val loss: 0.07752, in 0.232s\n",
      "[249/3000] 1 tree, 31 leaves, max depth = 9, train loss: 0.06353, val loss: 0.07751, in 0.256s\n",
      "[250/3000] 1 tree, 31 leaves, max depth = 10, train loss: 0.06343, val loss: 0.07744, in 0.249s\n",
      "[251/3000] 1 tree, 31 leaves, max depth = 14, train loss: 0.06334, val loss: 0.07744, in 0.228s\n",
      "[252/3000] 1 tree, 31 leaves, max depth = 8, train loss: 0.06320, val loss: 0.07732, in 0.411s\n",
      "[253/3000] 1 tree, 31 leaves, max depth = 12, train loss: 0.06309, val loss: 0.07725, in 0.274s\n",
      "[254/3000] 1 tree, 31 leaves, max depth = 8, train loss: 0.06303, val loss: 0.07722, in 0.198s\n",
      "[255/3000] 1 tree, 31 leaves, max depth = 14, train loss: 0.06292, val loss: 0.07719, in 0.259s\n",
      "[256/3000] 1 tree, 31 leaves, max depth = 11, train loss: 0.06284, val loss: 0.07714, in 0.196s\n",
      "[257/3000] 1 tree, 31 leaves, max depth = 11, train loss: 0.06271, val loss: 0.07705, in 0.289s\n",
      "[258/3000] 1 tree, 31 leaves, max depth = 14, train loss: 0.06259, val loss: 0.07699, in 0.188s\n",
      "[259/3000] 1 tree, 31 leaves, max depth = 9, train loss: 0.06250, val loss: 0.07695, in 0.228s\n",
      "[260/3000] 1 tree, 31 leaves, max depth = 11, train loss: 0.06242, val loss: 0.07690, in 0.232s\n",
      "[261/3000] 1 tree, 31 leaves, max depth = 11, train loss: 0.06232, val loss: 0.07687, in 0.209s\n",
      "[262/3000] 1 tree, 31 leaves, max depth = 10, train loss: 0.06212, val loss: 0.07671, in 0.304s\n",
      "[263/3000] 1 tree, 31 leaves, max depth = 12, train loss: 0.06203, val loss: 0.07665, in 0.224s\n",
      "[264/3000] 1 tree, 31 leaves, max depth = 11, train loss: 0.06195, val loss: 0.07668, in 0.199s\n",
      "[265/3000] 1 tree, 31 leaves, max depth = 9, train loss: 0.06186, val loss: 0.07665, in 0.250s\n",
      "[266/3000] 1 tree, 31 leaves, max depth = 10, train loss: 0.06179, val loss: 0.07664, in 0.229s\n",
      "[267/3000] 1 tree, 31 leaves, max depth = 12, train loss: 0.06168, val loss: 0.07658, in 0.236s\n",
      "[268/3000] 1 tree, 31 leaves, max depth = 12, train loss: 0.06159, val loss: 0.07655, in 0.249s\n",
      "[269/3000] 1 tree, 31 leaves, max depth = 13, train loss: 0.06151, val loss: 0.07651, in 0.200s\n",
      "[270/3000] 1 tree, 31 leaves, max depth = 10, train loss: 0.06142, val loss: 0.07646, in 0.205s\n",
      "[271/3000] 1 tree, 31 leaves, max depth = 9, train loss: 0.06131, val loss: 0.07643, in 0.248s\n",
      "[272/3000] 1 tree, 31 leaves, max depth = 9, train loss: 0.06117, val loss: 0.07631, in 0.412s\n",
      "[273/3000] 1 tree, 31 leaves, max depth = 9, train loss: 0.06105, val loss: 0.07625, in 0.280s\n",
      "[274/3000] 1 tree, 31 leaves, max depth = 8, train loss: 0.06096, val loss: 0.07619, in 0.265s\n",
      "[275/3000] 1 tree, 31 leaves, max depth = 9, train loss: 0.06086, val loss: 0.07617, in 0.293s\n",
      "[276/3000] 1 tree, 31 leaves, max depth = 10, train loss: 0.06076, val loss: 0.07616, in 0.240s\n",
      "[277/3000] 1 tree, 31 leaves, max depth = 11, train loss: 0.06064, val loss: 0.07612, in 0.288s\n",
      "[278/3000] 1 tree, 31 leaves, max depth = 10, train loss: 0.06054, val loss: 0.07609, in 0.253s\n",
      "[279/3000] 1 tree, 31 leaves, max depth = 8, train loss: 0.06039, val loss: 0.07594, in 0.268s\n",
      "[280/3000] 1 tree, 31 leaves, max depth = 10, train loss: 0.06029, val loss: 0.07590, in 0.275s\n",
      "[281/3000] 1 tree, 31 leaves, max depth = 12, train loss: 0.06019, val loss: 0.07584, in 0.237s\n",
      "[282/3000] 1 tree, 31 leaves, max depth = 11, train loss: 0.06011, val loss: 0.07581, in 0.238s\n",
      "[283/3000] 1 tree, 31 leaves, max depth = 11, train loss: 0.06004, val loss: 0.07578, in 0.213s\n",
      "[284/3000] 1 tree, 31 leaves, max depth = 9, train loss: 0.05996, val loss: 0.07575, in 0.195s\n",
      "[285/3000] 1 tree, 31 leaves, max depth = 9, train loss: 0.05987, val loss: 0.07571, in 0.219s\n",
      "[286/3000] 1 tree, 31 leaves, max depth = 9, train loss: 0.05979, val loss: 0.07566, in 0.199s\n",
      "[287/3000] 1 tree, 31 leaves, max depth = 12, train loss: 0.05970, val loss: 0.07563, in 0.234s\n",
      "[288/3000] 1 tree, 31 leaves, max depth = 12, train loss: 0.05960, val loss: 0.07560, in 0.274s\n",
      "[289/3000] 1 tree, 31 leaves, max depth = 9, train loss: 0.05951, val loss: 0.07559, in 0.245s\n",
      "[290/3000] 1 tree, 31 leaves, max depth = 13, train loss: 0.05939, val loss: 0.07556, in 0.250s\n",
      "[291/3000] 1 tree, 31 leaves, max depth = 12, train loss: 0.05931, val loss: 0.07552, in 0.196s\n",
      "[292/3000] 1 tree, 31 leaves, max depth = 12, train loss: 0.05921, val loss: 0.07549, in 0.462s\n",
      "[293/3000] 1 tree, 31 leaves, max depth = 9, train loss: 0.05911, val loss: 0.07539, in 0.281s\n",
      "[294/3000] 1 tree, 31 leaves, max depth = 13, train loss: 0.05900, val loss: 0.07531, in 0.246s\n",
      "[295/3000] 1 tree, 31 leaves, max depth = 9, train loss: 0.05893, val loss: 0.07529, in 0.218s\n",
      "[296/3000] 1 tree, 31 leaves, max depth = 11, train loss: 0.05885, val loss: 0.07523, in 0.223s\n",
      "[297/3000] 1 tree, 31 leaves, max depth = 9, train loss: 0.05877, val loss: 0.07521, in 0.208s\n",
      "[298/3000] 1 tree, 31 leaves, max depth = 13, train loss: 0.05860, val loss: 0.07510, in 0.231s\n",
      "[299/3000] 1 tree, 31 leaves, max depth = 9, train loss: 0.05845, val loss: 0.07500, in 0.226s\n",
      "[300/3000] 1 tree, 31 leaves, max depth = 11, train loss: 0.05837, val loss: 0.07498, in 0.251s\n",
      "[301/3000] 1 tree, 31 leaves, max depth = 11, train loss: 0.05828, val loss: 0.07494, in 0.240s\n",
      "[302/3000] 1 tree, 31 leaves, max depth = 11, train loss: 0.05818, val loss: 0.07490, in 0.258s\n",
      "[303/3000] 1 tree, 31 leaves, max depth = 9, train loss: 0.05809, val loss: 0.07485, in 0.248s\n",
      "[304/3000] 1 tree, 31 leaves, max depth = 11, train loss: 0.05802, val loss: 0.07483, in 0.214s\n",
      "[305/3000] 1 tree, 31 leaves, max depth = 12, train loss: 0.05794, val loss: 0.07479, in 0.296s\n",
      "[306/3000] 1 tree, 31 leaves, max depth = 9, train loss: 0.05787, val loss: 0.07479, in 0.216s\n",
      "[307/3000] 1 tree, 31 leaves, max depth = 8, train loss: 0.05778, val loss: 0.07473, in 0.285s\n",
      "[308/3000] 1 tree, 31 leaves, max depth = 14, train loss: 0.05768, val loss: 0.07468, in 0.218s\n",
      "[309/3000] 1 tree, 31 leaves, max depth = 11, train loss: 0.05763, val loss: 0.07469, in 0.248s\n",
      "[310/3000] 1 tree, 31 leaves, max depth = 12, train loss: 0.05753, val loss: 0.07466, in 0.292s\n",
      "[311/3000] 1 tree, 31 leaves, max depth = 9, train loss: 0.05744, val loss: 0.07464, in 0.220s\n",
      "[312/3000] 1 tree, 31 leaves, max depth = 8, train loss: 0.05735, val loss: 0.07458, in 0.408s\n",
      "[313/3000] 1 tree, 31 leaves, max depth = 12, train loss: 0.05727, val loss: 0.07456, in 0.228s\n",
      "[314/3000] 1 tree, 31 leaves, max depth = 10, train loss: 0.05718, val loss: 0.07452, in 0.254s\n",
      "[315/3000] 1 tree, 31 leaves, max depth = 10, train loss: 0.05712, val loss: 0.07450, in 0.190s\n",
      "[316/3000] 1 tree, 31 leaves, max depth = 10, train loss: 0.05701, val loss: 0.07441, in 0.278s\n",
      "[317/3000] 1 tree, 31 leaves, max depth = 13, train loss: 0.05694, val loss: 0.07440, in 0.236s\n",
      "[318/3000] 1 tree, 31 leaves, max depth = 9, train loss: 0.05686, val loss: 0.07438, in 0.198s\n",
      "[319/3000] 1 tree, 31 leaves, max depth = 14, train loss: 0.05678, val loss: 0.07432, in 0.237s\n",
      "[320/3000] 1 tree, 31 leaves, max depth = 12, train loss: 0.05667, val loss: 0.07426, in 0.249s\n",
      "[321/3000] 1 tree, 31 leaves, max depth = 11, train loss: 0.05660, val loss: 0.07423, in 0.199s\n",
      "[322/3000] 1 tree, 31 leaves, max depth = 12, train loss: 0.05650, val loss: 0.07420, in 0.267s\n",
      "[323/3000] 1 tree, 31 leaves, max depth = 12, train loss: 0.05642, val loss: 0.07416, in 0.239s\n",
      "[324/3000] 1 tree, 31 leaves, max depth = 9, train loss: 0.05635, val loss: 0.07418, in 0.213s\n",
      "[325/3000] 1 tree, 31 leaves, max depth = 11, train loss: 0.05628, val loss: 0.07414, in 0.224s\n",
      "[326/3000] 1 tree, 31 leaves, max depth = 12, train loss: 0.05613, val loss: 0.07400, in 0.244s\n",
      "[327/3000] 1 tree, 31 leaves, max depth = 14, train loss: 0.05606, val loss: 0.07395, in 0.208s\n",
      "[328/3000] 1 tree, 31 leaves, max depth = 10, train loss: 0.05600, val loss: 0.07394, in 0.192s\n",
      "[329/3000] 1 tree, 31 leaves, max depth = 11, train loss: 0.05589, val loss: 0.07390, in 0.290s\n",
      "[330/3000] 1 tree, 31 leaves, max depth = 8, train loss: 0.05577, val loss: 0.07386, in 0.277s\n",
      "[331/3000] 1 tree, 31 leaves, max depth = 11, train loss: 0.05570, val loss: 0.07383, in 0.217s\n",
      "[332/3000] 1 tree, 31 leaves, max depth = 12, train loss: 0.05561, val loss: 0.07378, in 0.390s\n",
      "[333/3000] 1 tree, 31 leaves, max depth = 12, train loss: 0.05552, val loss: 0.07374, in 0.325s\n",
      "[334/3000] 1 tree, 31 leaves, max depth = 13, train loss: 0.05545, val loss: 0.07372, in 0.200s\n",
      "[335/3000] 1 tree, 31 leaves, max depth = 9, train loss: 0.05538, val loss: 0.07368, in 0.205s\n",
      "[336/3000] 1 tree, 31 leaves, max depth = 10, train loss: 0.05529, val loss: 0.07366, in 0.252s\n",
      "[337/3000] 1 tree, 31 leaves, max depth = 12, train loss: 0.05522, val loss: 0.07365, in 0.242s\n",
      "[338/3000] 1 tree, 31 leaves, max depth = 8, train loss: 0.05516, val loss: 0.07363, in 0.199s\n",
      "[339/3000] 1 tree, 31 leaves, max depth = 11, train loss: 0.05510, val loss: 0.07363, in 0.223s\n",
      "[340/3000] 1 tree, 31 leaves, max depth = 8, train loss: 0.05499, val loss: 0.07357, in 0.285s\n",
      "[341/3000] 1 tree, 31 leaves, max depth = 10, train loss: 0.05490, val loss: 0.07353, in 0.217s\n",
      "[342/3000] 1 tree, 31 leaves, max depth = 12, train loss: 0.05484, val loss: 0.07350, in 0.209s\n",
      "[343/3000] 1 tree, 31 leaves, max depth = 14, train loss: 0.05475, val loss: 0.07344, in 0.229s\n",
      "[344/3000] 1 tree, 31 leaves, max depth = 11, train loss: 0.05468, val loss: 0.07341, in 0.213s\n",
      "[345/3000] 1 tree, 31 leaves, max depth = 11, train loss: 0.05461, val loss: 0.07338, in 0.211s\n",
      "[346/3000] 1 tree, 31 leaves, max depth = 10, train loss: 0.05452, val loss: 0.07335, in 0.256s\n",
      "[347/3000] 1 tree, 31 leaves, max depth = 10, train loss: 0.05444, val loss: 0.07331, in 0.219s\n",
      "[348/3000] 1 tree, 31 leaves, max depth = 12, train loss: 0.05437, val loss: 0.07329, in 0.190s\n",
      "[349/3000] 1 tree, 31 leaves, max depth = 15, train loss: 0.05429, val loss: 0.07328, in 0.255s\n",
      "[350/3000] 1 tree, 31 leaves, max depth = 11, train loss: 0.05419, val loss: 0.07320, in 0.233s\n",
      "[351/3000] 1 tree, 31 leaves, max depth = 9, train loss: 0.05413, val loss: 0.07316, in 0.199s\n",
      "[352/3000] 1 tree, 31 leaves, max depth = 12, train loss: 0.05408, val loss: 0.07315, in 0.336s\n",
      "[353/3000] 1 tree, 31 leaves, max depth = 10, train loss: 0.05402, val loss: 0.07312, in 0.199s\n",
      "[354/3000] 1 tree, 31 leaves, max depth = 9, train loss: 0.05397, val loss: 0.07307, in 0.254s\n",
      "[355/3000] 1 tree, 31 leaves, max depth = 12, train loss: 0.05388, val loss: 0.07301, in 0.211s\n",
      "[356/3000] 1 tree, 31 leaves, max depth = 10, train loss: 0.05376, val loss: 0.07293, in 0.281s\n",
      "[357/3000] 1 tree, 31 leaves, max depth = 11, train loss: 0.05368, val loss: 0.07290, in 0.225s\n",
      "[358/3000] 1 tree, 31 leaves, max depth = 12, train loss: 0.05360, val loss: 0.07284, in 0.211s\n",
      "[359/3000] 1 tree, 31 leaves, max depth = 12, train loss: 0.05354, val loss: 0.07286, in 0.202s\n",
      "[360/3000] 1 tree, 31 leaves, max depth = 13, train loss: 0.05346, val loss: 0.07286, in 0.211s\n",
      "[361/3000] 1 tree, 31 leaves, max depth = 16, train loss: 0.05339, val loss: 0.07283, in 0.236s\n",
      "[362/3000] 1 tree, 31 leaves, max depth = 11, train loss: 0.05329, val loss: 0.07281, in 0.205s\n",
      "[363/3000] 1 tree, 31 leaves, max depth = 13, train loss: 0.05323, val loss: 0.07279, in 0.202s\n",
      "[364/3000] 1 tree, 31 leaves, max depth = 17, train loss: 0.05317, val loss: 0.07277, in 0.218s\n",
      "[365/3000] 1 tree, 31 leaves, max depth = 12, train loss: 0.05310, val loss: 0.07276, in 0.193s\n",
      "[366/3000] 1 tree, 31 leaves, max depth = 14, train loss: 0.05303, val loss: 0.07277, in 0.210s\n",
      "[367/3000] 1 tree, 31 leaves, max depth = 8, train loss: 0.05292, val loss: 0.07271, in 0.274s\n",
      "[368/3000] 1 tree, 31 leaves, max depth = 10, train loss: 0.05283, val loss: 0.07268, in 0.232s\n",
      "[369/3000] 1 tree, 31 leaves, max depth = 11, train loss: 0.05276, val loss: 0.07267, in 0.216s\n",
      "[370/3000] 1 tree, 31 leaves, max depth = 9, train loss: 0.05269, val loss: 0.07264, in 0.202s\n",
      "[371/3000] 1 tree, 31 leaves, max depth = 8, train loss: 0.05264, val loss: 0.07260, in 0.233s\n",
      "[372/3000] 1 tree, 31 leaves, max depth = 12, train loss: 0.05256, val loss: 0.07260, in 0.226s\n",
      "[373/3000] 1 tree, 31 leaves, max depth = 10, train loss: 0.05249, val loss: 0.07257, in 0.436s\n",
      "[374/3000] 1 tree, 31 leaves, max depth = 12, train loss: 0.05243, val loss: 0.07254, in 0.198s\n",
      "[375/3000] 1 tree, 31 leaves, max depth = 15, train loss: 0.05237, val loss: 0.07251, in 0.214s\n",
      "[376/3000] 1 tree, 31 leaves, max depth = 11, train loss: 0.05229, val loss: 0.07249, in 0.222s\n",
      "[377/3000] 1 tree, 31 leaves, max depth = 13, train loss: 0.05225, val loss: 0.07249, in 0.215s\n",
      "[378/3000] 1 tree, 31 leaves, max depth = 12, train loss: 0.05218, val loss: 0.07249, in 0.249s\n",
      "[379/3000] 1 tree, 31 leaves, max depth = 12, train loss: 0.05208, val loss: 0.07243, in 0.233s\n",
      "[380/3000] 1 tree, 31 leaves, max depth = 11, train loss: 0.05202, val loss: 0.07243, in 0.208s\n",
      "[381/3000] 1 tree, 31 leaves, max depth = 12, train loss: 0.05197, val loss: 0.07243, in 0.210s\n",
      "[382/3000] 1 tree, 31 leaves, max depth = 13, train loss: 0.05188, val loss: 0.07236, in 0.239s\n",
      "[383/3000] 1 tree, 31 leaves, max depth = 12, train loss: 0.05181, val loss: 0.07233, in 0.256s\n",
      "[384/3000] 1 tree, 31 leaves, max depth = 10, train loss: 0.05175, val loss: 0.07233, in 0.211s\n",
      "[385/3000] 1 tree, 31 leaves, max depth = 15, train loss: 0.05169, val loss: 0.07231, in 0.255s\n",
      "[386/3000] 1 tree, 31 leaves, max depth = 14, train loss: 0.05161, val loss: 0.07229, in 0.213s\n",
      "[387/3000] 1 tree, 31 leaves, max depth = 10, train loss: 0.05150, val loss: 0.07226, in 0.236s\n",
      "[388/3000] 1 tree, 31 leaves, max depth = 12, train loss: 0.05143, val loss: 0.07226, in 0.221s\n",
      "[389/3000] 1 tree, 31 leaves, max depth = 8, train loss: 0.05136, val loss: 0.07220, in 0.199s\n",
      "[390/3000] 1 tree, 31 leaves, max depth = 10, train loss: 0.05129, val loss: 0.07219, in 0.209s\n",
      "[391/3000] 1 tree, 31 leaves, max depth = 11, train loss: 0.05122, val loss: 0.07215, in 0.260s\n",
      "[392/3000] 1 tree, 31 leaves, max depth = 9, train loss: 0.05117, val loss: 0.07212, in 0.202s\n",
      "[393/3000] 1 tree, 31 leaves, max depth = 11, train loss: 0.05112, val loss: 0.07210, in 0.336s\n",
      "[394/3000] 1 tree, 31 leaves, max depth = 12, train loss: 0.05104, val loss: 0.07206, in 0.245s\n",
      "[395/3000] 1 tree, 31 leaves, max depth = 13, train loss: 0.05100, val loss: 0.07205, in 0.216s\n",
      "[396/3000] 1 tree, 31 leaves, max depth = 12, train loss: 0.05092, val loss: 0.07205, in 0.268s\n",
      "[397/3000] 1 tree, 31 leaves, max depth = 10, train loss: 0.05088, val loss: 0.07204, in 0.190s\n",
      "[398/3000] 1 tree, 31 leaves, max depth = 10, train loss: 0.05081, val loss: 0.07200, in 0.271s\n",
      "[399/3000] 1 tree, 31 leaves, max depth = 10, train loss: 0.05075, val loss: 0.07199, in 0.214s\n",
      "[400/3000] 1 tree, 31 leaves, max depth = 16, train loss: 0.05070, val loss: 0.07197, in 0.242s\n",
      "[401/3000] 1 tree, 31 leaves, max depth = 11, train loss: 0.05063, val loss: 0.07196, in 0.197s\n",
      "[402/3000] 1 tree, 31 leaves, max depth = 13, train loss: 0.05057, val loss: 0.07194, in 0.228s\n",
      "[403/3000] 1 tree, 31 leaves, max depth = 11, train loss: 0.05048, val loss: 0.07190, in 0.242s\n",
      "[404/3000] 1 tree, 31 leaves, max depth = 10, train loss: 0.05043, val loss: 0.07189, in 0.173s\n",
      "[405/3000] 1 tree, 31 leaves, max depth = 10, train loss: 0.05035, val loss: 0.07186, in 0.232s\n",
      "[406/3000] 1 tree, 31 leaves, max depth = 12, train loss: 0.05028, val loss: 0.07185, in 0.288s\n",
      "[407/3000] 1 tree, 31 leaves, max depth = 12, train loss: 0.05020, val loss: 0.07183, in 0.232s\n",
      "[408/3000] 1 tree, 31 leaves, max depth = 8, train loss: 0.05013, val loss: 0.07182, in 0.212s\n",
      "[409/3000] 1 tree, 31 leaves, max depth = 10, train loss: 0.05008, val loss: 0.07182, in 0.182s\n",
      "[410/3000] 1 tree, 31 leaves, max depth = 11, train loss: 0.05001, val loss: 0.07178, in 0.217s\n",
      "[411/3000] 1 tree, 31 leaves, max depth = 13, train loss: 0.04994, val loss: 0.07176, in 0.259s\n",
      "[412/3000] 1 tree, 31 leaves, max depth = 10, train loss: 0.04988, val loss: 0.07176, in 0.216s\n",
      "[413/3000] 1 tree, 31 leaves, max depth = 12, train loss: 0.04980, val loss: 0.07170, in 0.348s\n",
      "[414/3000] 1 tree, 31 leaves, max depth = 15, train loss: 0.04973, val loss: 0.07168, in 0.283s\n",
      "[415/3000] 1 tree, 31 leaves, max depth = 11, train loss: 0.04967, val loss: 0.07166, in 0.214s\n",
      "[416/3000] 1 tree, 31 leaves, max depth = 12, train loss: 0.04957, val loss: 0.07158, in 0.270s\n",
      "[417/3000] 1 tree, 31 leaves, max depth = 17, train loss: 0.04949, val loss: 0.07154, in 0.265s\n",
      "[418/3000] 1 tree, 31 leaves, max depth = 13, train loss: 0.04942, val loss: 0.07152, in 0.209s\n",
      "[419/3000] 1 tree, 31 leaves, max depth = 12, train loss: 0.04937, val loss: 0.07150, in 0.245s\n",
      "[420/3000] 1 tree, 31 leaves, max depth = 11, train loss: 0.04931, val loss: 0.07152, in 0.191s\n",
      "[421/3000] 1 tree, 31 leaves, max depth = 7, train loss: 0.04924, val loss: 0.07149, in 0.197s\n",
      "[422/3000] 1 tree, 31 leaves, max depth = 10, train loss: 0.04918, val loss: 0.07146, in 0.191s\n",
      "[423/3000] 1 tree, 31 leaves, max depth = 12, train loss: 0.04911, val loss: 0.07146, in 0.269s\n",
      "[424/3000] 1 tree, 31 leaves, max depth = 8, train loss: 0.04901, val loss: 0.07139, in 0.295s\n",
      "[425/3000] 1 tree, 31 leaves, max depth = 13, train loss: 0.04893, val loss: 0.07139, in 0.243s\n",
      "[426/3000] 1 tree, 31 leaves, max depth = 14, train loss: 0.04889, val loss: 0.07138, in 0.195s\n",
      "[427/3000] 1 tree, 31 leaves, max depth = 12, train loss: 0.04882, val loss: 0.07135, in 0.264s\n",
      "[428/3000] 1 tree, 31 leaves, max depth = 9, train loss: 0.04877, val loss: 0.07134, in 0.194s\n",
      "[429/3000] 1 tree, 31 leaves, max depth = 12, train loss: 0.04873, val loss: 0.07134, in 0.197s\n",
      "[430/3000] 1 tree, 31 leaves, max depth = 12, train loss: 0.04865, val loss: 0.07129, in 0.256s\n",
      "[431/3000] 1 tree, 31 leaves, max depth = 9, train loss: 0.04858, val loss: 0.07130, in 0.210s\n",
      "[432/3000] 1 tree, 31 leaves, max depth = 8, train loss: 0.04852, val loss: 0.07131, in 0.200s\n",
      "[433/3000] 1 tree, 31 leaves, max depth = 13, train loss: 0.04846, val loss: 0.07133, in 0.211s\n",
      "[434/3000] 1 tree, 31 leaves, max depth = 9, train loss: 0.04840, val loss: 0.07129, in 0.371s\n",
      "[435/3000] 1 tree, 31 leaves, max depth = 9, train loss: 0.04833, val loss: 0.07127, in 0.235s\n",
      "[436/3000] 1 tree, 31 leaves, max depth = 12, train loss: 0.04828, val loss: 0.07125, in 0.243s\n",
      "[437/3000] 1 tree, 31 leaves, max depth = 10, train loss: 0.04823, val loss: 0.07121, in 0.210s\n",
      "[438/3000] 1 tree, 31 leaves, max depth = 14, train loss: 0.04818, val loss: 0.07119, in 0.273s\n",
      "[439/3000] 1 tree, 31 leaves, max depth = 12, train loss: 0.04812, val loss: 0.07117, in 0.180s\n",
      "[440/3000] 1 tree, 31 leaves, max depth = 14, train loss: 0.04804, val loss: 0.07116, in 0.313s\n",
      "[441/3000] 1 tree, 31 leaves, max depth = 11, train loss: 0.04797, val loss: 0.07115, in 0.345s\n",
      "[442/3000] 1 tree, 31 leaves, max depth = 12, train loss: 0.04791, val loss: 0.07112, in 0.225s\n",
      "[443/3000] 1 tree, 31 leaves, max depth = 12, train loss: 0.04778, val loss: 0.07100, in 0.259s\n",
      "[444/3000] 1 tree, 31 leaves, max depth = 9, train loss: 0.04772, val loss: 0.07098, in 0.262s\n",
      "[445/3000] 1 tree, 31 leaves, max depth = 9, train loss: 0.04765, val loss: 0.07095, in 0.251s\n",
      "[446/3000] 1 tree, 31 leaves, max depth = 9, train loss: 0.04757, val loss: 0.07091, in 0.205s\n",
      "[447/3000] 1 tree, 31 leaves, max depth = 15, train loss: 0.04752, val loss: 0.07089, in 0.197s\n",
      "[448/3000] 1 tree, 31 leaves, max depth = 10, train loss: 0.04747, val loss: 0.07089, in 0.209s\n",
      "[449/3000] 1 tree, 31 leaves, max depth = 11, train loss: 0.04739, val loss: 0.07086, in 0.247s\n",
      "[450/3000] 1 tree, 31 leaves, max depth = 10, train loss: 0.04733, val loss: 0.07090, in 0.212s\n",
      "[451/3000] 1 tree, 31 leaves, max depth = 9, train loss: 0.04728, val loss: 0.07086, in 0.224s\n",
      "[452/3000] 1 tree, 31 leaves, max depth = 11, train loss: 0.04724, val loss: 0.07085, in 0.187s\n",
      "[453/3000] 1 tree, 31 leaves, max depth = 10, train loss: 0.04718, val loss: 0.07079, in 0.230s\n",
      "[454/3000] 1 tree, 31 leaves, max depth = 10, train loss: 0.04708, val loss: 0.07074, in 0.222s\n",
      "[455/3000] 1 tree, 31 leaves, max depth = 13, train loss: 0.04700, val loss: 0.07066, in 0.421s\n",
      "[456/3000] 1 tree, 31 leaves, max depth = 8, train loss: 0.04694, val loss: 0.07064, in 0.227s\n",
      "[457/3000] 1 tree, 31 leaves, max depth = 11, train loss: 0.04686, val loss: 0.07061, in 0.247s\n",
      "[458/3000] 1 tree, 31 leaves, max depth = 13, train loss: 0.04680, val loss: 0.07061, in 0.290s\n",
      "[459/3000] 1 tree, 31 leaves, max depth = 9, train loss: 0.04673, val loss: 0.07058, in 0.259s\n",
      "[460/3000] 1 tree, 31 leaves, max depth = 14, train loss: 0.04668, val loss: 0.07057, in 0.205s\n",
      "[461/3000] 1 tree, 31 leaves, max depth = 9, train loss: 0.04660, val loss: 0.07054, in 0.229s\n",
      "[462/3000] 1 tree, 31 leaves, max depth = 10, train loss: 0.04650, val loss: 0.07047, in 0.261s\n",
      "[463/3000] 1 tree, 31 leaves, max depth = 10, train loss: 0.04644, val loss: 0.07045, in 0.222s\n",
      "[464/3000] 1 tree, 31 leaves, max depth = 11, train loss: 0.04638, val loss: 0.07044, in 0.224s\n",
      "[465/3000] 1 tree, 31 leaves, max depth = 11, train loss: 0.04632, val loss: 0.07042, in 0.244s\n",
      "[466/3000] 1 tree, 31 leaves, max depth = 11, train loss: 0.04625, val loss: 0.07041, in 0.222s\n",
      "[467/3000] 1 tree, 31 leaves, max depth = 12, train loss: 0.04621, val loss: 0.07040, in 0.198s\n",
      "[468/3000] 1 tree, 31 leaves, max depth = 11, train loss: 0.04612, val loss: 0.07035, in 0.264s\n",
      "[469/3000] 1 tree, 31 leaves, max depth = 9, train loss: 0.04607, val loss: 0.07034, in 0.199s\n",
      "[470/3000] 1 tree, 31 leaves, max depth = 11, train loss: 0.04601, val loss: 0.07030, in 0.206s\n",
      "[471/3000] 1 tree, 31 leaves, max depth = 13, train loss: 0.04596, val loss: 0.07030, in 0.209s\n",
      "[472/3000] 1 tree, 31 leaves, max depth = 9, train loss: 0.04591, val loss: 0.07030, in 0.227s\n",
      "[473/3000] 1 tree, 31 leaves, max depth = 11, train loss: 0.04585, val loss: 0.07027, in 0.210s\n",
      "[474/3000] 1 tree, 31 leaves, max depth = 14, train loss: 0.04580, val loss: 0.07027, in 0.384s\n",
      "[475/3000] 1 tree, 31 leaves, max depth = 10, train loss: 0.04575, val loss: 0.07026, in 0.242s\n",
      "[476/3000] 1 tree, 31 leaves, max depth = 11, train loss: 0.04569, val loss: 0.07023, in 0.242s\n",
      "[477/3000] 1 tree, 31 leaves, max depth = 11, train loss: 0.04560, val loss: 0.07017, in 0.248s\n",
      "[478/3000] 1 tree, 31 leaves, max depth = 11, train loss: 0.04554, val loss: 0.07014, in 0.250s\n",
      "[479/3000] 1 tree, 31 leaves, max depth = 10, train loss: 0.04547, val loss: 0.07012, in 0.230s\n",
      "[480/3000] 1 tree, 31 leaves, max depth = 11, train loss: 0.04541, val loss: 0.07008, in 0.242s\n",
      "[481/3000] 1 tree, 31 leaves, max depth = 12, train loss: 0.04532, val loss: 0.07004, in 0.250s\n",
      "[482/3000] 1 tree, 31 leaves, max depth = 12, train loss: 0.04526, val loss: 0.07003, in 0.242s\n",
      "[483/3000] 1 tree, 31 leaves, max depth = 12, train loss: 0.04522, val loss: 0.07002, in 0.205s\n",
      "[484/3000] 1 tree, 31 leaves, max depth = 10, train loss: 0.04515, val loss: 0.06998, in 0.243s\n",
      "[485/3000] 1 tree, 31 leaves, max depth = 9, train loss: 0.04510, val loss: 0.06999, in 0.197s\n",
      "[486/3000] 1 tree, 31 leaves, max depth = 16, train loss: 0.04502, val loss: 0.06996, in 0.230s\n",
      "[487/3000] 1 tree, 31 leaves, max depth = 9, train loss: 0.04494, val loss: 0.06991, in 0.259s\n",
      "[488/3000] 1 tree, 31 leaves, max depth = 11, train loss: 0.04488, val loss: 0.06993, in 0.207s\n",
      "[489/3000] 1 tree, 31 leaves, max depth = 11, train loss: 0.04482, val loss: 0.06992, in 0.230s\n",
      "[490/3000] 1 tree, 31 leaves, max depth = 9, train loss: 0.04476, val loss: 0.06992, in 0.303s\n",
      "[491/3000] 1 tree, 31 leaves, max depth = 9, train loss: 0.04470, val loss: 0.06992, in 0.204s\n",
      "[492/3000] 1 tree, 31 leaves, max depth = 11, train loss: 0.04463, val loss: 0.06992, in 0.254s\n",
      "[493/3000] 1 tree, 31 leaves, max depth = 12, train loss: 0.04459, val loss: 0.06993, in 0.185s\n",
      "[494/3000] 1 tree, 31 leaves, max depth = 14, train loss: 0.04453, val loss: 0.06991, in 0.256s\n",
      "[495/3000] 1 tree, 31 leaves, max depth = 11, train loss: 0.04446, val loss: 0.06992, in 0.425s\n",
      "[496/3000] 1 tree, 31 leaves, max depth = 12, train loss: 0.04441, val loss: 0.06992, in 0.236s\n",
      "[497/3000] 1 tree, 31 leaves, max depth = 10, train loss: 0.04435, val loss: 0.06989, in 0.272s\n",
      "[498/3000] 1 tree, 31 leaves, max depth = 12, train loss: 0.04431, val loss: 0.06989, in 0.199s\n",
      "[499/3000] 1 tree, 31 leaves, max depth = 9, train loss: 0.04425, val loss: 0.06990, in 0.226s\n",
      "[500/3000] 1 tree, 31 leaves, max depth = 9, train loss: 0.04421, val loss: 0.06989, in 0.211s\n",
      "[501/3000] 1 tree, 31 leaves, max depth = 8, train loss: 0.04418, val loss: 0.06990, in 0.217s\n",
      "[502/3000] 1 tree, 31 leaves, max depth = 15, train loss: 0.04412, val loss: 0.06988, in 0.202s\n",
      "[503/3000] 1 tree, 31 leaves, max depth = 8, train loss: 0.04407, val loss: 0.06989, in 0.201s\n",
      "[504/3000] 1 tree, 31 leaves, max depth = 10, train loss: 0.04400, val loss: 0.06988, in 0.224s\n",
      "[505/3000] 1 tree, 31 leaves, max depth = 10, train loss: 0.04393, val loss: 0.06985, in 0.240s\n",
      "[506/3000] 1 tree, 31 leaves, max depth = 11, train loss: 0.04387, val loss: 0.06985, in 0.245s\n",
      "[507/3000] 1 tree, 31 leaves, max depth = 10, train loss: 0.04380, val loss: 0.06981, in 0.343s\n",
      "[508/3000] 1 tree, 31 leaves, max depth = 13, train loss: 0.04375, val loss: 0.06984, in 0.216s\n",
      "[509/3000] 1 tree, 31 leaves, max depth = 10, train loss: 0.04368, val loss: 0.06979, in 0.279s\n",
      "[510/3000] 1 tree, 31 leaves, max depth = 13, train loss: 0.04363, val loss: 0.06978, in 0.213s\n",
      "[511/3000] 1 tree, 31 leaves, max depth = 15, train loss: 0.04357, val loss: 0.06976, in 0.259s\n",
      "[512/3000] 1 tree, 31 leaves, max depth = 9, train loss: 0.04351, val loss: 0.06976, in 0.273s\n",
      "[513/3000] 1 tree, 31 leaves, max depth = 9, train loss: 0.04345, val loss: 0.06975, in 0.253s\n",
      "[514/3000] 1 tree, 31 leaves, max depth = 9, train loss: 0.04340, val loss: 0.06972, in 0.253s\n",
      "[515/3000] 1 tree, 31 leaves, max depth = 11, train loss: 0.04335, val loss: 0.06970, in 0.320s\n",
      "[516/3000] 1 tree, 31 leaves, max depth = 15, train loss: 0.04329, val loss: 0.06968, in 0.245s\n",
      "[517/3000] 1 tree, 31 leaves, max depth = 10, train loss: 0.04323, val loss: 0.06968, in 0.234s\n",
      "[518/3000] 1 tree, 31 leaves, max depth = 10, train loss: 0.04316, val loss: 0.06964, in 0.239s\n",
      "[519/3000] 1 tree, 31 leaves, max depth = 10, train loss: 0.04310, val loss: 0.06962, in 0.270s\n",
      "[520/3000] 1 tree, 31 leaves, max depth = 11, train loss: 0.04302, val loss: 0.06954, in 0.213s\n",
      "[521/3000] 1 tree, 31 leaves, max depth = 11, train loss: 0.04296, val loss: 0.06953, in 0.272s\n",
      "[522/3000] 1 tree, 31 leaves, max depth = 11, train loss: 0.04292, val loss: 0.06952, in 0.205s\n",
      "[523/3000] 1 tree, 31 leaves, max depth = 11, train loss: 0.04288, val loss: 0.06949, in 0.193s\n",
      "[524/3000] 1 tree, 31 leaves, max depth = 10, train loss: 0.04281, val loss: 0.06947, in 0.250s\n",
      "[525/3000] 1 tree, 31 leaves, max depth = 14, train loss: 0.04276, val loss: 0.06945, in 0.203s\n",
      "[526/3000] 1 tree, 31 leaves, max depth = 12, train loss: 0.04272, val loss: 0.06944, in 0.197s\n",
      "[527/3000] 1 tree, 31 leaves, max depth = 11, train loss: 0.04266, val loss: 0.06942, in 0.198s\n",
      "[528/3000] 1 tree, 31 leaves, max depth = 13, train loss: 0.04260, val loss: 0.06938, in 0.218s\n",
      "[529/3000] 1 tree, 31 leaves, max depth = 11, train loss: 0.04255, val loss: 0.06938, in 0.191s\n",
      "[530/3000] 1 tree, 31 leaves, max depth = 10, train loss: 0.04250, val loss: 0.06938, in 0.186s\n",
      "[531/3000] 1 tree, 31 leaves, max depth = 10, train loss: 0.04246, val loss: 0.06936, in 0.202s\n",
      "[532/3000] 1 tree, 31 leaves, max depth = 9, train loss: 0.04241, val loss: 0.06931, in 0.232s\n",
      "[533/3000] 1 tree, 31 leaves, max depth = 10, train loss: 0.04236, val loss: 0.06930, in 0.223s\n",
      "[534/3000] 1 tree, 31 leaves, max depth = 10, train loss: 0.04230, val loss: 0.06928, in 0.234s\n",
      "[535/3000] 1 tree, 31 leaves, max depth = 11, train loss: 0.04224, val loss: 0.06924, in 0.222s\n",
      "[536/3000] 1 tree, 31 leaves, max depth = 9, train loss: 0.04218, val loss: 0.06921, in 0.395s\n",
      "[537/3000] 1 tree, 31 leaves, max depth = 14, train loss: 0.04212, val loss: 0.06920, in 0.264s\n",
      "[538/3000] 1 tree, 31 leaves, max depth = 11, train loss: 0.04208, val loss: 0.06917, in 0.228s\n",
      "[539/3000] 1 tree, 31 leaves, max depth = 9, train loss: 0.04200, val loss: 0.06913, in 0.245s\n",
      "[540/3000] 1 tree, 31 leaves, max depth = 10, train loss: 0.04192, val loss: 0.06911, in 0.285s\n",
      "[541/3000] 1 tree, 31 leaves, max depth = 13, train loss: 0.04187, val loss: 0.06911, in 0.210s\n",
      "[542/3000] 1 tree, 31 leaves, max depth = 12, train loss: 0.04183, val loss: 0.06908, in 0.215s\n",
      "[543/3000] 1 tree, 31 leaves, max depth = 9, train loss: 0.04177, val loss: 0.06907, in 0.298s\n",
      "[544/3000] 1 tree, 31 leaves, max depth = 10, train loss: 0.04171, val loss: 0.06905, in 0.266s\n",
      "[545/3000] 1 tree, 31 leaves, max depth = 10, train loss: 0.04166, val loss: 0.06906, in 0.194s\n",
      "[546/3000] 1 tree, 31 leaves, max depth = 8, train loss: 0.04161, val loss: 0.06902, in 0.246s\n",
      "[547/3000] 1 tree, 31 leaves, max depth = 10, train loss: 0.04155, val loss: 0.06901, in 0.240s\n",
      "[548/3000] 1 tree, 31 leaves, max depth = 9, train loss: 0.04152, val loss: 0.06901, in 0.202s\n",
      "[549/3000] 1 tree, 31 leaves, max depth = 10, train loss: 0.04147, val loss: 0.06898, in 0.262s\n",
      "[550/3000] 1 tree, 31 leaves, max depth = 17, train loss: 0.04143, val loss: 0.06896, in 0.200s\n",
      "[551/3000] 1 tree, 31 leaves, max depth = 12, train loss: 0.04133, val loss: 0.06887, in 0.282s\n",
      "[552/3000] 1 tree, 31 leaves, max depth = 11, train loss: 0.04128, val loss: 0.06887, in 0.277s\n",
      "[553/3000] 1 tree, 31 leaves, max depth = 9, train loss: 0.04122, val loss: 0.06881, in 0.244s\n",
      "[554/3000] 1 tree, 31 leaves, max depth = 12, train loss: 0.04118, val loss: 0.06881, in 0.229s\n",
      "[555/3000] 1 tree, 31 leaves, max depth = 11, train loss: 0.04114, val loss: 0.06881, in 0.189s\n",
      "[556/3000] 1 tree, 31 leaves, max depth = 13, train loss: 0.04110, val loss: 0.06880, in 0.355s\n",
      "[557/3000] 1 tree, 31 leaves, max depth = 10, train loss: 0.04104, val loss: 0.06880, in 0.303s\n",
      "[558/3000] 1 tree, 31 leaves, max depth = 10, train loss: 0.04100, val loss: 0.06878, in 0.203s\n",
      "[559/3000] 1 tree, 31 leaves, max depth = 10, train loss: 0.04096, val loss: 0.06876, in 0.208s\n",
      "[560/3000] 1 tree, 31 leaves, max depth = 16, train loss: 0.04092, val loss: 0.06874, in 0.203s\n",
      "[561/3000] 1 tree, 31 leaves, max depth = 9, train loss: 0.04089, val loss: 0.06873, in 0.211s\n",
      "[562/3000] 1 tree, 31 leaves, max depth = 11, train loss: 0.04086, val loss: 0.06872, in 0.166s\n",
      "[563/3000] 1 tree, 31 leaves, max depth = 9, train loss: 0.04080, val loss: 0.06870, in 0.254s\n",
      "[564/3000] 1 tree, 31 leaves, max depth = 12, train loss: 0.04076, val loss: 0.06867, in 0.210s\n",
      "[565/3000] 1 tree, 31 leaves, max depth = 11, train loss: 0.04072, val loss: 0.06868, in 0.211s\n",
      "[566/3000] 1 tree, 31 leaves, max depth = 11, train loss: 0.04067, val loss: 0.06864, in 0.241s\n",
      "[567/3000] 1 tree, 31 leaves, max depth = 10, train loss: 0.04063, val loss: 0.06863, in 0.210s\n",
      "[568/3000] 1 tree, 31 leaves, max depth = 15, train loss: 0.04059, val loss: 0.06860, in 0.204s\n",
      "[569/3000] 1 tree, 31 leaves, max depth = 12, train loss: 0.04054, val loss: 0.06860, in 0.222s\n",
      "[570/3000] 1 tree, 31 leaves, max depth = 12, train loss: 0.04051, val loss: 0.06859, in 0.197s\n",
      "[571/3000] 1 tree, 31 leaves, max depth = 10, train loss: 0.04045, val loss: 0.06855, in 0.252s\n",
      "[572/3000] 1 tree, 31 leaves, max depth = 13, train loss: 0.04040, val loss: 0.06855, in 0.230s\n",
      "[573/3000] 1 tree, 31 leaves, max depth = 10, train loss: 0.04035, val loss: 0.06855, in 0.239s\n",
      "[574/3000] 1 tree, 31 leaves, max depth = 12, train loss: 0.04030, val loss: 0.06853, in 0.258s\n",
      "[575/3000] 1 tree, 31 leaves, max depth = 10, train loss: 0.04025, val loss: 0.06853, in 0.297s\n",
      "[576/3000] 1 tree, 31 leaves, max depth = 11, train loss: 0.04018, val loss: 0.06851, in 0.235s\n",
      "[577/3000] 1 tree, 31 leaves, max depth = 11, train loss: 0.04013, val loss: 0.06847, in 0.377s\n",
      "[578/3000] 1 tree, 31 leaves, max depth = 17, train loss: 0.04010, val loss: 0.06845, in 0.196s\n",
      "[579/3000] 1 tree, 31 leaves, max depth = 14, train loss: 0.04005, val loss: 0.06846, in 0.227s\n",
      "[580/3000] 1 tree, 31 leaves, max depth = 15, train loss: 0.04001, val loss: 0.06843, in 0.211s\n",
      "[581/3000] 1 tree, 31 leaves, max depth = 11, train loss: 0.03995, val loss: 0.06841, in 0.213s\n",
      "[582/3000] 1 tree, 31 leaves, max depth = 12, train loss: 0.03988, val loss: 0.06839, in 0.293s\n",
      "[583/3000] 1 tree, 31 leaves, max depth = 14, train loss: 0.03982, val loss: 0.06837, in 0.244s\n",
      "[584/3000] 1 tree, 31 leaves, max depth = 11, train loss: 0.03977, val loss: 0.06837, in 0.197s\n",
      "[585/3000] 1 tree, 31 leaves, max depth = 16, train loss: 0.03972, val loss: 0.06836, in 0.216s\n",
      "[586/3000] 1 tree, 31 leaves, max depth = 8, train loss: 0.03967, val loss: 0.06834, in 0.252s\n",
      "[587/3000] 1 tree, 31 leaves, max depth = 11, train loss: 0.03963, val loss: 0.06833, in 0.215s\n",
      "[588/3000] 1 tree, 31 leaves, max depth = 14, train loss: 0.03958, val loss: 0.06832, in 0.210s\n",
      "[589/3000] 1 tree, 31 leaves, max depth = 11, train loss: 0.03954, val loss: 0.06830, in 0.228s\n",
      "[590/3000] 1 tree, 31 leaves, max depth = 10, train loss: 0.03951, val loss: 0.06829, in 0.196s\n",
      "[591/3000] 1 tree, 31 leaves, max depth = 11, train loss: 0.03947, val loss: 0.06828, in 0.218s\n",
      "[592/3000] 1 tree, 31 leaves, max depth = 14, train loss: 0.03942, val loss: 0.06828, in 0.290s\n",
      "[593/3000] 1 tree, 31 leaves, max depth = 14, train loss: 0.03937, val loss: 0.06825, in 0.228s\n",
      "[594/3000] 1 tree, 31 leaves, max depth = 12, train loss: 0.03932, val loss: 0.06820, in 0.225s\n",
      "[595/3000] 1 tree, 31 leaves, max depth = 12, train loss: 0.03927, val loss: 0.06819, in 0.275s\n",
      "[596/3000] 1 tree, 31 leaves, max depth = 9, train loss: 0.03923, val loss: 0.06818, in 0.197s\n",
      "[597/3000] 1 tree, 31 leaves, max depth = 12, train loss: 0.03915, val loss: 0.06814, in 0.424s\n",
      "[598/3000] 1 tree, 31 leaves, max depth = 9, train loss: 0.03910, val loss: 0.06814, in 0.244s\n",
      "[599/3000] 1 tree, 31 leaves, max depth = 11, train loss: 0.03905, val loss: 0.06811, in 0.519s\n",
      "[600/3000] 1 tree, 31 leaves, max depth = 14, train loss: 0.03899, val loss: 0.06810, in 0.256s\n",
      "[601/3000] 1 tree, 31 leaves, max depth = 11, train loss: 0.03895, val loss: 0.06807, in 0.230s\n",
      "[602/3000] 1 tree, 31 leaves, max depth = 13, train loss: 0.03885, val loss: 0.06796, in 0.219s\n",
      "[603/3000] 1 tree, 31 leaves, max depth = 12, train loss: 0.03880, val loss: 0.06796, in 0.253s\n",
      "[604/3000] 1 tree, 31 leaves, max depth = 11, train loss: 0.03875, val loss: 0.06795, in 0.278s\n",
      "[605/3000] 1 tree, 31 leaves, max depth = 13, train loss: 0.03870, val loss: 0.06793, in 0.279s\n",
      "[606/3000] 1 tree, 31 leaves, max depth = 10, train loss: 0.03864, val loss: 0.06792, in 0.262s\n",
      "[607/3000] 1 tree, 31 leaves, max depth = 9, train loss: 0.03860, val loss: 0.06792, in 0.255s\n",
      "[608/3000] 1 tree, 31 leaves, max depth = 14, train loss: 0.03856, val loss: 0.06790, in 0.202s\n",
      "[609/3000] 1 tree, 31 leaves, max depth = 11, train loss: 0.03850, val loss: 0.06786, in 0.244s\n",
      "[610/3000] 1 tree, 31 leaves, max depth = 9, train loss: 0.03846, val loss: 0.06784, in 0.230s\n",
      "[611/3000] 1 tree, 31 leaves, max depth = 13, train loss: 0.03841, val loss: 0.06784, in 0.232s\n",
      "[612/3000] 1 tree, 31 leaves, max depth = 12, train loss: 0.03837, val loss: 0.06782, in 0.196s\n",
      "[613/3000] 1 tree, 31 leaves, max depth = 8, train loss: 0.03832, val loss: 0.06781, in 0.237s\n",
      "[614/3000] 1 tree, 31 leaves, max depth = 10, train loss: 0.03827, val loss: 0.06782, in 0.198s\n",
      "[615/3000] 1 tree, 31 leaves, max depth = 10, train loss: 0.03823, val loss: 0.06782, in 0.208s\n",
      "[616/3000] 1 tree, 31 leaves, max depth = 9, train loss: 0.03820, val loss: 0.06781, in 0.199s\n",
      "[617/3000] 1 tree, 31 leaves, max depth = 15, train loss: 0.03815, val loss: 0.06781, in 0.206s\n",
      "[618/3000] 1 tree, 31 leaves, max depth = 13, train loss: 0.03810, val loss: 0.06782, in 0.355s\n",
      "[619/3000] 1 tree, 31 leaves, max depth = 10, train loss: 0.03805, val loss: 0.06779, in 0.251s\n",
      "[620/3000] 1 tree, 31 leaves, max depth = 9, train loss: 0.03799, val loss: 0.06775, in 0.237s\n",
      "[621/3000] 1 tree, 31 leaves, max depth = 12, train loss: 0.03796, val loss: 0.06775, in 0.187s\n",
      "[622/3000] 1 tree, 31 leaves, max depth = 8, train loss: 0.03792, val loss: 0.06774, in 0.208s\n",
      "[623/3000] 1 tree, 31 leaves, max depth = 13, train loss: 0.03788, val loss: 0.06773, in 0.219s\n",
      "[624/3000] 1 tree, 31 leaves, max depth = 11, train loss: 0.03784, val loss: 0.06771, in 0.218s\n",
      "[625/3000] 1 tree, 31 leaves, max depth = 10, train loss: 0.03779, val loss: 0.06768, in 0.222s\n",
      "[626/3000] 1 tree, 31 leaves, max depth = 13, train loss: 0.03774, val loss: 0.06766, in 0.216s\n",
      "[627/3000] 1 tree, 31 leaves, max depth = 10, train loss: 0.03771, val loss: 0.06765, in 0.202s\n",
      "[628/3000] 1 tree, 31 leaves, max depth = 8, train loss: 0.03765, val loss: 0.06761, in 0.245s\n",
      "[629/3000] 1 tree, 31 leaves, max depth = 11, train loss: 0.03761, val loss: 0.06760, in 0.233s\n",
      "[630/3000] 1 tree, 31 leaves, max depth = 15, train loss: 0.03755, val loss: 0.06757, in 0.262s\n",
      "[631/3000] 1 tree, 31 leaves, max depth = 12, train loss: 0.03749, val loss: 0.06753, in 0.274s\n",
      "[632/3000] 1 tree, 31 leaves, max depth = 8, train loss: 0.03746, val loss: 0.06752, in 0.193s\n",
      "[633/3000] 1 tree, 31 leaves, max depth = 10, train loss: 0.03741, val loss: 0.06749, in 0.216s\n",
      "[634/3000] 1 tree, 31 leaves, max depth = 14, train loss: 0.03736, val loss: 0.06747, in 0.188s\n",
      "[635/3000] 1 tree, 31 leaves, max depth = 11, train loss: 0.03731, val loss: 0.06745, in 0.245s\n",
      "[636/3000] 1 tree, 31 leaves, max depth = 9, train loss: 0.03727, val loss: 0.06742, in 0.234s\n",
      "[637/3000] 1 tree, 31 leaves, max depth = 9, train loss: 0.03724, val loss: 0.06742, in 0.200s\n",
      "[638/3000] 1 tree, 31 leaves, max depth = 14, train loss: 0.03721, val loss: 0.06741, in 0.190s\n",
      "[639/3000] 1 tree, 31 leaves, max depth = 14, train loss: 0.03717, val loss: 0.06741, in 0.397s\n",
      "[640/3000] 1 tree, 31 leaves, max depth = 13, train loss: 0.03712, val loss: 0.06737, in 0.280s\n",
      "[641/3000] 1 tree, 31 leaves, max depth = 10, train loss: 0.03707, val loss: 0.06733, in 0.278s\n",
      "[642/3000] 1 tree, 31 leaves, max depth = 14, train loss: 0.03702, val loss: 0.06731, in 0.239s\n",
      "[643/3000] 1 tree, 31 leaves, max depth = 10, train loss: 0.03695, val loss: 0.06726, in 0.277s\n",
      "[644/3000] 1 tree, 31 leaves, max depth = 9, train loss: 0.03692, val loss: 0.06726, in 0.204s\n",
      "[645/3000] 1 tree, 31 leaves, max depth = 9, train loss: 0.03686, val loss: 0.06722, in 0.245s\n",
      "[646/3000] 1 tree, 31 leaves, max depth = 11, train loss: 0.03681, val loss: 0.06722, in 0.247s\n",
      "[647/3000] 1 tree, 31 leaves, max depth = 12, train loss: 0.03676, val loss: 0.06722, in 0.222s\n",
      "[648/3000] 1 tree, 31 leaves, max depth = 13, train loss: 0.03671, val loss: 0.06719, in 0.228s\n",
      "[649/3000] 1 tree, 31 leaves, max depth = 11, train loss: 0.03666, val loss: 0.06720, in 0.214s\n",
      "[650/3000] 1 tree, 31 leaves, max depth = 10, train loss: 0.03661, val loss: 0.06718, in 0.278s\n",
      "[651/3000] 1 tree, 31 leaves, max depth = 9, train loss: 0.03656, val loss: 0.06715, in 0.250s\n",
      "[652/3000] 1 tree, 31 leaves, max depth = 12, train loss: 0.03652, val loss: 0.06716, in 0.199s\n",
      "[653/3000] 1 tree, 31 leaves, max depth = 12, train loss: 0.03648, val loss: 0.06717, in 0.210s\n",
      "[654/3000] 1 tree, 31 leaves, max depth = 10, train loss: 0.03643, val loss: 0.06717, in 0.266s\n",
      "[655/3000] 1 tree, 31 leaves, max depth = 15, train loss: 0.03640, val loss: 0.06718, in 0.196s\n",
      "[656/3000] 1 tree, 31 leaves, max depth = 8, train loss: 0.03635, val loss: 0.06717, in 0.285s\n",
      "[657/3000] 1 tree, 31 leaves, max depth = 11, train loss: 0.03632, val loss: 0.06714, in 0.194s\n",
      "[658/3000] 1 tree, 31 leaves, max depth = 9, train loss: 0.03628, val loss: 0.06715, in 0.214s\n",
      "[659/3000] 1 tree, 31 leaves, max depth = 11, train loss: 0.03623, val loss: 0.06715, in 0.369s\n",
      "[660/3000] 1 tree, 31 leaves, max depth = 10, train loss: 0.03619, val loss: 0.06712, in 0.231s\n",
      "[661/3000] 1 tree, 31 leaves, max depth = 11, train loss: 0.03615, val loss: 0.06712, in 0.231s\n",
      "[662/3000] 1 tree, 31 leaves, max depth = 13, train loss: 0.03609, val loss: 0.06710, in 0.245s\n",
      "[663/3000] 1 tree, 31 leaves, max depth = 13, train loss: 0.03606, val loss: 0.06708, in 0.224s\n",
      "[664/3000] 1 tree, 31 leaves, max depth = 13, train loss: 0.03602, val loss: 0.06706, in 0.192s\n",
      "[665/3000] 1 tree, 31 leaves, max depth = 10, train loss: 0.03595, val loss: 0.06702, in 0.214s\n",
      "[666/3000] 1 tree, 31 leaves, max depth = 12, train loss: 0.03591, val loss: 0.06699, in 0.202s\n",
      "[667/3000] 1 tree, 31 leaves, max depth = 10, train loss: 0.03587, val loss: 0.06700, in 0.216s\n",
      "[668/3000] 1 tree, 31 leaves, max depth = 13, train loss: 0.03584, val loss: 0.06700, in 0.226s\n",
      "[669/3000] 1 tree, 31 leaves, max depth = 11, train loss: 0.03580, val loss: 0.06698, in 0.195s\n",
      "[670/3000] 1 tree, 31 leaves, max depth = 8, train loss: 0.03576, val loss: 0.06696, in 0.238s\n",
      "[671/3000] 1 tree, 31 leaves, max depth = 8, train loss: 0.03572, val loss: 0.06696, in 0.246s\n",
      "[672/3000] 1 tree, 31 leaves, max depth = 13, train loss: 0.03568, val loss: 0.06696, in 0.224s\n",
      "[673/3000] 1 tree, 31 leaves, max depth = 17, train loss: 0.03564, val loss: 0.06695, in 0.198s\n",
      "[674/3000] 1 tree, 31 leaves, max depth = 10, train loss: 0.03559, val loss: 0.06689, in 0.241s\n",
      "[675/3000] 1 tree, 31 leaves, max depth = 13, train loss: 0.03554, val loss: 0.06687, in 0.211s\n",
      "[676/3000] 1 tree, 31 leaves, max depth = 14, train loss: 0.03550, val loss: 0.06686, in 0.194s\n",
      "[677/3000] 1 tree, 31 leaves, max depth = 9, train loss: 0.03544, val loss: 0.06682, in 0.217s\n",
      "[678/3000] 1 tree, 31 leaves, max depth = 12, train loss: 0.03538, val loss: 0.06679, in 0.267s\n",
      "[679/3000] 1 tree, 31 leaves, max depth = 9, train loss: 0.03534, val loss: 0.06678, in 0.208s\n",
      "[680/3000] 1 tree, 31 leaves, max depth = 17, train loss: 0.03531, val loss: 0.06678, in 0.338s\n",
      "[681/3000] 1 tree, 31 leaves, max depth = 11, train loss: 0.03526, val loss: 0.06677, in 0.239s\n",
      "[682/3000] 1 tree, 31 leaves, max depth = 12, train loss: 0.03523, val loss: 0.06677, in 0.216s\n",
      "[683/3000] 1 tree, 31 leaves, max depth = 12, train loss: 0.03518, val loss: 0.06674, in 0.253s\n",
      "[684/3000] 1 tree, 31 leaves, max depth = 10, train loss: 0.03513, val loss: 0.06673, in 0.303s\n",
      "[685/3000] 1 tree, 31 leaves, max depth = 9, train loss: 0.03508, val loss: 0.06672, in 0.278s\n",
      "[686/3000] 1 tree, 31 leaves, max depth = 9, train loss: 0.03504, val loss: 0.06670, in 0.206s\n",
      "[687/3000] 1 tree, 31 leaves, max depth = 12, train loss: 0.03501, val loss: 0.06669, in 0.188s\n",
      "[688/3000] 1 tree, 31 leaves, max depth = 9, train loss: 0.03497, val loss: 0.06668, in 0.230s\n",
      "[689/3000] 1 tree, 31 leaves, max depth = 10, train loss: 0.03492, val loss: 0.06665, in 0.255s\n",
      "[690/3000] 1 tree, 31 leaves, max depth = 15, train loss: 0.03489, val loss: 0.06665, in 0.223s\n",
      "[691/3000] 1 tree, 31 leaves, max depth = 15, train loss: 0.03486, val loss: 0.06667, in 0.202s\n",
      "[692/3000] 1 tree, 31 leaves, max depth = 8, train loss: 0.03480, val loss: 0.06666, in 0.245s\n",
      "[693/3000] 1 tree, 31 leaves, max depth = 10, train loss: 0.03475, val loss: 0.06664, in 0.233s\n",
      "[694/3000] 1 tree, 31 leaves, max depth = 10, train loss: 0.03470, val loss: 0.06663, in 0.209s\n",
      "[695/3000] 1 tree, 31 leaves, max depth = 12, train loss: 0.03467, val loss: 0.06661, in 0.196s\n",
      "[696/3000] 1 tree, 31 leaves, max depth = 12, train loss: 0.03464, val loss: 0.06664, in 0.214s\n",
      "[697/3000] 1 tree, 31 leaves, max depth = 10, train loss: 0.03458, val loss: 0.06663, in 0.281s\n",
      "[698/3000] 1 tree, 31 leaves, max depth = 12, train loss: 0.03454, val loss: 0.06662, in 0.233s\n",
      "[699/3000] 1 tree, 31 leaves, max depth = 12, train loss: 0.03451, val loss: 0.06660, in 0.214s\n",
      "[700/3000] 1 tree, 31 leaves, max depth = 13, train loss: 0.03446, val loss: 0.06658, in 0.399s\n",
      "[701/3000] 1 tree, 31 leaves, max depth = 10, train loss: 0.03441, val loss: 0.06656, in 0.285s\n",
      "[702/3000] 1 tree, 31 leaves, max depth = 11, train loss: 0.03437, val loss: 0.06654, in 0.249s\n",
      "[703/3000] 1 tree, 31 leaves, max depth = 9, train loss: 0.03433, val loss: 0.06650, in 0.241s\n",
      "[704/3000] 1 tree, 31 leaves, max depth = 10, train loss: 0.03430, val loss: 0.06650, in 0.188s\n",
      "[705/3000] 1 tree, 31 leaves, max depth = 13, train loss: 0.03425, val loss: 0.06650, in 0.227s\n",
      "[706/3000] 1 tree, 31 leaves, max depth = 12, train loss: 0.03421, val loss: 0.06648, in 0.285s\n",
      "[707/3000] 1 tree, 31 leaves, max depth = 13, train loss: 0.03416, val loss: 0.06648, in 0.229s\n",
      "[708/3000] 1 tree, 31 leaves, max depth = 14, train loss: 0.03410, val loss: 0.06642, in 0.256s\n",
      "[709/3000] 1 tree, 31 leaves, max depth = 14, train loss: 0.03405, val loss: 0.06643, in 0.280s\n",
      "[710/3000] 1 tree, 31 leaves, max depth = 11, train loss: 0.03402, val loss: 0.06642, in 0.222s\n",
      "[711/3000] 1 tree, 31 leaves, max depth = 15, train loss: 0.03397, val loss: 0.06643, in 0.245s\n",
      "[712/3000] 1 tree, 31 leaves, max depth = 12, train loss: 0.03393, val loss: 0.06642, in 0.242s\n",
      "[713/3000] 1 tree, 31 leaves, max depth = 11, train loss: 0.03389, val loss: 0.06644, in 0.229s\n",
      "[714/3000] 1 tree, 31 leaves, max depth = 13, train loss: 0.03383, val loss: 0.06639, in 0.201s\n",
      "[715/3000] 1 tree, 31 leaves, max depth = 14, train loss: 0.03380, val loss: 0.06638, in 0.197s\n",
      "[716/3000] 1 tree, 31 leaves, max depth = 10, train loss: 0.03376, val loss: 0.06636, in 0.212s\n",
      "[717/3000] 1 tree, 31 leaves, max depth = 10, train loss: 0.03372, val loss: 0.06635, in 0.270s\n",
      "[718/3000] 1 tree, 31 leaves, max depth = 11, train loss: 0.03368, val loss: 0.06637, in 0.221s\n",
      "[719/3000] 1 tree, 31 leaves, max depth = 15, train loss: 0.03363, val loss: 0.06635, in 0.265s\n",
      "[720/3000] 1 tree, 31 leaves, max depth = 13, train loss: 0.03358, val loss: 0.06635, in 0.399s\n",
      "[721/3000] 1 tree, 31 leaves, max depth = 11, train loss: 0.03354, val loss: 0.06634, in 0.246s\n",
      "[722/3000] 1 tree, 31 leaves, max depth = 10, train loss: 0.03348, val loss: 0.06633, in 0.228s\n",
      "[723/3000] 1 tree, 31 leaves, max depth = 9, train loss: 0.03345, val loss: 0.06633, in 0.197s\n",
      "[724/3000] 1 tree, 31 leaves, max depth = 8, train loss: 0.03342, val loss: 0.06632, in 0.196s\n",
      "[725/3000] 1 tree, 31 leaves, max depth = 15, train loss: 0.03339, val loss: 0.06629, in 0.227s\n",
      "[726/3000] 1 tree, 31 leaves, max depth = 11, train loss: 0.03336, val loss: 0.06628, in 0.203s\n",
      "[727/3000] 1 tree, 31 leaves, max depth = 10, train loss: 0.03332, val loss: 0.06628, in 0.207s\n",
      "[728/3000] 1 tree, 31 leaves, max depth = 10, train loss: 0.03329, val loss: 0.06628, in 0.229s\n",
      "[729/3000] 1 tree, 31 leaves, max depth = 13, train loss: 0.03325, val loss: 0.06629, in 0.228s\n",
      "[730/3000] 1 tree, 31 leaves, max depth = 11, train loss: 0.03320, val loss: 0.06627, in 0.220s\n",
      "[731/3000] 1 tree, 31 leaves, max depth = 10, train loss: 0.03316, val loss: 0.06625, in 0.248s\n",
      "[732/3000] 1 tree, 31 leaves, max depth = 15, train loss: 0.03312, val loss: 0.06625, in 0.233s\n",
      "[733/3000] 1 tree, 31 leaves, max depth = 10, train loss: 0.03307, val loss: 0.06622, in 0.284s\n",
      "[734/3000] 1 tree, 31 leaves, max depth = 14, train loss: 0.03302, val loss: 0.06622, in 0.289s\n",
      "[735/3000] 1 tree, 31 leaves, max depth = 10, train loss: 0.03299, val loss: 0.06621, in 0.208s\n",
      "[736/3000] 1 tree, 31 leaves, max depth = 9, train loss: 0.03296, val loss: 0.06620, in 0.197s\n",
      "[737/3000] 1 tree, 31 leaves, max depth = 12, train loss: 0.03293, val loss: 0.06619, in 0.181s\n",
      "[738/3000] 1 tree, 31 leaves, max depth = 11, train loss: 0.03288, val loss: 0.06619, in 0.251s\n",
      "[739/3000] 1 tree, 31 leaves, max depth = 11, train loss: 0.03283, val loss: 0.06616, in 0.254s\n",
      "[740/3000] 1 tree, 31 leaves, max depth = 11, train loss: 0.03280, val loss: 0.06615, in 0.201s\n",
      "[741/3000] 1 tree, 31 leaves, max depth = 14, train loss: 0.03275, val loss: 0.06615, in 0.429s\n",
      "[742/3000] 1 tree, 31 leaves, max depth = 10, train loss: 0.03270, val loss: 0.06612, in 0.311s\n",
      "[743/3000] 1 tree, 31 leaves, max depth = 12, train loss: 0.03265, val loss: 0.06611, in 0.243s\n",
      "[744/3000] 1 tree, 31 leaves, max depth = 11, train loss: 0.03261, val loss: 0.06611, in 0.195s\n",
      "[745/3000] 1 tree, 31 leaves, max depth = 12, train loss: 0.03258, val loss: 0.06609, in 0.242s\n",
      "[746/3000] 1 tree, 31 leaves, max depth = 15, train loss: 0.03254, val loss: 0.06609, in 0.232s\n",
      "[747/3000] 1 tree, 31 leaves, max depth = 11, train loss: 0.03250, val loss: 0.06609, in 0.266s\n",
      "[748/3000] 1 tree, 31 leaves, max depth = 12, train loss: 0.03247, val loss: 0.06608, in 0.209s\n",
      "[749/3000] 1 tree, 31 leaves, max depth = 12, train loss: 0.03243, val loss: 0.06610, in 0.232s\n",
      "[750/3000] 1 tree, 31 leaves, max depth = 13, train loss: 0.03239, val loss: 0.06608, in 0.206s\n",
      "[751/3000] 1 tree, 31 leaves, max depth = 9, train loss: 0.03236, val loss: 0.06608, in 0.193s\n",
      "[752/3000] 1 tree, 31 leaves, max depth = 11, train loss: 0.03233, val loss: 0.06605, in 0.193s\n",
      "[753/3000] 1 tree, 31 leaves, max depth = 12, train loss: 0.03228, val loss: 0.06606, in 0.234s\n",
      "[754/3000] 1 tree, 31 leaves, max depth = 12, train loss: 0.03226, val loss: 0.06605, in 0.182s\n",
      "[755/3000] 1 tree, 31 leaves, max depth = 16, train loss: 0.03223, val loss: 0.06604, in 0.209s\n",
      "[756/3000] 1 tree, 31 leaves, max depth = 10, train loss: 0.03219, val loss: 0.06605, in 0.202s\n",
      "[757/3000] 1 tree, 31 leaves, max depth = 11, train loss: 0.03216, val loss: 0.06605, in 0.182s\n",
      "[758/3000] 1 tree, 31 leaves, max depth = 12, train loss: 0.03212, val loss: 0.06605, in 0.239s\n",
      "[759/3000] 1 tree, 31 leaves, max depth = 9, train loss: 0.03209, val loss: 0.06604, in 0.200s\n",
      "[760/3000] 1 tree, 31 leaves, max depth = 13, train loss: 0.03205, val loss: 0.06603, in 0.230s\n",
      "[761/3000] 1 tree, 31 leaves, max depth = 15, train loss: 0.03203, val loss: 0.06601, in 0.340s\n",
      "[762/3000] 1 tree, 31 leaves, max depth = 8, train loss: 0.03199, val loss: 0.06600, in 0.241s\n",
      "[763/3000] 1 tree, 31 leaves, max depth = 11, train loss: 0.03195, val loss: 0.06600, in 0.222s\n",
      "[764/3000] 1 tree, 31 leaves, max depth = 11, train loss: 0.03192, val loss: 0.06602, in 0.216s\n",
      "[765/3000] 1 tree, 31 leaves, max depth = 14, train loss: 0.03188, val loss: 0.06602, in 0.235s\n",
      "[766/3000] 1 tree, 31 leaves, max depth = 9, train loss: 0.03184, val loss: 0.06599, in 0.226s\n",
      "[767/3000] 1 tree, 31 leaves, max depth = 11, train loss: 0.03180, val loss: 0.06598, in 0.253s\n",
      "[768/3000] 1 tree, 31 leaves, max depth = 10, train loss: 0.03175, val loss: 0.06594, in 0.218s\n",
      "[769/3000] 1 tree, 31 leaves, max depth = 11, train loss: 0.03170, val loss: 0.06592, in 0.233s\n",
      "[770/3000] 1 tree, 31 leaves, max depth = 11, train loss: 0.03166, val loss: 0.06591, in 0.232s\n",
      "[771/3000] 1 tree, 31 leaves, max depth = 12, train loss: 0.03162, val loss: 0.06590, in 0.210s\n",
      "[772/3000] 1 tree, 31 leaves, max depth = 12, train loss: 0.03158, val loss: 0.06590, in 0.230s\n",
      "[773/3000] 1 tree, 31 leaves, max depth = 9, train loss: 0.03153, val loss: 0.06587, in 0.237s\n",
      "[774/3000] 1 tree, 31 leaves, max depth = 8, train loss: 0.03149, val loss: 0.06586, in 0.289s\n",
      "[775/3000] 1 tree, 31 leaves, max depth = 9, train loss: 0.03146, val loss: 0.06585, in 0.277s\n",
      "[776/3000] 1 tree, 31 leaves, max depth = 8, train loss: 0.03144, val loss: 0.06585, in 0.215s\n",
      "[777/3000] 1 tree, 31 leaves, max depth = 12, train loss: 0.03141, val loss: 0.06583, in 0.252s\n",
      "[778/3000] 1 tree, 31 leaves, max depth = 11, train loss: 0.03138, val loss: 0.06579, in 0.223s\n",
      "[779/3000] 1 tree, 31 leaves, max depth = 12, train loss: 0.03134, val loss: 0.06580, in 0.260s\n",
      "[780/3000] 1 tree, 31 leaves, max depth = 10, train loss: 0.03129, val loss: 0.06577, in 0.205s\n",
      "[781/3000] 1 tree, 31 leaves, max depth = 9, train loss: 0.03126, val loss: 0.06576, in 0.203s\n",
      "[782/3000] 1 tree, 31 leaves, max depth = 10, train loss: 0.03122, val loss: 0.06573, in 0.334s\n",
      "[783/3000] 1 tree, 31 leaves, max depth = 11, train loss: 0.03118, val loss: 0.06572, in 0.209s\n",
      "[784/3000] 1 tree, 31 leaves, max depth = 17, train loss: 0.03114, val loss: 0.06570, in 0.242s\n",
      "[785/3000] 1 tree, 31 leaves, max depth = 9, train loss: 0.03111, val loss: 0.06571, in 0.193s\n",
      "[786/3000] 1 tree, 31 leaves, max depth = 11, train loss: 0.03107, val loss: 0.06569, in 0.261s\n",
      "[787/3000] 1 tree, 31 leaves, max depth = 9, train loss: 0.03103, val loss: 0.06569, in 0.272s\n",
      "[788/3000] 1 tree, 31 leaves, max depth = 9, train loss: 0.03100, val loss: 0.06569, in 0.235s\n",
      "[789/3000] 1 tree, 31 leaves, max depth = 10, train loss: 0.03095, val loss: 0.06564, in 0.234s\n",
      "[790/3000] 1 tree, 31 leaves, max depth = 8, train loss: 0.03092, val loss: 0.06564, in 0.219s\n",
      "[791/3000] 1 tree, 31 leaves, max depth = 9, train loss: 0.03090, val loss: 0.06565, in 0.204s\n",
      "[792/3000] 1 tree, 31 leaves, max depth = 10, train loss: 0.03085, val loss: 0.06563, in 0.212s\n",
      "[793/3000] 1 tree, 31 leaves, max depth = 9, train loss: 0.03082, val loss: 0.06563, in 0.203s\n",
      "[794/3000] 1 tree, 31 leaves, max depth = 11, train loss: 0.03079, val loss: 0.06560, in 0.205s\n",
      "[795/3000] 1 tree, 31 leaves, max depth = 14, train loss: 0.03076, val loss: 0.06559, in 0.199s\n",
      "[796/3000] 1 tree, 31 leaves, max depth = 9, train loss: 0.03072, val loss: 0.06558, in 0.213s\n",
      "[797/3000] 1 tree, 31 leaves, max depth = 13, train loss: 0.03068, val loss: 0.06558, in 0.248s\n",
      "[798/3000] 1 tree, 31 leaves, max depth = 11, train loss: 0.03065, val loss: 0.06557, in 0.240s\n",
      "[799/3000] 1 tree, 31 leaves, max depth = 11, train loss: 0.03062, val loss: 0.06557, in 0.222s\n",
      "[800/3000] 1 tree, 31 leaves, max depth = 10, train loss: 0.03057, val loss: 0.06558, in 0.248s\n",
      "[801/3000] 1 tree, 31 leaves, max depth = 9, train loss: 0.03053, val loss: 0.06557, in 0.265s\n",
      "[802/3000] 1 tree, 31 leaves, max depth = 10, train loss: 0.03049, val loss: 0.06555, in 0.215s\n",
      "[803/3000] 1 tree, 31 leaves, max depth = 14, train loss: 0.03045, val loss: 0.06551, in 0.382s\n",
      "[804/3000] 1 tree, 31 leaves, max depth = 15, train loss: 0.03041, val loss: 0.06546, in 0.286s\n",
      "[805/3000] 1 tree, 31 leaves, max depth = 11, train loss: 0.03037, val loss: 0.06543, in 0.245s\n",
      "[806/3000] 1 tree, 31 leaves, max depth = 8, train loss: 0.03034, val loss: 0.06540, in 0.233s\n",
      "[807/3000] 1 tree, 31 leaves, max depth = 13, train loss: 0.03029, val loss: 0.06535, in 0.214s\n",
      "[808/3000] 1 tree, 31 leaves, max depth = 9, train loss: 0.03026, val loss: 0.06537, in 0.224s\n",
      "[809/3000] 1 tree, 31 leaves, max depth = 10, train loss: 0.03024, val loss: 0.06535, in 0.176s\n",
      "[810/3000] 1 tree, 31 leaves, max depth = 10, train loss: 0.03020, val loss: 0.06535, in 0.216s\n",
      "[811/3000] 1 tree, 31 leaves, max depth = 10, train loss: 0.03017, val loss: 0.06536, in 0.211s\n",
      "[812/3000] 1 tree, 31 leaves, max depth = 13, train loss: 0.03013, val loss: 0.06531, in 0.244s\n",
      "[813/3000] 1 tree, 31 leaves, max depth = 9, train loss: 0.03010, val loss: 0.06528, in 0.177s\n",
      "[814/3000] 1 tree, 31 leaves, max depth = 13, train loss: 0.03006, val loss: 0.06528, in 0.217s\n",
      "[815/3000] 1 tree, 31 leaves, max depth = 10, train loss: 0.03003, val loss: 0.06528, in 0.240s\n",
      "[816/3000] 1 tree, 31 leaves, max depth = 9, train loss: 0.02998, val loss: 0.06525, in 0.234s\n",
      "[817/3000] 1 tree, 31 leaves, max depth = 12, train loss: 0.02993, val loss: 0.06522, in 0.249s\n",
      "[818/3000] 1 tree, 31 leaves, max depth = 11, train loss: 0.02992, val loss: 0.06523, in 0.194s\n",
      "[819/3000] 1 tree, 31 leaves, max depth = 10, train loss: 0.02988, val loss: 0.06523, in 0.242s\n",
      "[820/3000] 1 tree, 31 leaves, max depth = 12, train loss: 0.02985, val loss: 0.06523, in 0.207s\n",
      "[821/3000] 1 tree, 31 leaves, max depth = 11, train loss: 0.02980, val loss: 0.06520, in 0.260s\n",
      "[822/3000] 1 tree, 31 leaves, max depth = 11, train loss: 0.02978, val loss: 0.06518, in 0.192s\n",
      "[823/3000] 1 tree, 31 leaves, max depth = 12, train loss: 0.02974, val loss: 0.06515, in 0.238s\n",
      "[824/3000] 1 tree, 31 leaves, max depth = 8, train loss: 0.02971, val loss: 0.06514, in 0.402s\n",
      "[825/3000] 1 tree, 31 leaves, max depth = 13, train loss: 0.02969, val loss: 0.06515, in 0.189s\n",
      "[826/3000] 1 tree, 31 leaves, max depth = 13, train loss: 0.02966, val loss: 0.06515, in 0.238s\n",
      "[827/3000] 1 tree, 31 leaves, max depth = 14, train loss: 0.02962, val loss: 0.06514, in 0.221s\n",
      "[828/3000] 1 tree, 31 leaves, max depth = 17, train loss: 0.02960, val loss: 0.06512, in 0.177s\n",
      "[829/3000] 1 tree, 31 leaves, max depth = 8, train loss: 0.02958, val loss: 0.06513, in 0.202s\n",
      "[830/3000] 1 tree, 31 leaves, max depth = 10, train loss: 0.02956, val loss: 0.06513, in 0.193s\n",
      "[831/3000] 1 tree, 31 leaves, max depth = 13, train loss: 0.02952, val loss: 0.06514, in 0.230s\n",
      "[832/3000] 1 tree, 31 leaves, max depth = 9, train loss: 0.02948, val loss: 0.06513, in 0.258s\n",
      "[833/3000] 1 tree, 31 leaves, max depth = 12, train loss: 0.02945, val loss: 0.06512, in 0.209s\n",
      "[834/3000] 1 tree, 31 leaves, max depth = 11, train loss: 0.02942, val loss: 0.06512, in 0.248s\n",
      "[835/3000] 1 tree, 31 leaves, max depth = 11, train loss: 0.02938, val loss: 0.06513, in 0.215s\n",
      "[836/3000] 1 tree, 31 leaves, max depth = 13, train loss: 0.02934, val loss: 0.06513, in 0.213s\n",
      "[837/3000] 1 tree, 31 leaves, max depth = 12, train loss: 0.02930, val loss: 0.06511, in 0.228s\n",
      "[838/3000] 1 tree, 31 leaves, max depth = 9, train loss: 0.02926, val loss: 0.06510, in 0.220s\n",
      "[839/3000] 1 tree, 31 leaves, max depth = 12, train loss: 0.02922, val loss: 0.06508, in 0.261s\n",
      "[840/3000] 1 tree, 31 leaves, max depth = 13, train loss: 0.02918, val loss: 0.06505, in 0.211s\n",
      "[841/3000] 1 tree, 31 leaves, max depth = 15, train loss: 0.02915, val loss: 0.06507, in 0.197s\n",
      "[842/3000] 1 tree, 31 leaves, max depth = 14, train loss: 0.02911, val loss: 0.06506, in 0.236s\n",
      "[843/3000] 1 tree, 31 leaves, max depth = 11, train loss: 0.02908, val loss: 0.06504, in 0.261s\n",
      "[844/3000] 1 tree, 31 leaves, max depth = 8, train loss: 0.02905, val loss: 0.06503, in 0.338s\n",
      "[845/3000] 1 tree, 31 leaves, max depth = 10, train loss: 0.02902, val loss: 0.06504, in 0.199s\n",
      "[846/3000] 1 tree, 31 leaves, max depth = 10, train loss: 0.02900, val loss: 0.06503, in 0.289s\n",
      "[847/3000] 1 tree, 31 leaves, max depth = 10, train loss: 0.02897, val loss: 0.06503, in 0.207s\n",
      "[848/3000] 1 tree, 31 leaves, max depth = 10, train loss: 0.02893, val loss: 0.06502, in 0.242s\n",
      "[849/3000] 1 tree, 31 leaves, max depth = 11, train loss: 0.02890, val loss: 0.06499, in 0.306s\n",
      "[850/3000] 1 tree, 31 leaves, max depth = 13, train loss: 0.02885, val loss: 0.06499, in 0.308s\n",
      "[851/3000] 1 tree, 31 leaves, max depth = 13, train loss: 0.02881, val loss: 0.06498, in 0.258s\n",
      "[852/3000] 1 tree, 31 leaves, max depth = 12, train loss: 0.02878, val loss: 0.06499, in 0.266s\n",
      "[853/3000] 1 tree, 31 leaves, max depth = 13, train loss: 0.02876, val loss: 0.06500, in 0.180s\n",
      "[854/3000] 1 tree, 31 leaves, max depth = 13, train loss: 0.02873, val loss: 0.06499, in 0.185s\n",
      "[855/3000] 1 tree, 31 leaves, max depth = 13, train loss: 0.02869, val loss: 0.06498, in 0.268s\n",
      "[856/3000] 1 tree, 31 leaves, max depth = 13, train loss: 0.02866, val loss: 0.06497, in 0.219s\n",
      "[857/3000] 1 tree, 31 leaves, max depth = 12, train loss: 0.02863, val loss: 0.06497, in 0.258s\n",
      "[858/3000] 1 tree, 31 leaves, max depth = 9, train loss: 0.02860, val loss: 0.06496, in 0.224s\n",
      "[859/3000] 1 tree, 31 leaves, max depth = 15, train loss: 0.02857, val loss: 0.06497, in 0.199s\n",
      "[860/3000] 1 tree, 31 leaves, max depth = 12, train loss: 0.02855, val loss: 0.06496, in 0.204s\n",
      "[861/3000] 1 tree, 31 leaves, max depth = 11, train loss: 0.02851, val loss: 0.06494, in 0.232s\n",
      "[862/3000] 1 tree, 31 leaves, max depth = 12, train loss: 0.02847, val loss: 0.06493, in 0.241s\n",
      "[863/3000] 1 tree, 31 leaves, max depth = 12, train loss: 0.02845, val loss: 0.06492, in 0.233s\n",
      "[864/3000] 1 tree, 31 leaves, max depth = 11, train loss: 0.02841, val loss: 0.06491, in 0.360s\n",
      "[865/3000] 1 tree, 31 leaves, max depth = 12, train loss: 0.02838, val loss: 0.06489, in 0.270s\n",
      "[866/3000] 1 tree, 31 leaves, max depth = 11, train loss: 0.02835, val loss: 0.06488, in 0.276s\n",
      "[867/3000] 1 tree, 31 leaves, max depth = 10, train loss: 0.02830, val loss: 0.06487, in 0.256s\n",
      "[868/3000] 1 tree, 31 leaves, max depth = 9, train loss: 0.02828, val loss: 0.06486, in 0.215s\n",
      "[869/3000] 1 tree, 31 leaves, max depth = 12, train loss: 0.02822, val loss: 0.06482, in 0.244s\n",
      "[870/3000] 1 tree, 31 leaves, max depth = 10, train loss: 0.02819, val loss: 0.06482, in 0.233s\n",
      "[871/3000] 1 tree, 31 leaves, max depth = 14, train loss: 0.02816, val loss: 0.06483, in 0.220s\n",
      "[872/3000] 1 tree, 31 leaves, max depth = 12, train loss: 0.02813, val loss: 0.06483, in 0.234s\n",
      "[873/3000] 1 tree, 31 leaves, max depth = 15, train loss: 0.02810, val loss: 0.06483, in 0.189s\n",
      "[874/3000] 1 tree, 31 leaves, max depth = 12, train loss: 0.02806, val loss: 0.06479, in 0.243s\n",
      "[875/3000] 1 tree, 31 leaves, max depth = 9, train loss: 0.02803, val loss: 0.06478, in 0.236s\n",
      "[876/3000] 1 tree, 31 leaves, max depth = 10, train loss: 0.02801, val loss: 0.06476, in 0.209s\n",
      "[877/3000] 1 tree, 31 leaves, max depth = 11, train loss: 0.02797, val loss: 0.06474, in 0.243s\n",
      "[878/3000] 1 tree, 31 leaves, max depth = 9, train loss: 0.02794, val loss: 0.06472, in 0.236s\n",
      "[879/3000] 1 tree, 31 leaves, max depth = 11, train loss: 0.02790, val loss: 0.06470, in 0.218s\n",
      "[880/3000] 1 tree, 31 leaves, max depth = 12, train loss: 0.02788, val loss: 0.06468, in 0.194s\n",
      "[881/3000] 1 tree, 31 leaves, max depth = 10, train loss: 0.02783, val loss: 0.06468, in 0.286s\n",
      "[882/3000] 1 tree, 31 leaves, max depth = 14, train loss: 0.02780, val loss: 0.06467, in 0.252s\n",
      "[883/3000] 1 tree, 31 leaves, max depth = 10, train loss: 0.02777, val loss: 0.06465, in 0.210s\n",
      "[884/3000] 1 tree, 31 leaves, max depth = 11, train loss: 0.02774, val loss: 0.06465, in 0.356s\n",
      "[885/3000] 1 tree, 31 leaves, max depth = 11, train loss: 0.02771, val loss: 0.06464, in 0.250s\n",
      "[886/3000] 1 tree, 31 leaves, max depth = 9, train loss: 0.02769, val loss: 0.06464, in 0.218s\n",
      "[887/3000] 1 tree, 31 leaves, max depth = 10, train loss: 0.02767, val loss: 0.06464, in 0.191s\n",
      "[888/3000] 1 tree, 31 leaves, max depth = 11, train loss: 0.02763, val loss: 0.06462, in 0.219s\n",
      "[889/3000] 1 tree, 31 leaves, max depth = 9, train loss: 0.02760, val loss: 0.06461, in 0.192s\n",
      "[890/3000] 1 tree, 31 leaves, max depth = 18, train loss: 0.02758, val loss: 0.06460, in 0.206s\n",
      "[891/3000] 1 tree, 31 leaves, max depth = 18, train loss: 0.02755, val loss: 0.06459, in 0.208s\n",
      "[892/3000] 1 tree, 31 leaves, max depth = 11, train loss: 0.02752, val loss: 0.06459, in 0.265s\n",
      "[893/3000] 1 tree, 31 leaves, max depth = 12, train loss: 0.02749, val loss: 0.06457, in 0.191s\n",
      "[894/3000] 1 tree, 31 leaves, max depth = 12, train loss: 0.02745, val loss: 0.06455, in 0.302s\n",
      "[895/3000] 1 tree, 31 leaves, max depth = 9, train loss: 0.02741, val loss: 0.06452, in 0.244s\n",
      "[896/3000] 1 tree, 31 leaves, max depth = 10, train loss: 0.02738, val loss: 0.06451, in 0.220s\n",
      "[897/3000] 1 tree, 31 leaves, max depth = 9, train loss: 0.02733, val loss: 0.06448, in 0.276s\n",
      "[898/3000] 1 tree, 31 leaves, max depth = 11, train loss: 0.02731, val loss: 0.06446, in 0.198s\n",
      "[899/3000] 1 tree, 31 leaves, max depth = 11, train loss: 0.02728, val loss: 0.06446, in 0.203s\n",
      "[900/3000] 1 tree, 31 leaves, max depth = 11, train loss: 0.02725, val loss: 0.06442, in 0.263s\n",
      "[901/3000] 1 tree, 31 leaves, max depth = 11, train loss: 0.02723, val loss: 0.06441, in 0.176s\n",
      "[902/3000] 1 tree, 31 leaves, max depth = 10, train loss: 0.02720, val loss: 0.06441, in 0.203s\n",
      "[903/3000] 1 tree, 31 leaves, max depth = 9, train loss: 0.02718, val loss: 0.06440, in 0.233s\n",
      "[904/3000] 1 tree, 31 leaves, max depth = 10, train loss: 0.02715, val loss: 0.06439, in 0.210s\n",
      "[905/3000] 1 tree, 31 leaves, max depth = 14, train loss: 0.02712, val loss: 0.06438, in 0.358s\n",
      "[906/3000] 1 tree, 31 leaves, max depth = 11, train loss: 0.02709, val loss: 0.06438, in 0.230s\n",
      "[907/3000] 1 tree, 31 leaves, max depth = 10, train loss: 0.02706, val loss: 0.06439, in 0.250s\n",
      "[908/3000] 1 tree, 31 leaves, max depth = 10, train loss: 0.02703, val loss: 0.06439, in 0.203s\n",
      "[909/3000] 1 tree, 31 leaves, max depth = 13, train loss: 0.02700, val loss: 0.06439, in 0.263s\n",
      "[910/3000] 1 tree, 31 leaves, max depth = 14, train loss: 0.02697, val loss: 0.06438, in 0.268s\n",
      "[911/3000] 1 tree, 31 leaves, max depth = 13, train loss: 0.02694, val loss: 0.06438, in 0.216s\n",
      "[912/3000] 1 tree, 31 leaves, max depth = 11, train loss: 0.02690, val loss: 0.06435, in 0.241s\n",
      "[913/3000] 1 tree, 31 leaves, max depth = 11, train loss: 0.02688, val loss: 0.06433, in 0.234s\n",
      "[914/3000] 1 tree, 31 leaves, max depth = 12, train loss: 0.02684, val loss: 0.06435, in 0.237s\n",
      "[915/3000] 1 tree, 31 leaves, max depth = 9, train loss: 0.02679, val loss: 0.06432, in 0.281s\n",
      "[916/3000] 1 tree, 31 leaves, max depth = 10, train loss: 0.02676, val loss: 0.06432, in 0.227s\n",
      "[917/3000] 1 tree, 31 leaves, max depth = 17, train loss: 0.02673, val loss: 0.06432, in 0.196s\n",
      "[918/3000] 1 tree, 31 leaves, max depth = 13, train loss: 0.02670, val loss: 0.06431, in 0.253s\n",
      "[919/3000] 1 tree, 31 leaves, max depth = 14, train loss: 0.02666, val loss: 0.06430, in 0.209s\n",
      "[920/3000] 1 tree, 31 leaves, max depth = 11, train loss: 0.02662, val loss: 0.06427, in 0.275s\n",
      "[921/3000] 1 tree, 31 leaves, max depth = 11, train loss: 0.02659, val loss: 0.06428, in 0.280s\n",
      "[922/3000] 1 tree, 31 leaves, max depth = 10, train loss: 0.02656, val loss: 0.06428, in 0.235s\n",
      "[923/3000] 1 tree, 31 leaves, max depth = 12, train loss: 0.02653, val loss: 0.06425, in 0.269s\n",
      "[924/3000] 1 tree, 31 leaves, max depth = 10, train loss: 0.02648, val loss: 0.06424, in 0.241s\n",
      "[925/3000] 1 tree, 31 leaves, max depth = 14, train loss: 0.02644, val loss: 0.06422, in 0.216s\n",
      "[926/3000] 1 tree, 31 leaves, max depth = 11, train loss: 0.02641, val loss: 0.06421, in 0.358s\n",
      "[927/3000] 1 tree, 31 leaves, max depth = 8, train loss: 0.02637, val loss: 0.06419, in 0.302s\n",
      "[928/3000] 1 tree, 31 leaves, max depth = 12, train loss: 0.02634, val loss: 0.06416, in 0.241s\n",
      "[929/3000] 1 tree, 31 leaves, max depth = 14, train loss: 0.02631, val loss: 0.06417, in 0.211s\n",
      "[930/3000] 1 tree, 31 leaves, max depth = 9, train loss: 0.02628, val loss: 0.06417, in 0.237s\n",
      "[931/3000] 1 tree, 31 leaves, max depth = 10, train loss: 0.02624, val loss: 0.06416, in 0.255s\n",
      "[932/3000] 1 tree, 31 leaves, max depth = 9, train loss: 0.02622, val loss: 0.06417, in 0.210s\n",
      "[933/3000] 1 tree, 31 leaves, max depth = 12, train loss: 0.02619, val loss: 0.06417, in 0.215s\n",
      "[934/3000] 1 tree, 31 leaves, max depth = 10, train loss: 0.02617, val loss: 0.06415, in 0.187s\n",
      "[935/3000] 1 tree, 31 leaves, max depth = 10, train loss: 0.02615, val loss: 0.06414, in 0.200s\n",
      "[936/3000] 1 tree, 31 leaves, max depth = 13, train loss: 0.02612, val loss: 0.06413, in 0.218s\n",
      "[937/3000] 1 tree, 31 leaves, max depth = 11, train loss: 0.02608, val loss: 0.06411, in 0.254s\n",
      "[938/3000] 1 tree, 31 leaves, max depth = 16, train loss: 0.02606, val loss: 0.06411, in 0.206s\n",
      "[939/3000] 1 tree, 31 leaves, max depth = 11, train loss: 0.02603, val loss: 0.06411, in 0.200s\n",
      "[940/3000] 1 tree, 31 leaves, max depth = 10, train loss: 0.02600, val loss: 0.06410, in 0.239s\n",
      "[941/3000] 1 tree, 31 leaves, max depth = 11, train loss: 0.02597, val loss: 0.06409, in 0.238s\n",
      "[942/3000] 1 tree, 31 leaves, max depth = 11, train loss: 0.02594, val loss: 0.06409, in 0.253s\n",
      "[943/3000] 1 tree, 31 leaves, max depth = 12, train loss: 0.02591, val loss: 0.06409, in 0.257s\n",
      "[944/3000] 1 tree, 31 leaves, max depth = 13, train loss: 0.02589, val loss: 0.06409, in 0.187s\n",
      "[945/3000] 1 tree, 31 leaves, max depth = 11, train loss: 0.02587, val loss: 0.06408, in 0.225s\n",
      "[946/3000] 1 tree, 31 leaves, max depth = 13, train loss: 0.02585, val loss: 0.06410, in 0.181s\n",
      "[947/3000] 1 tree, 31 leaves, max depth = 9, train loss: 0.02583, val loss: 0.06411, in 0.339s\n",
      "[948/3000] 1 tree, 31 leaves, max depth = 12, train loss: 0.02578, val loss: 0.06408, in 0.314s\n",
      "[949/3000] 1 tree, 31 leaves, max depth = 14, train loss: 0.02575, val loss: 0.06407, in 0.198s\n",
      "[950/3000] 1 tree, 31 leaves, max depth = 12, train loss: 0.02571, val loss: 0.06406, in 0.261s\n",
      "[951/3000] 1 tree, 31 leaves, max depth = 12, train loss: 0.02568, val loss: 0.06403, in 0.228s\n",
      "[952/3000] 1 tree, 31 leaves, max depth = 15, train loss: 0.02566, val loss: 0.06404, in 0.199s\n",
      "[953/3000] 1 tree, 31 leaves, max depth = 10, train loss: 0.02564, val loss: 0.06402, in 0.236s\n",
      "[954/3000] 1 tree, 31 leaves, max depth = 13, train loss: 0.02561, val loss: 0.06401, in 0.207s\n",
      "[955/3000] 1 tree, 31 leaves, max depth = 10, train loss: 0.02558, val loss: 0.06400, in 0.230s\n",
      "[956/3000] 1 tree, 31 leaves, max depth = 11, train loss: 0.02556, val loss: 0.06400, in 0.212s\n",
      "[957/3000] 1 tree, 31 leaves, max depth = 9, train loss: 0.02552, val loss: 0.06398, in 0.292s\n",
      "[958/3000] 1 tree, 31 leaves, max depth = 10, train loss: 0.02550, val loss: 0.06398, in 0.191s\n",
      "[959/3000] 1 tree, 31 leaves, max depth = 11, train loss: 0.02548, val loss: 0.06397, in 0.202s\n",
      "[960/3000] 1 tree, 31 leaves, max depth = 9, train loss: 0.02545, val loss: 0.06397, in 0.226s\n",
      "[961/3000] 1 tree, 31 leaves, max depth = 8, train loss: 0.02541, val loss: 0.06395, in 0.246s\n",
      "[962/3000] 1 tree, 31 leaves, max depth = 10, train loss: 0.02539, val loss: 0.06395, in 0.249s\n",
      "[963/3000] 1 tree, 31 leaves, max depth = 13, train loss: 0.02536, val loss: 0.06393, in 0.196s\n",
      "[964/3000] 1 tree, 31 leaves, max depth = 11, train loss: 0.02533, val loss: 0.06392, in 0.218s\n",
      "[965/3000] 1 tree, 31 leaves, max depth = 12, train loss: 0.02529, val loss: 0.06391, in 0.239s\n",
      "[966/3000] 1 tree, 31 leaves, max depth = 9, train loss: 0.02527, val loss: 0.06390, in 0.215s\n",
      "[967/3000] 1 tree, 31 leaves, max depth = 11, train loss: 0.02525, val loss: 0.06389, in 0.336s\n",
      "[968/3000] 1 tree, 31 leaves, max depth = 10, train loss: 0.02522, val loss: 0.06388, in 0.237s\n",
      "[969/3000] 1 tree, 31 leaves, max depth = 9, train loss: 0.02520, val loss: 0.06385, in 0.213s\n",
      "[970/3000] 1 tree, 31 leaves, max depth = 12, train loss: 0.02517, val loss: 0.06385, in 0.203s\n",
      "[971/3000] 1 tree, 31 leaves, max depth = 15, train loss: 0.02515, val loss: 0.06385, in 0.196s\n",
      "[972/3000] 1 tree, 31 leaves, max depth = 14, train loss: 0.02512, val loss: 0.06384, in 0.229s\n",
      "[973/3000] 1 tree, 31 leaves, max depth = 12, train loss: 0.02510, val loss: 0.06386, in 0.228s\n",
      "[974/3000] 1 tree, 31 leaves, max depth = 11, train loss: 0.02507, val loss: 0.06385, in 0.209s\n",
      "[975/3000] 1 tree, 31 leaves, max depth = 14, train loss: 0.02503, val loss: 0.06383, in 0.261s\n",
      "[976/3000] 1 tree, 31 leaves, max depth = 13, train loss: 0.02499, val loss: 0.06384, in 0.212s\n",
      "[977/3000] 1 tree, 31 leaves, max depth = 12, train loss: 0.02496, val loss: 0.06384, in 0.335s\n",
      "[978/3000] 1 tree, 31 leaves, max depth = 13, train loss: 0.02493, val loss: 0.06383, in 0.225s\n",
      "[979/3000] 1 tree, 31 leaves, max depth = 11, train loss: 0.02491, val loss: 0.06385, in 0.200s\n",
      "[980/3000] 1 tree, 31 leaves, max depth = 8, train loss: 0.02489, val loss: 0.06385, in 0.213s\n",
      "[981/3000] 1 tree, 31 leaves, max depth = 15, train loss: 0.02486, val loss: 0.06385, in 0.258s\n",
      "[982/3000] 1 tree, 31 leaves, max depth = 10, train loss: 0.02484, val loss: 0.06384, in 0.212s\n",
      "[983/3000] 1 tree, 31 leaves, max depth = 9, train loss: 0.02481, val loss: 0.06383, in 0.246s\n",
      "[984/3000] 1 tree, 31 leaves, max depth = 12, train loss: 0.02477, val loss: 0.06378, in 0.228s\n",
      "[985/3000] 1 tree, 31 leaves, max depth = 9, train loss: 0.02474, val loss: 0.06379, in 0.251s\n",
      "[986/3000] 1 tree, 31 leaves, max depth = 10, train loss: 0.02471, val loss: 0.06381, in 0.262s\n",
      "[987/3000] 1 tree, 31 leaves, max depth = 13, train loss: 0.02468, val loss: 0.06379, in 0.378s\n",
      "[988/3000] 1 tree, 31 leaves, max depth = 17, train loss: 0.02465, val loss: 0.06376, in 0.273s\n",
      "[989/3000] 1 tree, 31 leaves, max depth = 9, train loss: 0.02463, val loss: 0.06376, in 0.479s\n",
      "[990/3000] 1 tree, 31 leaves, max depth = 12, train loss: 0.02460, val loss: 0.06375, in 0.229s\n",
      "[991/3000] 1 tree, 31 leaves, max depth = 16, train loss: 0.02457, val loss: 0.06376, in 0.224s\n",
      "[992/3000] 1 tree, 31 leaves, max depth = 17, train loss: 0.02453, val loss: 0.06376, in 0.257s\n",
      "[993/3000] 1 tree, 31 leaves, max depth = 16, train loss: 0.02451, val loss: 0.06376, in 0.229s\n",
      "[994/3000] 1 tree, 31 leaves, max depth = 10, train loss: 0.02448, val loss: 0.06375, in 0.243s\n",
      "[995/3000] 1 tree, 31 leaves, max depth = 10, train loss: 0.02443, val loss: 0.06372, in 0.298s\n",
      "[996/3000] 1 tree, 31 leaves, max depth = 13, train loss: 0.02437, val loss: 0.06365, in 0.299s\n",
      "[997/3000] 1 tree, 31 leaves, max depth = 10, train loss: 0.02433, val loss: 0.06365, in 0.205s\n",
      "[998/3000] 1 tree, 31 leaves, max depth = 16, train loss: 0.02431, val loss: 0.06364, in 0.208s\n",
      "[999/3000] 1 tree, 31 leaves, max depth = 10, train loss: 0.02429, val loss: 0.06365, in 0.208s\n",
      "[1000/3000] 1 tree, 31 leaves, max depth = 12, train loss: 0.02426, val loss: 0.06366, in 0.191s\n",
      "[1001/3000] 1 tree, 31 leaves, max depth = 11, train loss: 0.02425, val loss: 0.06366, in 0.213s\n",
      "[1002/3000] 1 tree, 31 leaves, max depth = 14, train loss: 0.02422, val loss: 0.06365, in 0.242s\n",
      "[1003/3000] 1 tree, 31 leaves, max depth = 11, train loss: 0.02419, val loss: 0.06365, in 0.212s\n",
      "[1004/3000] 1 tree, 31 leaves, max depth = 13, train loss: 0.02416, val loss: 0.06363, in 0.227s\n",
      "[1005/3000] 1 tree, 31 leaves, max depth = 11, train loss: 0.02413, val loss: 0.06362, in 0.221s\n",
      "[1006/3000] 1 tree, 31 leaves, max depth = 10, train loss: 0.02410, val loss: 0.06362, in 0.241s\n",
      "[1007/3000] 1 tree, 31 leaves, max depth = 10, train loss: 0.02408, val loss: 0.06361, in 0.198s\n",
      "[1008/3000] 1 tree, 31 leaves, max depth = 15, train loss: 0.02405, val loss: 0.06361, in 0.209s\n",
      "[1009/3000] 1 tree, 31 leaves, max depth = 10, train loss: 0.02401, val loss: 0.06359, in 0.348s\n",
      "[1010/3000] 1 tree, 31 leaves, max depth = 14, train loss: 0.02399, val loss: 0.06360, in 0.224s\n",
      "[1011/3000] 1 tree, 31 leaves, max depth = 11, train loss: 0.02396, val loss: 0.06358, in 0.223s\n",
      "[1012/3000] 1 tree, 31 leaves, max depth = 12, train loss: 0.02392, val loss: 0.06355, in 0.289s\n",
      "[1013/3000] 1 tree, 31 leaves, max depth = 14, train loss: 0.02388, val loss: 0.06355, in 0.270s\n",
      "[1014/3000] 1 tree, 31 leaves, max depth = 11, train loss: 0.02385, val loss: 0.06353, in 0.228s\n",
      "[1015/3000] 1 tree, 31 leaves, max depth = 9, train loss: 0.02383, val loss: 0.06353, in 0.208s\n",
      "[1016/3000] 1 tree, 31 leaves, max depth = 9, train loss: 0.02381, val loss: 0.06350, in 0.213s\n",
      "[1017/3000] 1 tree, 31 leaves, max depth = 10, train loss: 0.02378, val loss: 0.06350, in 0.205s\n",
      "[1018/3000] 1 tree, 31 leaves, max depth = 10, train loss: 0.02377, val loss: 0.06348, in 0.196s\n",
      "[1019/3000] 1 tree, 31 leaves, max depth = 13, train loss: 0.02374, val loss: 0.06347, in 0.194s\n",
      "[1020/3000] 1 tree, 31 leaves, max depth = 11, train loss: 0.02371, val loss: 0.06344, in 0.240s\n",
      "[1021/3000] 1 tree, 31 leaves, max depth = 15, train loss: 0.02368, val loss: 0.06343, in 0.230s\n",
      "[1022/3000] 1 tree, 31 leaves, max depth = 10, train loss: 0.02364, val loss: 0.06338, in 0.276s\n",
      "[1023/3000] 1 tree, 31 leaves, max depth = 9, train loss: 0.02361, val loss: 0.06338, in 0.192s\n",
      "[1024/3000] 1 tree, 31 leaves, max depth = 11, train loss: 0.02359, val loss: 0.06338, in 0.195s\n",
      "[1025/3000] 1 tree, 31 leaves, max depth = 13, train loss: 0.02356, val loss: 0.06337, in 0.263s\n",
      "[1026/3000] 1 tree, 31 leaves, max depth = 10, train loss: 0.02353, val loss: 0.06337, in 0.254s\n",
      "[1027/3000] 1 tree, 31 leaves, max depth = 9, train loss: 0.02349, val loss: 0.06334, in 0.298s\n",
      "[1028/3000] 1 tree, 31 leaves, max depth = 8, train loss: 0.02347, val loss: 0.06335, in 0.252s\n",
      "[1029/3000] 1 tree, 31 leaves, max depth = 9, train loss: 0.02344, val loss: 0.06332, in 0.234s\n",
      "[1030/3000] 1 tree, 31 leaves, max depth = 10, train loss: 0.02341, val loss: 0.06332, in 0.431s\n",
      "[1031/3000] 1 tree, 31 leaves, max depth = 11, train loss: 0.02338, val loss: 0.06331, in 0.280s\n",
      "[1032/3000] 1 tree, 31 leaves, max depth = 12, train loss: 0.02336, val loss: 0.06331, in 0.239s\n",
      "[1033/3000] 1 tree, 31 leaves, max depth = 15, train loss: 0.02334, val loss: 0.06330, in 0.221s\n",
      "[1034/3000] 1 tree, 31 leaves, max depth = 11, train loss: 0.02331, val loss: 0.06330, in 0.217s\n",
      "[1035/3000] 1 tree, 31 leaves, max depth = 10, train loss: 0.02329, val loss: 0.06330, in 0.217s\n",
      "[1036/3000] 1 tree, 31 leaves, max depth = 10, train loss: 0.02326, val loss: 0.06329, in 0.231s\n",
      "[1037/3000] 1 tree, 31 leaves, max depth = 14, train loss: 0.02323, val loss: 0.06328, in 0.227s\n",
      "[1038/3000] 1 tree, 31 leaves, max depth = 16, train loss: 0.02321, val loss: 0.06327, in 0.187s\n",
      "[1039/3000] 1 tree, 31 leaves, max depth = 11, train loss: 0.02319, val loss: 0.06329, in 0.207s\n",
      "[1040/3000] 1 tree, 31 leaves, max depth = 9, train loss: 0.02316, val loss: 0.06330, in 0.216s\n",
      "[1041/3000] 1 tree, 31 leaves, max depth = 10, train loss: 0.02313, val loss: 0.06331, in 0.260s\n",
      "[1042/3000] 1 tree, 31 leaves, max depth = 10, train loss: 0.02311, val loss: 0.06331, in 0.209s\n",
      "[1043/3000] 1 tree, 31 leaves, max depth = 10, train loss: 0.02309, val loss: 0.06330, in 0.248s\n",
      "[1044/3000] 1 tree, 31 leaves, max depth = 14, train loss: 0.02306, val loss: 0.06332, in 0.293s\n",
      "[1045/3000] 1 tree, 31 leaves, max depth = 12, train loss: 0.02303, val loss: 0.06330, in 0.261s\n",
      "[1046/3000] 1 tree, 31 leaves, max depth = 17, train loss: 0.02300, val loss: 0.06330, in 0.230s\n",
      "[1047/3000] 1 tree, 31 leaves, max depth = 9, train loss: 0.02299, val loss: 0.06329, in 0.213s\n",
      "[1048/3000] 1 tree, 31 leaves, max depth = 11, train loss: 0.02297, val loss: 0.06327, in 0.217s\n",
      "[1049/3000] 1 tree, 31 leaves, max depth = 10, train loss: 0.02295, val loss: 0.06328, in 0.190s\n",
      "[1050/3000] 1 tree, 31 leaves, max depth = 10, train loss: 0.02292, val loss: 0.06327, in 0.371s\n",
      "[1051/3000] 1 tree, 31 leaves, max depth = 9, train loss: 0.02291, val loss: 0.06327, in 0.236s\n",
      "[1052/3000] 1 tree, 31 leaves, max depth = 11, train loss: 0.02288, val loss: 0.06325, in 0.288s\n",
      "[1053/3000] 1 tree, 31 leaves, max depth = 10, train loss: 0.02287, val loss: 0.06323, in 0.187s\n",
      "[1054/3000] 1 tree, 31 leaves, max depth = 10, train loss: 0.02285, val loss: 0.06323, in 0.249s\n",
      "[1055/3000] 1 tree, 31 leaves, max depth = 10, train loss: 0.02282, val loss: 0.06322, in 0.207s\n",
      "[1056/3000] 1 tree, 31 leaves, max depth = 10, train loss: 0.02279, val loss: 0.06321, in 0.211s\n",
      "[1057/3000] 1 tree, 31 leaves, max depth = 13, train loss: 0.02276, val loss: 0.06321, in 0.224s\n",
      "[1058/3000] 1 tree, 31 leaves, max depth = 10, train loss: 0.02274, val loss: 0.06320, in 0.217s\n",
      "[1059/3000] 1 tree, 31 leaves, max depth = 15, train loss: 0.02271, val loss: 0.06320, in 0.203s\n",
      "[1060/3000] 1 tree, 31 leaves, max depth = 10, train loss: 0.02269, val loss: 0.06319, in 0.201s\n",
      "[1061/3000] 1 tree, 31 leaves, max depth = 11, train loss: 0.02266, val loss: 0.06318, in 0.211s\n",
      "[1062/3000] 1 tree, 31 leaves, max depth = 11, train loss: 0.02263, val loss: 0.06318, in 0.268s\n",
      "[1063/3000] 1 tree, 31 leaves, max depth = 9, train loss: 0.02260, val loss: 0.06318, in 0.283s\n",
      "[1064/3000] 1 tree, 31 leaves, max depth = 9, train loss: 0.02258, val loss: 0.06319, in 0.251s\n",
      "[1065/3000] 1 tree, 31 leaves, max depth = 12, train loss: 0.02255, val loss: 0.06319, in 0.208s\n",
      "[1066/3000] 1 tree, 31 leaves, max depth = 14, train loss: 0.02253, val loss: 0.06318, in 0.201s\n",
      "[1067/3000] 1 tree, 31 leaves, max depth = 11, train loss: 0.02251, val loss: 0.06319, in 0.206s\n",
      "[1068/3000] 1 tree, 31 leaves, max depth = 11, train loss: 0.02248, val loss: 0.06320, in 0.195s\n",
      "[1069/3000] 1 tree, 31 leaves, max depth = 9, train loss: 0.02247, val loss: 0.06319, in 0.187s\n",
      "[1070/3000] 1 tree, 31 leaves, max depth = 11, train loss: 0.02245, val loss: 0.06318, in 0.241s\n",
      "[1071/3000] 1 tree, 31 leaves, max depth = 12, train loss: 0.02242, val loss: 0.06317, in 0.367s\n",
      "[1072/3000] 1 tree, 31 leaves, max depth = 13, train loss: 0.02239, val loss: 0.06318, in 0.267s\n",
      "[1073/3000] 1 tree, 31 leaves, max depth = 12, train loss: 0.02236, val loss: 0.06318, in 0.274s\n",
      "[1074/3000] 1 tree, 31 leaves, max depth = 16, train loss: 0.02234, val loss: 0.06317, in 0.247s\n",
      "[1075/3000] 1 tree, 31 leaves, max depth = 11, train loss: 0.02232, val loss: 0.06318, in 0.245s\n",
      "[1076/3000] 1 tree, 31 leaves, max depth = 12, train loss: 0.02229, val loss: 0.06318, in 0.228s\n",
      "[1077/3000] 1 tree, 31 leaves, max depth = 13, train loss: 0.02228, val loss: 0.06319, in 0.198s\n",
      "[1078/3000] 1 tree, 31 leaves, max depth = 9, train loss: 0.02226, val loss: 0.06319, in 0.225s\n",
      "[1079/3000] 1 tree, 31 leaves, max depth = 11, train loss: 0.02224, val loss: 0.06320, in 0.205s\n",
      "[1080/3000] 1 tree, 31 leaves, max depth = 12, train loss: 0.02221, val loss: 0.06320, in 0.232s\n",
      "[1081/3000] 1 tree, 31 leaves, max depth = 14, train loss: 0.02219, val loss: 0.06321, in 0.205s\n",
      "[1082/3000] 1 tree, 31 leaves, max depth = 16, train loss: 0.02216, val loss: 0.06321, in 0.209s\n",
      "[1083/3000] 1 tree, 31 leaves, max depth = 11, train loss: 0.02214, val loss: 0.06320, in 0.194s\n",
      "[1084/3000] 1 tree, 31 leaves, max depth = 12, train loss: 0.02212, val loss: 0.06318, in 0.209s\n",
      "[1085/3000] 1 tree, 31 leaves, max depth = 11, train loss: 0.02209, val loss: 0.06318, in 0.207s\n",
      "[1086/3000] 1 tree, 31 leaves, max depth = 11, train loss: 0.02207, val loss: 0.06316, in 0.229s\n",
      "[1087/3000] 1 tree, 31 leaves, max depth = 10, train loss: 0.02204, val loss: 0.06315, in 0.236s\n",
      "[1088/3000] 1 tree, 31 leaves, max depth = 13, train loss: 0.02201, val loss: 0.06312, in 0.205s\n",
      "[1089/3000] 1 tree, 31 leaves, max depth = 13, train loss: 0.02198, val loss: 0.06312, in 0.227s\n",
      "[1090/3000] 1 tree, 31 leaves, max depth = 11, train loss: 0.02196, val loss: 0.06312, in 0.232s\n",
      "[1091/3000] 1 tree, 31 leaves, max depth = 13, train loss: 0.02192, val loss: 0.06312, in 0.257s\n",
      "[1092/3000] 1 tree, 31 leaves, max depth = 10, train loss: 0.02189, val loss: 0.06314, in 0.417s\n",
      "[1093/3000] 1 tree, 31 leaves, max depth = 17, train loss: 0.02187, val loss: 0.06313, in 0.268s\n",
      "[1094/3000] 1 tree, 31 leaves, max depth = 12, train loss: 0.02185, val loss: 0.06314, in 0.260s\n",
      "[1095/3000] 1 tree, 31 leaves, max depth = 12, train loss: 0.02182, val loss: 0.06316, in 0.187s\n",
      "[1096/3000] 1 tree, 31 leaves, max depth = 10, train loss: 0.02179, val loss: 0.06313, in 0.255s\n",
      "[1097/3000] 1 tree, 31 leaves, max depth = 11, train loss: 0.02176, val loss: 0.06311, in 0.273s\n",
      "[1098/3000] 1 tree, 31 leaves, max depth = 13, train loss: 0.02174, val loss: 0.06311, in 0.218s\n",
      "[1099/3000] 1 tree, 31 leaves, max depth = 10, train loss: 0.02171, val loss: 0.06311, in 0.231s\n",
      "[1100/3000] 1 tree, 31 leaves, max depth = 10, train loss: 0.02169, val loss: 0.06309, in 0.231s\n",
      "[1101/3000] 1 tree, 31 leaves, max depth = 9, train loss: 0.02167, val loss: 0.06308, in 0.237s\n",
      "[1102/3000] 1 tree, 31 leaves, max depth = 11, train loss: 0.02164, val loss: 0.06308, in 0.263s\n",
      "[1103/3000] 1 tree, 31 leaves, max depth = 12, train loss: 0.02161, val loss: 0.06306, in 0.219s\n",
      "[1104/3000] 1 tree, 31 leaves, max depth = 12, train loss: 0.02159, val loss: 0.06307, in 0.187s\n",
      "[1105/3000] 1 tree, 31 leaves, max depth = 13, train loss: 0.02157, val loss: 0.06307, in 0.242s\n",
      "[1106/3000] 1 tree, 31 leaves, max depth = 12, train loss: 0.02155, val loss: 0.06304, in 0.245s\n",
      "[1107/3000] 1 tree, 31 leaves, max depth = 12, train loss: 0.02153, val loss: 0.06304, in 0.223s\n",
      "[1108/3000] 1 tree, 31 leaves, max depth = 10, train loss: 0.02150, val loss: 0.06305, in 0.208s\n",
      "[1109/3000] 1 tree, 31 leaves, max depth = 13, train loss: 0.02146, val loss: 0.06302, in 0.249s\n",
      "[1110/3000] 1 tree, 31 leaves, max depth = 11, train loss: 0.02144, val loss: 0.06301, in 0.262s\n",
      "[1111/3000] 1 tree, 31 leaves, max depth = 13, train loss: 0.02141, val loss: 0.06300, in 0.244s\n",
      "[1112/3000] 1 tree, 31 leaves, max depth = 11, train loss: 0.02138, val loss: 0.06299, in 0.462s\n",
      "[1113/3000] 1 tree, 31 leaves, max depth = 12, train loss: 0.02136, val loss: 0.06301, in 0.266s\n",
      "[1114/3000] 1 tree, 31 leaves, max depth = 10, train loss: 0.02134, val loss: 0.06301, in 0.259s\n",
      "[1115/3000] 1 tree, 31 leaves, max depth = 16, train loss: 0.02132, val loss: 0.06300, in 0.236s\n",
      "[1116/3000] 1 tree, 31 leaves, max depth = 12, train loss: 0.02129, val loss: 0.06299, in 0.253s\n",
      "[1117/3000] 1 tree, 31 leaves, max depth = 9, train loss: 0.02128, val loss: 0.06300, in 0.220s\n",
      "[1118/3000] 1 tree, 31 leaves, max depth = 9, train loss: 0.02126, val loss: 0.06300, in 0.192s\n",
      "[1119/3000] 1 tree, 31 leaves, max depth = 11, train loss: 0.02123, val loss: 0.06298, in 0.241s\n",
      "[1120/3000] 1 tree, 31 leaves, max depth = 11, train loss: 0.02121, val loss: 0.06297, in 0.228s\n",
      "[1121/3000] 1 tree, 31 leaves, max depth = 11, train loss: 0.02118, val loss: 0.06295, in 0.233s\n",
      "[1122/3000] 1 tree, 31 leaves, max depth = 12, train loss: 0.02116, val loss: 0.06295, in 0.218s\n",
      "[1123/3000] 1 tree, 31 leaves, max depth = 10, train loss: 0.02113, val loss: 0.06293, in 0.217s\n",
      "[1124/3000] 1 tree, 31 leaves, max depth = 13, train loss: 0.02110, val loss: 0.06288, in 0.260s\n",
      "[1125/3000] 1 tree, 31 leaves, max depth = 11, train loss: 0.02108, val loss: 0.06290, in 0.198s\n",
      "[1126/3000] 1 tree, 31 leaves, max depth = 10, train loss: 0.02106, val loss: 0.06288, in 0.242s\n",
      "[1127/3000] 1 tree, 31 leaves, max depth = 8, train loss: 0.02104, val loss: 0.06287, in 0.218s\n",
      "[1128/3000] 1 tree, 31 leaves, max depth = 12, train loss: 0.02102, val loss: 0.06288, in 0.187s\n",
      "[1129/3000] 1 tree, 31 leaves, max depth = 14, train loss: 0.02099, val loss: 0.06286, in 0.232s\n",
      "[1130/3000] 1 tree, 31 leaves, max depth = 15, train loss: 0.02096, val loss: 0.06285, in 0.226s\n",
      "[1131/3000] 1 tree, 31 leaves, max depth = 15, train loss: 0.02095, val loss: 0.06284, in 0.217s\n",
      "[1132/3000] 1 tree, 31 leaves, max depth = 10, train loss: 0.02092, val loss: 0.06284, in 0.364s\n",
      "[1133/3000] 1 tree, 31 leaves, max depth = 9, train loss: 0.02089, val loss: 0.06280, in 0.282s\n",
      "[1134/3000] 1 tree, 31 leaves, max depth = 10, train loss: 0.02087, val loss: 0.06281, in 0.228s\n",
      "[1135/3000] 1 tree, 31 leaves, max depth = 15, train loss: 0.02084, val loss: 0.06282, in 0.224s\n",
      "[1136/3000] 1 tree, 31 leaves, max depth = 14, train loss: 0.02082, val loss: 0.06281, in 0.240s\n",
      "[1137/3000] 1 tree, 31 leaves, max depth = 10, train loss: 0.02080, val loss: 0.06280, in 0.191s\n",
      "[1138/3000] 1 tree, 31 leaves, max depth = 13, train loss: 0.02077, val loss: 0.06277, in 0.241s\n",
      "[1139/3000] 1 tree, 31 leaves, max depth = 13, train loss: 0.02074, val loss: 0.06278, in 0.262s\n",
      "[1140/3000] 1 tree, 31 leaves, max depth = 12, train loss: 0.02072, val loss: 0.06278, in 0.212s\n",
      "[1141/3000] 1 tree, 31 leaves, max depth = 13, train loss: 0.02069, val loss: 0.06278, in 0.269s\n",
      "[1142/3000] 1 tree, 31 leaves, max depth = 9, train loss: 0.02067, val loss: 0.06276, in 0.224s\n",
      "[1143/3000] 1 tree, 31 leaves, max depth = 12, train loss: 0.02064, val loss: 0.06278, in 0.252s\n",
      "[1144/3000] 1 tree, 31 leaves, max depth = 9, train loss: 0.02062, val loss: 0.06277, in 0.253s\n",
      "[1145/3000] 1 tree, 31 leaves, max depth = 14, train loss: 0.02060, val loss: 0.06276, in 0.187s\n",
      "[1146/3000] 1 tree, 31 leaves, max depth = 11, train loss: 0.02057, val loss: 0.06277, in 0.284s\n",
      "[1147/3000] 1 tree, 31 leaves, max depth = 13, train loss: 0.02054, val loss: 0.06274, in 0.255s\n",
      "[1148/3000] 1 tree, 31 leaves, max depth = 13, train loss: 0.02053, val loss: 0.06276, in 0.197s\n",
      "[1149/3000] 1 tree, 31 leaves, max depth = 10, train loss: 0.02050, val loss: 0.06273, in 0.203s\n",
      "[1150/3000] 1 tree, 31 leaves, max depth = 19, train loss: 0.02048, val loss: 0.06273, in 0.180s\n",
      "[1151/3000] 1 tree, 31 leaves, max depth = 12, train loss: 0.02046, val loss: 0.06273, in 0.243s\n",
      "[1152/3000] 1 tree, 31 leaves, max depth = 14, train loss: 0.02043, val loss: 0.06274, in 0.220s\n",
      "[1153/3000] 1 tree, 31 leaves, max depth = 14, train loss: 0.02041, val loss: 0.06274, in 0.350s\n",
      "[1154/3000] 1 tree, 31 leaves, max depth = 10, train loss: 0.02039, val loss: 0.06273, in 0.231s\n",
      "[1155/3000] 1 tree, 31 leaves, max depth = 14, train loss: 0.02037, val loss: 0.06272, in 0.232s\n",
      "[1156/3000] 1 tree, 31 leaves, max depth = 10, train loss: 0.02035, val loss: 0.06273, in 0.195s\n",
      "[1157/3000] 1 tree, 31 leaves, max depth = 13, train loss: 0.02031, val loss: 0.06270, in 0.239s\n",
      "[1158/3000] 1 tree, 31 leaves, max depth = 8, train loss: 0.02030, val loss: 0.06270, in 0.208s\n",
      "[1159/3000] 1 tree, 31 leaves, max depth = 10, train loss: 0.02028, val loss: 0.06271, in 0.200s\n",
      "[1160/3000] 1 tree, 31 leaves, max depth = 13, train loss: 0.02026, val loss: 0.06271, in 0.274s\n",
      "[1161/3000] 1 tree, 31 leaves, max depth = 9, train loss: 0.02024, val loss: 0.06270, in 0.195s\n",
      "[1162/3000] 1 tree, 31 leaves, max depth = 13, train loss: 0.02021, val loss: 0.06269, in 0.295s\n",
      "[1163/3000] 1 tree, 31 leaves, max depth = 11, train loss: 0.02019, val loss: 0.06269, in 0.191s\n",
      "[1164/3000] 1 tree, 31 leaves, max depth = 10, train loss: 0.02016, val loss: 0.06269, in 0.223s\n",
      "[1165/3000] 1 tree, 31 leaves, max depth = 12, train loss: 0.02014, val loss: 0.06269, in 0.255s\n",
      "[1166/3000] 1 tree, 31 leaves, max depth = 12, train loss: 0.02013, val loss: 0.06268, in 0.199s\n",
      "[1167/3000] 1 tree, 31 leaves, max depth = 17, train loss: 0.02011, val loss: 0.06267, in 0.232s\n",
      "[1168/3000] 1 tree, 31 leaves, max depth = 9, train loss: 0.02009, val loss: 0.06266, in 0.203s\n",
      "[1169/3000] 1 tree, 31 leaves, max depth = 11, train loss: 0.02008, val loss: 0.06265, in 0.191s\n",
      "[1170/3000] 1 tree, 31 leaves, max depth = 10, train loss: 0.02006, val loss: 0.06265, in 0.197s\n",
      "[1171/3000] 1 tree, 31 leaves, max depth = 15, train loss: 0.02004, val loss: 0.06265, in 0.187s\n",
      "[1172/3000] 1 tree, 31 leaves, max depth = 10, train loss: 0.02003, val loss: 0.06264, in 0.208s\n",
      "[1173/3000] 1 tree, 31 leaves, max depth = 16, train loss: 0.02000, val loss: 0.06263, in 0.362s\n",
      "[1174/3000] 1 tree, 31 leaves, max depth = 12, train loss: 0.01997, val loss: 0.06262, in 0.308s\n",
      "[1175/3000] 1 tree, 31 leaves, max depth = 13, train loss: 0.01995, val loss: 0.06262, in 0.285s\n",
      "[1176/3000] 1 tree, 31 leaves, max depth = 11, train loss: 0.01991, val loss: 0.06258, in 0.291s\n",
      "[1177/3000] 1 tree, 31 leaves, max depth = 12, train loss: 0.01988, val loss: 0.06257, in 0.239s\n",
      "[1178/3000] 1 tree, 31 leaves, max depth = 13, train loss: 0.01986, val loss: 0.06257, in 0.321s\n",
      "[1179/3000] 1 tree, 31 leaves, max depth = 10, train loss: 0.01983, val loss: 0.06256, in 0.246s\n",
      "[1180/3000] 1 tree, 31 leaves, max depth = 8, train loss: 0.01980, val loss: 0.06259, in 0.251s\n",
      "[1181/3000] 1 tree, 31 leaves, max depth = 10, train loss: 0.01978, val loss: 0.06258, in 0.264s\n",
      "[1182/3000] 1 tree, 31 leaves, max depth = 17, train loss: 0.01976, val loss: 0.06258, in 0.238s\n",
      "[1183/3000] 1 tree, 31 leaves, max depth = 11, train loss: 0.01973, val loss: 0.06258, in 0.233s\n",
      "[1184/3000] 1 tree, 31 leaves, max depth = 14, train loss: 0.01971, val loss: 0.06257, in 0.239s\n",
      "[1185/3000] 1 tree, 31 leaves, max depth = 10, train loss: 0.01969, val loss: 0.06256, in 0.238s\n",
      "[1186/3000] 1 tree, 31 leaves, max depth = 16, train loss: 0.01966, val loss: 0.06256, in 0.227s\n",
      "[1187/3000] 1 tree, 31 leaves, max depth = 16, train loss: 0.01965, val loss: 0.06255, in 0.197s\n",
      "[1188/3000] 1 tree, 31 leaves, max depth = 10, train loss: 0.01963, val loss: 0.06255, in 0.218s\n",
      "[1189/3000] 1 tree, 31 leaves, max depth = 10, train loss: 0.01961, val loss: 0.06254, in 0.280s\n",
      "[1190/3000] 1 tree, 31 leaves, max depth = 9, train loss: 0.01959, val loss: 0.06254, in 0.216s\n",
      "[1191/3000] 1 tree, 31 leaves, max depth = 13, train loss: 0.01956, val loss: 0.06251, in 0.273s\n",
      "[1192/3000] 1 tree, 31 leaves, max depth = 13, train loss: 0.01954, val loss: 0.06250, in 0.202s\n",
      "[1193/3000] 1 tree, 31 leaves, max depth = 14, train loss: 0.01952, val loss: 0.06251, in 0.213s\n",
      "[1194/3000] 1 tree, 31 leaves, max depth = 9, train loss: 0.01949, val loss: 0.06250, in 0.379s\n",
      "[1195/3000] 1 tree, 31 leaves, max depth = 10, train loss: 0.01947, val loss: 0.06248, in 0.247s\n",
      "[1196/3000] 1 tree, 31 leaves, max depth = 10, train loss: 0.01945, val loss: 0.06249, in 0.225s\n",
      "[1197/3000] 1 tree, 31 leaves, max depth = 9, train loss: 0.01944, val loss: 0.06249, in 0.220s\n",
      "[1198/3000] 1 tree, 31 leaves, max depth = 11, train loss: 0.01942, val loss: 0.06252, in 0.273s\n",
      "[1199/3000] 1 tree, 31 leaves, max depth = 14, train loss: 0.01941, val loss: 0.06251, in 0.196s\n",
      "[1200/3000] 1 tree, 31 leaves, max depth = 12, train loss: 0.01938, val loss: 0.06251, in 0.215s\n",
      "[1201/3000] 1 tree, 31 leaves, max depth = 13, train loss: 0.01936, val loss: 0.06253, in 0.200s\n",
      "[1202/3000] 1 tree, 31 leaves, max depth = 10, train loss: 0.01935, val loss: 0.06254, in 0.253s\n",
      "[1203/3000] 1 tree, 31 leaves, max depth = 10, train loss: 0.01933, val loss: 0.06254, in 0.194s\n",
      "[1204/3000] 1 tree, 31 leaves, max depth = 10, train loss: 0.01931, val loss: 0.06252, in 0.222s\n",
      "[1205/3000] 1 tree, 31 leaves, max depth = 13, train loss: 0.01929, val loss: 0.06253, in 0.221s\n",
      "[1206/3000] 1 tree, 31 leaves, max depth = 13, train loss: 0.01926, val loss: 0.06250, in 0.233s\n",
      "[1207/3000] 1 tree, 31 leaves, max depth = 9, train loss: 0.01924, val loss: 0.06251, in 0.252s\n",
      "[1208/3000] 1 tree, 31 leaves, max depth = 13, train loss: 0.01922, val loss: 0.06252, in 0.179s\n",
      "[1209/3000] 1 tree, 31 leaves, max depth = 13, train loss: 0.01920, val loss: 0.06253, in 0.196s\n",
      "[1210/3000] 1 tree, 31 leaves, max depth = 12, train loss: 0.01917, val loss: 0.06250, in 0.287s\n",
      "[1211/3000] 1 tree, 31 leaves, max depth = 11, train loss: 0.01915, val loss: 0.06250, in 0.202s\n",
      "[1212/3000] 1 tree, 31 leaves, max depth = 10, train loss: 0.01913, val loss: 0.06249, in 0.259s\n",
      "[1213/3000] 1 tree, 31 leaves, max depth = 10, train loss: 0.01911, val loss: 0.06249, in 0.255s\n",
      "[1214/3000] 1 tree, 31 leaves, max depth = 11, train loss: 0.01908, val loss: 0.06245, in 0.443s\n",
      "[1215/3000] 1 tree, 31 leaves, max depth = 14, train loss: 0.01905, val loss: 0.06244, in 0.269s\n",
      "[1216/3000] 1 tree, 31 leaves, max depth = 13, train loss: 0.01903, val loss: 0.06244, in 0.254s\n",
      "[1217/3000] 1 tree, 31 leaves, max depth = 9, train loss: 0.01901, val loss: 0.06243, in 0.224s\n",
      "[1218/3000] 1 tree, 31 leaves, max depth = 12, train loss: 0.01899, val loss: 0.06245, in 0.232s\n",
      "[1219/3000] 1 tree, 31 leaves, max depth = 12, train loss: 0.01898, val loss: 0.06243, in 0.241s\n",
      "[1220/3000] 1 tree, 31 leaves, max depth = 12, train loss: 0.01895, val loss: 0.06245, in 0.204s\n",
      "[1221/3000] 1 tree, 31 leaves, max depth = 13, train loss: 0.01894, val loss: 0.06244, in 0.214s\n",
      "[1222/3000] 1 tree, 31 leaves, max depth = 11, train loss: 0.01891, val loss: 0.06244, in 0.300s\n",
      "[1223/3000] 1 tree, 31 leaves, max depth = 12, train loss: 0.01889, val loss: 0.06243, in 0.233s\n",
      "[1224/3000] 1 tree, 31 leaves, max depth = 8, train loss: 0.01886, val loss: 0.06243, in 0.236s\n",
      "[1225/3000] 1 tree, 31 leaves, max depth = 12, train loss: 0.01883, val loss: 0.06240, in 0.211s\n",
      "[1226/3000] 1 tree, 31 leaves, max depth = 12, train loss: 0.01881, val loss: 0.06241, in 0.204s\n",
      "[1227/3000] 1 tree, 31 leaves, max depth = 13, train loss: 0.01880, val loss: 0.06241, in 0.186s\n",
      "[1228/3000] 1 tree, 31 leaves, max depth = 12, train loss: 0.01878, val loss: 0.06241, in 0.227s\n",
      "[1229/3000] 1 tree, 31 leaves, max depth = 11, train loss: 0.01876, val loss: 0.06240, in 0.240s\n",
      "[1230/3000] 1 tree, 31 leaves, max depth = 8, train loss: 0.01874, val loss: 0.06240, in 0.197s\n",
      "[1231/3000] 1 tree, 31 leaves, max depth = 11, train loss: 0.01872, val loss: 0.06239, in 0.247s\n",
      "[1232/3000] 1 tree, 31 leaves, max depth = 12, train loss: 0.01870, val loss: 0.06237, in 0.229s\n",
      "[1233/3000] 1 tree, 31 leaves, max depth = 10, train loss: 0.01869, val loss: 0.06236, in 0.185s\n",
      "[1234/3000] 1 tree, 31 leaves, max depth = 9, train loss: 0.01867, val loss: 0.06237, in 0.413s\n",
      "[1235/3000] 1 tree, 31 leaves, max depth = 12, train loss: 0.01865, val loss: 0.06236, in 0.230s\n",
      "[1236/3000] 1 tree, 31 leaves, max depth = 12, train loss: 0.01863, val loss: 0.06234, in 0.296s\n",
      "[1237/3000] 1 tree, 31 leaves, max depth = 13, train loss: 0.01861, val loss: 0.06235, in 0.260s\n",
      "[1238/3000] 1 tree, 31 leaves, max depth = 13, train loss: 0.01859, val loss: 0.06233, in 0.268s\n",
      "[1239/3000] 1 tree, 31 leaves, max depth = 10, train loss: 0.01857, val loss: 0.06234, in 0.244s\n",
      "[1240/3000] 1 tree, 31 leaves, max depth = 11, train loss: 0.01855, val loss: 0.06235, in 0.223s\n",
      "[1241/3000] 1 tree, 31 leaves, max depth = 12, train loss: 0.01852, val loss: 0.06235, in 0.277s\n",
      "[1242/3000] 1 tree, 31 leaves, max depth = 13, train loss: 0.01851, val loss: 0.06235, in 0.246s\n",
      "[1243/3000] 1 tree, 31 leaves, max depth = 14, train loss: 0.01849, val loss: 0.06235, in 0.224s\n",
      "[1244/3000] 1 tree, 31 leaves, max depth = 10, train loss: 0.01848, val loss: 0.06235, in 0.222s\n",
      "[1245/3000] 1 tree, 31 leaves, max depth = 10, train loss: 0.01847, val loss: 0.06234, in 0.240s\n",
      "[1246/3000] 1 tree, 31 leaves, max depth = 13, train loss: 0.01845, val loss: 0.06235, in 0.221s\n",
      "[1247/3000] 1 tree, 31 leaves, max depth = 12, train loss: 0.01843, val loss: 0.06234, in 0.214s\n",
      "[1248/3000] 1 tree, 31 leaves, max depth = 9, train loss: 0.01841, val loss: 0.06232, in 0.278s\n",
      "[1249/3000] 1 tree, 31 leaves, max depth = 9, train loss: 0.01838, val loss: 0.06228, in 0.244s\n",
      "[1250/3000] 1 tree, 31 leaves, max depth = 12, train loss: 0.01834, val loss: 0.06223, in 0.272s\n",
      "[1251/3000] 1 tree, 31 leaves, max depth = 11, train loss: 0.01832, val loss: 0.06223, in 0.224s\n",
      "[1252/3000] 1 tree, 31 leaves, max depth = 10, train loss: 0.01830, val loss: 0.06221, in 0.188s\n",
      "[1253/3000] 1 tree, 31 leaves, max depth = 13, train loss: 0.01828, val loss: 0.06221, in 0.244s\n",
      "[1254/3000] 1 tree, 31 leaves, max depth = 14, train loss: 0.01827, val loss: 0.06222, in 0.335s\n",
      "[1255/3000] 1 tree, 31 leaves, max depth = 10, train loss: 0.01825, val loss: 0.06224, in 0.195s\n",
      "[1256/3000] 1 tree, 31 leaves, max depth = 9, train loss: 0.01823, val loss: 0.06223, in 0.232s\n",
      "[1257/3000] 1 tree, 31 leaves, max depth = 11, train loss: 0.01820, val loss: 0.06222, in 0.205s\n",
      "[1258/3000] 1 tree, 31 leaves, max depth = 14, train loss: 0.01819, val loss: 0.06222, in 0.228s\n",
      "[1259/3000] 1 tree, 31 leaves, max depth = 14, train loss: 0.01817, val loss: 0.06222, in 0.242s\n",
      "[1260/3000] 1 tree, 31 leaves, max depth = 12, train loss: 0.01814, val loss: 0.06222, in 0.241s\n",
      "[1261/3000] 1 tree, 31 leaves, max depth = 13, train loss: 0.01813, val loss: 0.06221, in 0.235s\n",
      "[1262/3000] 1 tree, 31 leaves, max depth = 11, train loss: 0.01810, val loss: 0.06219, in 0.274s\n",
      "[1263/3000] 1 tree, 31 leaves, max depth = 12, train loss: 0.01808, val loss: 0.06218, in 0.299s\n",
      "[1264/3000] 1 tree, 31 leaves, max depth = 11, train loss: 0.01806, val loss: 0.06216, in 0.196s\n",
      "[1265/3000] 1 tree, 31 leaves, max depth = 9, train loss: 0.01804, val loss: 0.06215, in 0.263s\n",
      "[1266/3000] 1 tree, 31 leaves, max depth = 11, train loss: 0.01803, val loss: 0.06215, in 0.254s\n",
      "[1267/3000] 1 tree, 31 leaves, max depth = 11, train loss: 0.01800, val loss: 0.06216, in 0.253s\n",
      "[1268/3000] 1 tree, 31 leaves, max depth = 13, train loss: 0.01798, val loss: 0.06215, in 0.227s\n",
      "[1269/3000] 1 tree, 31 leaves, max depth = 13, train loss: 0.01796, val loss: 0.06217, in 0.252s\n",
      "[1270/3000] 1 tree, 31 leaves, max depth = 12, train loss: 0.01793, val loss: 0.06215, in 0.275s\n",
      "[1271/3000] 1 tree, 31 leaves, max depth = 13, train loss: 0.01791, val loss: 0.06215, in 0.241s\n",
      "[1272/3000] 1 tree, 31 leaves, max depth = 17, train loss: 0.01789, val loss: 0.06215, in 0.204s\n",
      "[1273/3000] 1 tree, 31 leaves, max depth = 9, train loss: 0.01787, val loss: 0.06216, in 0.294s\n",
      "[1274/3000] 1 tree, 31 leaves, max depth = 8, train loss: 0.01786, val loss: 0.06217, in 0.201s\n",
      "[1275/3000] 1 tree, 31 leaves, max depth = 13, train loss: 0.01784, val loss: 0.06216, in 0.415s\n",
      "[1276/3000] 1 tree, 31 leaves, max depth = 11, train loss: 0.01781, val loss: 0.06215, in 0.255s\n",
      "[1277/3000] 1 tree, 31 leaves, max depth = 11, train loss: 0.01779, val loss: 0.06214, in 0.217s\n",
      "[1278/3000] 1 tree, 31 leaves, max depth = 12, train loss: 0.01778, val loss: 0.06214, in 0.197s\n",
      "[1279/3000] 1 tree, 31 leaves, max depth = 11, train loss: 0.01777, val loss: 0.06214, in 0.220s\n",
      "[1280/3000] 1 tree, 31 leaves, max depth = 11, train loss: 0.01775, val loss: 0.06212, in 0.227s\n",
      "[1281/3000] 1 tree, 31 leaves, max depth = 11, train loss: 0.01774, val loss: 0.06211, in 0.232s\n",
      "[1282/3000] 1 tree, 31 leaves, max depth = 10, train loss: 0.01771, val loss: 0.06212, in 0.273s\n",
      "[1283/3000] 1 tree, 31 leaves, max depth = 12, train loss: 0.01769, val loss: 0.06212, in 0.208s\n",
      "[1284/3000] 1 tree, 31 leaves, max depth = 9, train loss: 0.01768, val loss: 0.06214, in 0.218s\n",
      "[1285/3000] 1 tree, 31 leaves, max depth = 9, train loss: 0.01766, val loss: 0.06214, in 0.256s\n",
      "[1286/3000] 1 tree, 31 leaves, max depth = 14, train loss: 0.01764, val loss: 0.06213, in 0.234s\n",
      "[1287/3000] 1 tree, 31 leaves, max depth = 12, train loss: 0.01762, val loss: 0.06212, in 0.201s\n",
      "[1288/3000] 1 tree, 31 leaves, max depth = 11, train loss: 0.01762, val loss: 0.06212, in 0.209s\n",
      "[1289/3000] 1 tree, 31 leaves, max depth = 10, train loss: 0.01760, val loss: 0.06213, in 0.224s\n",
      "[1290/3000] 1 tree, 31 leaves, max depth = 13, train loss: 0.01756, val loss: 0.06210, in 0.292s\n",
      "[1291/3000] 1 tree, 31 leaves, max depth = 13, train loss: 0.01754, val loss: 0.06210, in 0.220s\n",
      "[1292/3000] 1 tree, 31 leaves, max depth = 13, train loss: 0.01753, val loss: 0.06209, in 0.283s\n",
      "[1293/3000] 1 tree, 31 leaves, max depth = 13, train loss: 0.01750, val loss: 0.06208, in 0.266s\n",
      "[1294/3000] 1 tree, 31 leaves, max depth = 12, train loss: 0.01749, val loss: 0.06208, in 0.235s\n",
      "[1295/3000] 1 tree, 31 leaves, max depth = 10, train loss: 0.01745, val loss: 0.06203, in 0.419s\n",
      "[1296/3000] 1 tree, 31 leaves, max depth = 11, train loss: 0.01743, val loss: 0.06203, in 0.273s\n",
      "[1297/3000] 1 tree, 31 leaves, max depth = 12, train loss: 0.01742, val loss: 0.06202, in 0.217s\n",
      "[1298/3000] 1 tree, 31 leaves, max depth = 15, train loss: 0.01740, val loss: 0.06201, in 0.270s\n",
      "[1299/3000] 1 tree, 31 leaves, max depth = 11, train loss: 0.01739, val loss: 0.06201, in 0.220s\n",
      "[1300/3000] 1 tree, 31 leaves, max depth = 11, train loss: 0.01737, val loss: 0.06201, in 0.293s\n",
      "[1301/3000] 1 tree, 31 leaves, max depth = 11, train loss: 0.01734, val loss: 0.06200, in 0.243s\n",
      "[1302/3000] 1 tree, 31 leaves, max depth = 12, train loss: 0.01733, val loss: 0.06201, in 0.282s\n",
      "[1303/3000] 1 tree, 31 leaves, max depth = 10, train loss: 0.01730, val loss: 0.06199, in 0.306s\n",
      "[1304/3000] 1 tree, 31 leaves, max depth = 10, train loss: 0.01728, val loss: 0.06198, in 0.249s\n",
      "[1305/3000] 1 tree, 31 leaves, max depth = 10, train loss: 0.01726, val loss: 0.06197, in 0.218s\n",
      "[1306/3000] 1 tree, 31 leaves, max depth = 10, train loss: 0.01724, val loss: 0.06197, in 0.228s\n",
      "[1307/3000] 1 tree, 31 leaves, max depth = 9, train loss: 0.01722, val loss: 0.06197, in 0.227s\n",
      "[1308/3000] 1 tree, 31 leaves, max depth = 13, train loss: 0.01720, val loss: 0.06199, in 0.321s\n",
      "[1309/3000] 1 tree, 31 leaves, max depth = 11, train loss: 0.01718, val loss: 0.06198, in 0.236s\n",
      "[1310/3000] 1 tree, 31 leaves, max depth = 10, train loss: 0.01717, val loss: 0.06198, in 0.266s\n",
      "[1311/3000] 1 tree, 31 leaves, max depth = 9, train loss: 0.01714, val loss: 0.06194, in 0.296s\n",
      "[1312/3000] 1 tree, 31 leaves, max depth = 9, train loss: 0.01713, val loss: 0.06195, in 0.239s\n",
      "[1313/3000] 1 tree, 31 leaves, max depth = 8, train loss: 0.01710, val loss: 0.06193, in 0.293s\n",
      "[1314/3000] 1 tree, 31 leaves, max depth = 11, train loss: 0.01708, val loss: 0.06194, in 0.247s\n",
      "[1315/3000] 1 tree, 31 leaves, max depth = 13, train loss: 0.01706, val loss: 0.06195, in 0.219s\n",
      "[1316/3000] 1 tree, 31 leaves, max depth = 16, train loss: 0.01704, val loss: 0.06195, in 0.400s\n",
      "[1317/3000] 1 tree, 31 leaves, max depth = 13, train loss: 0.01702, val loss: 0.06194, in 0.333s\n",
      "[1318/3000] 1 tree, 31 leaves, max depth = 16, train loss: 0.01700, val loss: 0.06193, in 0.280s\n",
      "[1319/3000] 1 tree, 31 leaves, max depth = 9, train loss: 0.01699, val loss: 0.06194, in 0.186s\n",
      "[1320/3000] 1 tree, 31 leaves, max depth = 13, train loss: 0.01695, val loss: 0.06190, in 0.301s\n",
      "[1321/3000] 1 tree, 31 leaves, max depth = 9, train loss: 0.01693, val loss: 0.06191, in 0.213s\n",
      "[1322/3000] 1 tree, 31 leaves, max depth = 12, train loss: 0.01691, val loss: 0.06190, in 0.198s\n",
      "[1323/3000] 1 tree, 31 leaves, max depth = 11, train loss: 0.01689, val loss: 0.06189, in 0.240s\n",
      "[1324/3000] 1 tree, 31 leaves, max depth = 13, train loss: 0.01688, val loss: 0.06188, in 0.202s\n",
      "[1325/3000] 1 tree, 31 leaves, max depth = 10, train loss: 0.01686, val loss: 0.06189, in 0.192s\n",
      "[1326/3000] 1 tree, 31 leaves, max depth = 10, train loss: 0.01684, val loss: 0.06190, in 0.258s\n",
      "[1327/3000] 1 tree, 31 leaves, max depth = 9, train loss: 0.01683, val loss: 0.06189, in 0.211s\n",
      "[1328/3000] 1 tree, 31 leaves, max depth = 10, train loss: 0.01681, val loss: 0.06187, in 0.256s\n",
      "[1329/3000] 1 tree, 31 leaves, max depth = 12, train loss: 0.01679, val loss: 0.06185, in 0.228s\n",
      "[1330/3000] 1 tree, 31 leaves, max depth = 11, train loss: 0.01677, val loss: 0.06186, in 0.253s\n",
      "[1331/3000] 1 tree, 31 leaves, max depth = 10, train loss: 0.01676, val loss: 0.06185, in 0.213s\n",
      "[1332/3000] 1 tree, 31 leaves, max depth = 10, train loss: 0.01674, val loss: 0.06186, in 0.203s\n",
      "[1333/3000] 1 tree, 31 leaves, max depth = 9, train loss: 0.01672, val loss: 0.06185, in 0.254s\n",
      "[1334/3000] 1 tree, 31 leaves, max depth = 11, train loss: 0.01671, val loss: 0.06186, in 0.196s\n",
      "[1335/3000] 1 tree, 31 leaves, max depth = 9, train loss: 0.01669, val loss: 0.06188, in 0.218s\n",
      "[1336/3000] 1 tree, 31 leaves, max depth = 14, train loss: 0.01666, val loss: 0.06188, in 0.422s\n",
      "[1337/3000] 1 tree, 31 leaves, max depth = 12, train loss: 0.01664, val loss: 0.06190, in 0.219s\n",
      "[1338/3000] 1 tree, 31 leaves, max depth = 9, train loss: 0.01662, val loss: 0.06187, in 0.276s\n",
      "[1339/3000] 1 tree, 31 leaves, max depth = 11, train loss: 0.01659, val loss: 0.06185, in 0.219s\n",
      "[1340/3000] 1 tree, 31 leaves, max depth = 13, train loss: 0.01658, val loss: 0.06184, in 0.235s\n",
      "[1341/3000] 1 tree, 31 leaves, max depth = 15, train loss: 0.01656, val loss: 0.06184, in 0.230s\n",
      "[1342/3000] 1 tree, 31 leaves, max depth = 12, train loss: 0.01654, val loss: 0.06184, in 0.202s\n",
      "[1343/3000] 1 tree, 31 leaves, max depth = 11, train loss: 0.01652, val loss: 0.06183, in 0.237s\n",
      "[1344/3000] 1 tree, 31 leaves, max depth = 9, train loss: 0.01650, val loss: 0.06181, in 0.248s\n",
      "[1345/3000] 1 tree, 31 leaves, max depth = 13, train loss: 0.01649, val loss: 0.06180, in 0.241s\n",
      "[1346/3000] 1 tree, 31 leaves, max depth = 12, train loss: 0.01647, val loss: 0.06178, in 0.215s\n",
      "[1347/3000] 1 tree, 31 leaves, max depth = 9, train loss: 0.01644, val loss: 0.06176, in 0.232s\n",
      "[1348/3000] 1 tree, 31 leaves, max depth = 12, train loss: 0.01643, val loss: 0.06177, in 0.190s\n",
      "[1349/3000] 1 tree, 31 leaves, max depth = 13, train loss: 0.01641, val loss: 0.06176, in 0.239s\n",
      "[1350/3000] 1 tree, 31 leaves, max depth = 11, train loss: 0.01640, val loss: 0.06176, in 0.205s\n",
      "[1351/3000] 1 tree, 31 leaves, max depth = 11, train loss: 0.01638, val loss: 0.06176, in 0.202s\n",
      "[1352/3000] 1 tree, 31 leaves, max depth = 12, train loss: 0.01636, val loss: 0.06178, in 0.211s\n",
      "[1353/3000] 1 tree, 31 leaves, max depth = 12, train loss: 0.01634, val loss: 0.06178, in 0.242s\n",
      "[1354/3000] 1 tree, 31 leaves, max depth = 9, train loss: 0.01632, val loss: 0.06180, in 0.209s\n",
      "[1355/3000] 1 tree, 31 leaves, max depth = 10, train loss: 0.01631, val loss: 0.06181, in 0.195s\n",
      "[1356/3000] 1 tree, 31 leaves, max depth = 13, train loss: 0.01629, val loss: 0.06182, in 0.241s\n",
      "[1357/3000] 1 tree, 31 leaves, max depth = 10, train loss: 0.01627, val loss: 0.06181, in 0.430s\n",
      "[1358/3000] 1 tree, 31 leaves, max depth = 10, train loss: 0.01625, val loss: 0.06182, in 0.302s\n",
      "[1359/3000] 1 tree, 31 leaves, max depth = 12, train loss: 0.01623, val loss: 0.06184, in 0.294s\n",
      "[1360/3000] 1 tree, 31 leaves, max depth = 13, train loss: 0.01621, val loss: 0.06184, in 0.233s\n",
      "[1361/3000] 1 tree, 31 leaves, max depth = 8, train loss: 0.01620, val loss: 0.06184, in 0.196s\n",
      "[1362/3000] 1 tree, 31 leaves, max depth = 13, train loss: 0.01618, val loss: 0.06183, in 0.195s\n",
      "[1363/3000] 1 tree, 31 leaves, max depth = 10, train loss: 0.01617, val loss: 0.06182, in 0.194s\n",
      "[1364/3000] 1 tree, 31 leaves, max depth = 11, train loss: 0.01615, val loss: 0.06183, in 0.213s\n",
      "[1365/3000] 1 tree, 31 leaves, max depth = 10, train loss: 0.01613, val loss: 0.06181, in 0.212s\n",
      "[1366/3000] 1 tree, 31 leaves, max depth = 10, train loss: 0.01612, val loss: 0.06183, in 0.260s\n",
      "[1367/3000] 1 tree, 31 leaves, max depth = 10, train loss: 0.01610, val loss: 0.06182, in 0.221s\n",
      "[1368/3000] 1 tree, 31 leaves, max depth = 14, train loss: 0.01609, val loss: 0.06182, in 0.192s\n",
      "[1369/3000] 1 tree, 31 leaves, max depth = 10, train loss: 0.01607, val loss: 0.06181, in 0.243s\n",
      "[1370/3000] 1 tree, 31 leaves, max depth = 11, train loss: 0.01605, val loss: 0.06179, in 0.197s\n",
      "[1371/3000] 1 tree, 31 leaves, max depth = 12, train loss: 0.01603, val loss: 0.06179, in 0.224s\n",
      "[1372/3000] 1 tree, 31 leaves, max depth = 15, train loss: 0.01601, val loss: 0.06178, in 0.262s\n",
      "[1373/3000] 1 tree, 31 leaves, max depth = 16, train loss: 0.01599, val loss: 0.06176, in 0.238s\n",
      "[1374/3000] 1 tree, 31 leaves, max depth = 9, train loss: 0.01597, val loss: 0.06178, in 0.220s\n",
      "[1375/3000] 1 tree, 31 leaves, max depth = 13, train loss: 0.01596, val loss: 0.06178, in 0.219s\n",
      "[1376/3000] 1 tree, 31 leaves, max depth = 11, train loss: 0.01594, val loss: 0.06180, in 0.243s\n",
      "[1377/3000] 1 tree, 31 leaves, max depth = 12, train loss: 0.01592, val loss: 0.06178, in 0.406s\n",
      "[1378/3000] 1 tree, 31 leaves, max depth = 13, train loss: 0.01590, val loss: 0.06177, in 0.265s\n",
      "[1379/3000] 1 tree, 31 leaves, max depth = 11, train loss: 0.01589, val loss: 0.06176, in 0.533s\n",
      "[1380/3000] 1 tree, 31 leaves, max depth = 11, train loss: 0.01586, val loss: 0.06176, in 0.287s\n",
      "[1381/3000] 1 tree, 31 leaves, max depth = 12, train loss: 0.01584, val loss: 0.06179, in 0.294s\n",
      "[1382/3000] 1 tree, 31 leaves, max depth = 12, train loss: 0.01582, val loss: 0.06180, in 0.278s\n",
      "[1383/3000] 1 tree, 31 leaves, max depth = 9, train loss: 0.01581, val loss: 0.06180, in 0.246s\n",
      "[1384/3000] 1 tree, 31 leaves, max depth = 9, train loss: 0.01579, val loss: 0.06180, in 0.182s\n",
      "[1385/3000] 1 tree, 31 leaves, max depth = 11, train loss: 0.01578, val loss: 0.06181, in 0.195s\n",
      "[1386/3000] 1 tree, 31 leaves, max depth = 10, train loss: 0.01577, val loss: 0.06181, in 0.196s\n",
      "[1387/3000] 1 tree, 31 leaves, max depth = 13, train loss: 0.01575, val loss: 0.06180, in 0.201s\n",
      "[1388/3000] 1 tree, 31 leaves, max depth = 10, train loss: 0.01573, val loss: 0.06181, in 0.221s\n",
      "[1389/3000] 1 tree, 31 leaves, max depth = 12, train loss: 0.01571, val loss: 0.06182, in 0.221s\n",
      "[1390/3000] 1 tree, 31 leaves, max depth = 11, train loss: 0.01570, val loss: 0.06182, in 0.211s\n",
      "[1391/3000] 1 tree, 31 leaves, max depth = 11, train loss: 0.01568, val loss: 0.06181, in 0.240s\n",
      "[1392/3000] 1 tree, 31 leaves, max depth = 10, train loss: 0.01566, val loss: 0.06180, in 0.259s\n",
      "[1393/3000] 1 tree, 31 leaves, max depth = 16, train loss: 0.01565, val loss: 0.06180, in 0.212s\n",
      "[1394/3000] 1 tree, 31 leaves, max depth = 10, train loss: 0.01563, val loss: 0.06180, in 0.181s\n",
      "[1395/3000] 1 tree, 31 leaves, max depth = 11, train loss: 0.01562, val loss: 0.06178, in 0.237s\n",
      "[1396/3000] 1 tree, 31 leaves, max depth = 14, train loss: 0.01560, val loss: 0.06178, in 0.253s\n",
      "[1397/3000] 1 tree, 31 leaves, max depth = 14, train loss: 0.01558, val loss: 0.06178, in 0.238s\n",
      "[1398/3000] 1 tree, 31 leaves, max depth = 14, train loss: 0.01556, val loss: 0.06178, in 0.393s\n",
      "[1399/3000] 1 tree, 31 leaves, max depth = 13, train loss: 0.01554, val loss: 0.06178, in 0.214s\n",
      "[1400/3000] 1 tree, 31 leaves, max depth = 13, train loss: 0.01553, val loss: 0.06179, in 0.209s\n",
      "[1401/3000] 1 tree, 31 leaves, max depth = 11, train loss: 0.01552, val loss: 0.06178, in 0.229s\n",
      "[1402/3000] 1 tree, 31 leaves, max depth = 9, train loss: 0.01551, val loss: 0.06178, in 0.202s\n",
      "[1403/3000] 1 tree, 31 leaves, max depth = 13, train loss: 0.01549, val loss: 0.06178, in 0.215s\n",
      "[1404/3000] 1 tree, 31 leaves, max depth = 14, train loss: 0.01548, val loss: 0.06177, in 0.223s\n",
      "[1405/3000] 1 tree, 31 leaves, max depth = 11, train loss: 0.01546, val loss: 0.06177, in 0.265s\n",
      "[1406/3000] 1 tree, 31 leaves, max depth = 13, train loss: 0.01545, val loss: 0.06176, in 0.188s\n",
      "[1407/3000] 1 tree, 31 leaves, max depth = 11, train loss: 0.01543, val loss: 0.06176, in 0.252s\n",
      "[1408/3000] 1 tree, 31 leaves, max depth = 10, train loss: 0.01542, val loss: 0.06175, in 0.193s\n",
      "[1409/3000] 1 tree, 31 leaves, max depth = 11, train loss: 0.01541, val loss: 0.06177, in 0.224s\n",
      "[1410/3000] 1 tree, 31 leaves, max depth = 13, train loss: 0.01539, val loss: 0.06177, in 0.191s\n",
      "[1411/3000] 1 tree, 31 leaves, max depth = 11, train loss: 0.01537, val loss: 0.06179, in 0.284s\n",
      "[1412/3000] 1 tree, 31 leaves, max depth = 12, train loss: 0.01536, val loss: 0.06180, in 0.195s\n",
      "[1413/3000] 1 tree, 31 leaves, max depth = 19, train loss: 0.01535, val loss: 0.06180, in 0.187s\n",
      "[1414/3000] 1 tree, 31 leaves, max depth = 12, train loss: 0.01533, val loss: 0.06180, in 0.246s\n",
      "[1415/3000] 1 tree, 31 leaves, max depth = 10, train loss: 0.01532, val loss: 0.06180, in 0.217s\n",
      "[1416/3000] 1 tree, 31 leaves, max depth = 17, train loss: 0.01530, val loss: 0.06180, in 0.228s\n",
      "[1417/3000] 1 tree, 31 leaves, max depth = 10, train loss: 0.01528, val loss: 0.06179, in 0.264s\n",
      "[1418/3000] 1 tree, 31 leaves, max depth = 12, train loss: 0.01526, val loss: 0.06178, in 0.213s\n",
      "[1419/3000] 1 tree, 31 leaves, max depth = 13, train loss: 0.01525, val loss: 0.06180, in 0.361s\n",
      "[1420/3000] 1 tree, 31 leaves, max depth = 10, train loss: 0.01523, val loss: 0.06180, in 0.253s\n",
      "[1421/3000] 1 tree, 31 leaves, max depth = 12, train loss: 0.01521, val loss: 0.06182, in 0.234s\n",
      "[1422/3000] 1 tree, 31 leaves, max depth = 13, train loss: 0.01519, val loss: 0.06179, in 0.179s\n",
      "[1423/3000] 1 tree, 31 leaves, max depth = 14, train loss: 0.01517, val loss: 0.06179, in 0.263s\n",
      "[1424/3000] 1 tree, 31 leaves, max depth = 11, train loss: 0.01514, val loss: 0.06178, in 0.258s\n",
      "[1425/3000] 1 tree, 31 leaves, max depth = 14, train loss: 0.01512, val loss: 0.06178, in 0.247s\n",
      "[1426/3000] 1 tree, 31 leaves, max depth = 13, train loss: 0.01511, val loss: 0.06178, in 0.235s\n",
      "[1427/3000] 1 tree, 31 leaves, max depth = 10, train loss: 0.01509, val loss: 0.06178, in 0.213s\n",
      "[1428/3000] 1 tree, 31 leaves, max depth = 11, train loss: 0.01508, val loss: 0.06178, in 0.234s\n",
      "[1429/3000] 1 tree, 31 leaves, max depth = 8, train loss: 0.01506, val loss: 0.06179, in 0.212s\n",
      "[1430/3000] 1 tree, 31 leaves, max depth = 9, train loss: 0.01505, val loss: 0.06178, in 0.196s\n",
      "[1431/3000] 1 tree, 31 leaves, max depth = 12, train loss: 0.01504, val loss: 0.06180, in 0.191s\n",
      "[1432/3000] 1 tree, 31 leaves, max depth = 10, train loss: 0.01502, val loss: 0.06179, in 0.228s\n",
      "[1433/3000] 1 tree, 31 leaves, max depth = 12, train loss: 0.01499, val loss: 0.06176, in 0.232s\n",
      "[1434/3000] 1 tree, 31 leaves, max depth = 14, train loss: 0.01498, val loss: 0.06176, in 0.209s\n",
      "[1435/3000] 1 tree, 31 leaves, max depth = 13, train loss: 0.01497, val loss: 0.06177, in 0.199s\n",
      "[1436/3000] 1 tree, 31 leaves, max depth = 12, train loss: 0.01496, val loss: 0.06176, in 0.225s\n",
      "[1437/3000] 1 tree, 31 leaves, max depth = 11, train loss: 0.01494, val loss: 0.06178, in 0.233s\n",
      "[1438/3000] 1 tree, 31 leaves, max depth = 11, train loss: 0.01493, val loss: 0.06178, in 0.206s\n",
      "[1439/3000] 1 tree, 31 leaves, max depth = 10, train loss: 0.01491, val loss: 0.06179, in 0.408s\n",
      "[1440/3000] 1 tree, 31 leaves, max depth = 11, train loss: 0.01489, val loss: 0.06180, in 0.241s\n",
      "[1441/3000] 1 tree, 31 leaves, max depth = 14, train loss: 0.01487, val loss: 0.06180, in 0.319s\n",
      "[1442/3000] 1 tree, 31 leaves, max depth = 17, train loss: 0.01486, val loss: 0.06181, in 0.213s\n",
      "[1443/3000] 1 tree, 31 leaves, max depth = 12, train loss: 0.01485, val loss: 0.06183, in 0.247s\n",
      "[1444/3000] 1 tree, 31 leaves, max depth = 10, train loss: 0.01483, val loss: 0.06183, in 0.220s\n",
      "[1445/3000] 1 tree, 31 leaves, max depth = 12, train loss: 0.01481, val loss: 0.06185, in 0.295s\n",
      "[1446/3000] 1 tree, 31 leaves, max depth = 12, train loss: 0.01479, val loss: 0.06181, in 0.266s\n",
      "[1447/3000] 1 tree, 31 leaves, max depth = 10, train loss: 0.01478, val loss: 0.06182, in 0.239s\n",
      "[1448/3000] 1 tree, 31 leaves, max depth = 11, train loss: 0.01476, val loss: 0.06182, in 0.257s\n",
      "[1449/3000] 1 tree, 31 leaves, max depth = 10, train loss: 0.01474, val loss: 0.06182, in 0.212s\n",
      "[1450/3000] 1 tree, 31 leaves, max depth = 9, train loss: 0.01473, val loss: 0.06182, in 0.209s\n",
      "[1451/3000] 1 tree, 31 leaves, max depth = 14, train loss: 0.01471, val loss: 0.06183, in 0.188s\n",
      "[1452/3000] 1 tree, 31 leaves, max depth = 11, train loss: 0.01470, val loss: 0.06182, in 0.201s\n",
      "[1453/3000] 1 tree, 31 leaves, max depth = 11, train loss: 0.01469, val loss: 0.06182, in 0.197s\n",
      "[1454/3000] 1 tree, 31 leaves, max depth = 13, train loss: 0.01468, val loss: 0.06181, in 0.199s\n",
      "[1455/3000] 1 tree, 31 leaves, max depth = 13, train loss: 0.01467, val loss: 0.06180, in 0.233s\n",
      "[1456/3000] 1 tree, 31 leaves, max depth = 10, train loss: 0.01465, val loss: 0.06181, in 0.221s\n",
      "[1457/3000] 1 tree, 31 leaves, max depth = 16, train loss: 0.01464, val loss: 0.06180, in 0.201s\n",
      "[1458/3000] 1 tree, 31 leaves, max depth = 16, train loss: 0.01462, val loss: 0.06180, in 0.235s\n",
      "[1459/3000] 1 tree, 31 leaves, max depth = 12, train loss: 0.01460, val loss: 0.06181, in 0.221s\n",
      "[1460/3000] 1 tree, 31 leaves, max depth = 11, train loss: 0.01459, val loss: 0.06182, in 0.365s\n",
      "[1461/3000] 1 tree, 31 leaves, max depth = 17, train loss: 0.01457, val loss: 0.06182, in 0.248s\n",
      "[1462/3000] 1 tree, 31 leaves, max depth = 10, train loss: 0.01456, val loss: 0.06184, in 0.246s\n",
      "[1463/3000] 1 tree, 31 leaves, max depth = 11, train loss: 0.01454, val loss: 0.06184, in 0.270s\n",
      "[1464/3000] 1 tree, 31 leaves, max depth = 14, train loss: 0.01452, val loss: 0.06184, in 0.269s\n",
      "[1465/3000] 1 tree, 31 leaves, max depth = 10, train loss: 0.01451, val loss: 0.06183, in 0.227s\n",
      "[1466/3000] 1 tree, 31 leaves, max depth = 16, train loss: 0.01449, val loss: 0.06185, in 0.210s\n",
      "[1467/3000] 1 tree, 31 leaves, max depth = 15, train loss: 0.01448, val loss: 0.06185, in 0.214s\n",
      "[1468/3000] 1 tree, 31 leaves, max depth = 13, train loss: 0.01446, val loss: 0.06185, in 0.236s\n",
      "[1469/3000] 1 tree, 31 leaves, max depth = 13, train loss: 0.01445, val loss: 0.06185, in 0.229s\n",
      "[1470/3000] 1 tree, 31 leaves, max depth = 10, train loss: 0.01443, val loss: 0.06184, in 0.284s\n",
      "[1471/3000] 1 tree, 31 leaves, max depth = 11, train loss: 0.01441, val loss: 0.06185, in 0.248s\n",
      "[1472/3000] 1 tree, 31 leaves, max depth = 13, train loss: 0.01439, val loss: 0.06184, in 0.263s\n",
      "[1473/3000] 1 tree, 31 leaves, max depth = 12, train loss: 0.01438, val loss: 0.06183, in 0.199s\n",
      "[1474/3000] 1 tree, 31 leaves, max depth = 9, train loss: 0.01437, val loss: 0.06183, in 0.238s\n",
      "[1475/3000] 1 tree, 31 leaves, max depth = 10, train loss: 0.01436, val loss: 0.06182, in 0.204s\n",
      "[1476/3000] 1 tree, 31 leaves, max depth = 12, train loss: 0.01434, val loss: 0.06182, in 0.256s\n",
      "[1477/3000] 1 tree, 31 leaves, max depth = 9, train loss: 0.01432, val loss: 0.06182, in 0.255s\n",
      "[1478/3000] 1 tree, 31 leaves, max depth = 15, train loss: 0.01431, val loss: 0.06183, in 0.203s\n",
      "[1479/3000] 1 tree, 31 leaves, max depth = 12, train loss: 0.01429, val loss: 0.06183, in 0.246s\n",
      "[1480/3000] 1 tree, 31 leaves, max depth = 14, train loss: 0.01428, val loss: 0.06182, in 0.375s\n",
      "[1481/3000] 1 tree, 31 leaves, max depth = 12, train loss: 0.01426, val loss: 0.06181, in 0.265s\n",
      "[1482/3000] 1 tree, 31 leaves, max depth = 13, train loss: 0.01425, val loss: 0.06178, in 0.235s\n",
      "[1483/3000] 1 tree, 31 leaves, max depth = 11, train loss: 0.01423, val loss: 0.06178, in 0.245s\n",
      "[1484/3000] 1 tree, 31 leaves, max depth = 13, train loss: 0.01422, val loss: 0.06178, in 0.236s\n",
      "[1485/3000] 1 tree, 31 leaves, max depth = 12, train loss: 0.01420, val loss: 0.06177, in 0.238s\n",
      "[1486/3000] 1 tree, 31 leaves, max depth = 12, train loss: 0.01418, val loss: 0.06176, in 0.298s\n",
      "[1487/3000] 1 tree, 31 leaves, max depth = 9, train loss: 0.01417, val loss: 0.06175, in 0.233s\n",
      "[1488/3000] 1 tree, 31 leaves, max depth = 11, train loss: 0.01416, val loss: 0.06176, in 0.189s\n",
      "[1489/3000] 1 tree, 31 leaves, max depth = 11, train loss: 0.01414, val loss: 0.06175, in 0.233s\n",
      "[1490/3000] 1 tree, 31 leaves, max depth = 13, train loss: 0.01413, val loss: 0.06175, in 0.260s\n",
      "[1491/3000] 1 tree, 31 leaves, max depth = 12, train loss: 0.01411, val loss: 0.06174, in 0.232s\n",
      "[1492/3000] 1 tree, 31 leaves, max depth = 14, train loss: 0.01409, val loss: 0.06177, in 0.270s\n",
      "[1493/3000] 1 tree, 31 leaves, max depth = 12, train loss: 0.01407, val loss: 0.06173, in 0.309s\n",
      "[1494/3000] 1 tree, 31 leaves, max depth = 9, train loss: 0.01405, val loss: 0.06173, in 0.297s\n",
      "[1495/3000] 1 tree, 31 leaves, max depth = 14, train loss: 0.01403, val loss: 0.06172, in 0.231s\n",
      "[1496/3000] 1 tree, 31 leaves, max depth = 12, train loss: 0.01402, val loss: 0.06172, in 0.198s\n",
      "[1497/3000] 1 tree, 31 leaves, max depth = 9, train loss: 0.01401, val loss: 0.06173, in 0.298s\n",
      "[1498/3000] 1 tree, 31 leaves, max depth = 10, train loss: 0.01399, val loss: 0.06173, in 0.232s\n",
      "[1499/3000] 1 tree, 31 leaves, max depth = 10, train loss: 0.01397, val loss: 0.06172, in 0.211s\n",
      "[1500/3000] 1 tree, 31 leaves, max depth = 11, train loss: 0.01395, val loss: 0.06172, in 0.419s\n",
      "[1501/3000] 1 tree, 31 leaves, max depth = 11, train loss: 0.01394, val loss: 0.06171, in 0.225s\n",
      "[1502/3000] 1 tree, 31 leaves, max depth = 14, train loss: 0.01392, val loss: 0.06170, in 0.248s\n",
      "[1503/3000] 1 tree, 31 leaves, max depth = 10, train loss: 0.01391, val loss: 0.06170, in 0.199s\n",
      "[1504/3000] 1 tree, 31 leaves, max depth = 11, train loss: 0.01390, val loss: 0.06169, in 0.204s\n",
      "[1505/3000] 1 tree, 31 leaves, max depth = 9, train loss: 0.01387, val loss: 0.06167, in 0.367s\n",
      "[1506/3000] 1 tree, 31 leaves, max depth = 16, train loss: 0.01385, val loss: 0.06167, in 0.301s\n",
      "[1507/3000] 1 tree, 31 leaves, max depth = 10, train loss: 0.01383, val loss: 0.06167, in 0.301s\n",
      "[1508/3000] 1 tree, 31 leaves, max depth = 12, train loss: 0.01381, val loss: 0.06167, in 0.238s\n",
      "[1509/3000] 1 tree, 31 leaves, max depth = 14, train loss: 0.01380, val loss: 0.06166, in 0.260s\n",
      "[1510/3000] 1 tree, 31 leaves, max depth = 11, train loss: 0.01378, val loss: 0.06167, in 0.216s\n",
      "[1511/3000] 1 tree, 31 leaves, max depth = 10, train loss: 0.01376, val loss: 0.06165, in 0.254s\n",
      "[1512/3000] 1 tree, 31 leaves, max depth = 10, train loss: 0.01374, val loss: 0.06165, in 0.219s\n",
      "[1513/3000] 1 tree, 31 leaves, max depth = 11, train loss: 0.01373, val loss: 0.06164, in 0.207s\n",
      "[1514/3000] 1 tree, 31 leaves, max depth = 14, train loss: 0.01372, val loss: 0.06165, in 0.196s\n",
      "[1515/3000] 1 tree, 31 leaves, max depth = 14, train loss: 0.01370, val loss: 0.06166, in 0.246s\n",
      "[1516/3000] 1 tree, 31 leaves, max depth = 10, train loss: 0.01368, val loss: 0.06165, in 0.191s\n",
      "[1517/3000] 1 tree, 31 leaves, max depth = 13, train loss: 0.01366, val loss: 0.06167, in 0.250s\n",
      "[1518/3000] 1 tree, 31 leaves, max depth = 11, train loss: 0.01365, val loss: 0.06165, in 0.197s\n",
      "[1519/3000] 1 tree, 31 leaves, max depth = 11, train loss: 0.01364, val loss: 0.06164, in 0.223s\n",
      "[1520/3000] 1 tree, 31 leaves, max depth = 9, train loss: 0.01362, val loss: 0.06163, in 0.361s\n",
      "[1521/3000] 1 tree, 31 leaves, max depth = 10, train loss: 0.01361, val loss: 0.06163, in 0.252s\n",
      "[1522/3000] 1 tree, 31 leaves, max depth = 10, train loss: 0.01359, val loss: 0.06161, in 0.299s\n",
      "[1523/3000] 1 tree, 31 leaves, max depth = 11, train loss: 0.01358, val loss: 0.06161, in 0.190s\n",
      "[1524/3000] 1 tree, 31 leaves, max depth = 11, train loss: 0.01356, val loss: 0.06160, in 0.281s\n",
      "[1525/3000] 1 tree, 31 leaves, max depth = 14, train loss: 0.01354, val loss: 0.06159, in 0.225s\n",
      "[1526/3000] 1 tree, 31 leaves, max depth = 12, train loss: 0.01352, val loss: 0.06158, in 0.230s\n",
      "[1527/3000] 1 tree, 31 leaves, max depth = 15, train loss: 0.01351, val loss: 0.06157, in 0.194s\n",
      "[1528/3000] 1 tree, 31 leaves, max depth = 12, train loss: 0.01350, val loss: 0.06156, in 0.222s\n",
      "[1529/3000] 1 tree, 31 leaves, max depth = 10, train loss: 0.01349, val loss: 0.06156, in 0.197s\n",
      "[1530/3000] 1 tree, 31 leaves, max depth = 11, train loss: 0.01348, val loss: 0.06157, in 0.197s\n",
      "[1531/3000] 1 tree, 31 leaves, max depth = 11, train loss: 0.01346, val loss: 0.06157, in 0.219s\n",
      "[1532/3000] 1 tree, 31 leaves, max depth = 11, train loss: 0.01345, val loss: 0.06155, in 0.260s\n",
      "[1533/3000] 1 tree, 31 leaves, max depth = 11, train loss: 0.01343, val loss: 0.06154, in 0.264s\n",
      "[1534/3000] 1 tree, 31 leaves, max depth = 11, train loss: 0.01341, val loss: 0.06155, in 0.264s\n",
      "[1535/3000] 1 tree, 31 leaves, max depth = 10, train loss: 0.01340, val loss: 0.06154, in 0.228s\n",
      "[1536/3000] 1 tree, 31 leaves, max depth = 12, train loss: 0.01338, val loss: 0.06154, in 0.198s\n",
      "[1537/3000] 1 tree, 31 leaves, max depth = 15, train loss: 0.01337, val loss: 0.06155, in 0.291s\n",
      "[1538/3000] 1 tree, 31 leaves, max depth = 12, train loss: 0.01335, val loss: 0.06153, in 0.284s\n",
      "[1539/3000] 1 tree, 31 leaves, max depth = 12, train loss: 0.01333, val loss: 0.06154, in 0.253s\n",
      "[1540/3000] 1 tree, 31 leaves, max depth = 10, train loss: 0.01331, val loss: 0.06152, in 0.264s\n",
      "[1541/3000] 1 tree, 31 leaves, max depth = 13, train loss: 0.01330, val loss: 0.06150, in 0.279s\n",
      "[1542/3000] 1 tree, 31 leaves, max depth = 10, train loss: 0.01329, val loss: 0.06150, in 0.274s\n",
      "[1543/3000] 1 tree, 31 leaves, max depth = 14, train loss: 0.01327, val loss: 0.06150, in 0.260s\n",
      "[1544/3000] 1 tree, 31 leaves, max depth = 11, train loss: 0.01326, val loss: 0.06149, in 0.249s\n",
      "[1545/3000] 1 tree, 31 leaves, max depth = 16, train loss: 0.01324, val loss: 0.06150, in 0.197s\n",
      "[1546/3000] 1 tree, 31 leaves, max depth = 8, train loss: 0.01323, val loss: 0.06149, in 0.229s\n",
      "[1547/3000] 1 tree, 31 leaves, max depth = 10, train loss: 0.01321, val loss: 0.06149, in 0.197s\n",
      "[1548/3000] 1 tree, 31 leaves, max depth = 10, train loss: 0.01320, val loss: 0.06147, in 0.279s\n",
      "[1549/3000] 1 tree, 31 leaves, max depth = 14, train loss: 0.01319, val loss: 0.06148, in 0.228s\n",
      "[1550/3000] 1 tree, 31 leaves, max depth = 10, train loss: 0.01317, val loss: 0.06148, in 0.249s\n",
      "[1551/3000] 1 tree, 31 leaves, max depth = 15, train loss: 0.01316, val loss: 0.06148, in 0.214s\n",
      "[1552/3000] 1 tree, 31 leaves, max depth = 11, train loss: 0.01314, val loss: 0.06147, in 0.217s\n",
      "[1553/3000] 1 tree, 31 leaves, max depth = 13, train loss: 0.01312, val loss: 0.06147, in 0.262s\n",
      "[1554/3000] 1 tree, 31 leaves, max depth = 11, train loss: 0.01311, val loss: 0.06146, in 0.237s\n",
      "[1555/3000] 1 tree, 31 leaves, max depth = 12, train loss: 0.01309, val loss: 0.06145, in 0.193s\n",
      "[1556/3000] 1 tree, 31 leaves, max depth = 12, train loss: 0.01308, val loss: 0.06146, in 0.253s\n",
      "[1557/3000] 1 tree, 31 leaves, max depth = 10, train loss: 0.01306, val loss: 0.06147, in 0.228s\n",
      "[1558/3000] 1 tree, 31 leaves, max depth = 9, train loss: 0.01304, val loss: 0.06146, in 0.270s\n",
      "[1559/3000] 1 tree, 31 leaves, max depth = 13, train loss: 0.01303, val loss: 0.06146, in 0.215s\n",
      "[1560/3000] 1 tree, 31 leaves, max depth = 10, train loss: 0.01301, val loss: 0.06149, in 0.200s\n",
      "[1561/3000] 1 tree, 31 leaves, max depth = 10, train loss: 0.01300, val loss: 0.06148, in 0.353s\n",
      "[1562/3000] 1 tree, 31 leaves, max depth = 9, train loss: 0.01298, val loss: 0.06148, in 0.275s\n",
      "[1563/3000] 1 tree, 31 leaves, max depth = 11, train loss: 0.01296, val loss: 0.06147, in 0.298s\n",
      "[1564/3000] 1 tree, 31 leaves, max depth = 14, train loss: 0.01295, val loss: 0.06146, in 0.209s\n",
      "[1565/3000] 1 tree, 31 leaves, max depth = 10, train loss: 0.01294, val loss: 0.06147, in 0.215s\n",
      "[1566/3000] 1 tree, 31 leaves, max depth = 13, train loss: 0.01292, val loss: 0.06146, in 0.206s\n",
      "[1567/3000] 1 tree, 31 leaves, max depth = 13, train loss: 0.01290, val loss: 0.06146, in 0.221s\n",
      "[1568/3000] 1 tree, 31 leaves, max depth = 11, train loss: 0.01289, val loss: 0.06144, in 0.229s\n",
      "[1569/3000] 1 tree, 31 leaves, max depth = 12, train loss: 0.01288, val loss: 0.06145, in 0.198s\n",
      "[1570/3000] 1 tree, 31 leaves, max depth = 10, train loss: 0.01287, val loss: 0.06145, in 0.246s\n",
      "[1571/3000] 1 tree, 31 leaves, max depth = 11, train loss: 0.01285, val loss: 0.06145, in 0.243s\n",
      "[1572/3000] 1 tree, 31 leaves, max depth = 9, train loss: 0.01283, val loss: 0.06144, in 0.259s\n",
      "[1573/3000] 1 tree, 31 leaves, max depth = 11, train loss: 0.01282, val loss: 0.06143, in 0.196s\n",
      "[1574/3000] 1 tree, 31 leaves, max depth = 12, train loss: 0.01281, val loss: 0.06142, in 0.203s\n",
      "[1575/3000] 1 tree, 31 leaves, max depth = 10, train loss: 0.01280, val loss: 0.06144, in 0.285s\n",
      "[1576/3000] 1 tree, 31 leaves, max depth = 12, train loss: 0.01278, val loss: 0.06142, in 0.242s\n",
      "[1577/3000] 1 tree, 31 leaves, max depth = 10, train loss: 0.01276, val loss: 0.06143, in 0.221s\n",
      "[1578/3000] 1 tree, 31 leaves, max depth = 13, train loss: 0.01275, val loss: 0.06144, in 0.221s\n",
      "[1579/3000] 1 tree, 31 leaves, max depth = 12, train loss: 0.01274, val loss: 0.06143, in 0.233s\n",
      "[1580/3000] 1 tree, 31 leaves, max depth = 14, train loss: 0.01272, val loss: 0.06143, in 0.247s\n",
      "[1581/3000] 1 tree, 31 leaves, max depth = 13, train loss: 0.01271, val loss: 0.06142, in 0.386s\n",
      "[1582/3000] 1 tree, 31 leaves, max depth = 11, train loss: 0.01270, val loss: 0.06143, in 0.284s\n",
      "[1583/3000] 1 tree, 31 leaves, max depth = 13, train loss: 0.01268, val loss: 0.06143, in 0.280s\n",
      "[1584/3000] 1 tree, 31 leaves, max depth = 13, train loss: 0.01266, val loss: 0.06142, in 0.277s\n",
      "[1585/3000] 1 tree, 31 leaves, max depth = 10, train loss: 0.01265, val loss: 0.06142, in 0.248s\n",
      "[1586/3000] 1 tree, 31 leaves, max depth = 11, train loss: 0.01263, val loss: 0.06142, in 0.250s\n",
      "[1587/3000] 1 tree, 31 leaves, max depth = 10, train loss: 0.01262, val loss: 0.06142, in 0.227s\n",
      "[1588/3000] 1 tree, 31 leaves, max depth = 9, train loss: 0.01261, val loss: 0.06141, in 0.202s\n",
      "[1589/3000] 1 tree, 31 leaves, max depth = 9, train loss: 0.01260, val loss: 0.06142, in 0.207s\n",
      "[1590/3000] 1 tree, 31 leaves, max depth = 9, train loss: 0.01259, val loss: 0.06142, in 0.235s\n",
      "[1591/3000] 1 tree, 31 leaves, max depth = 13, train loss: 0.01258, val loss: 0.06143, in 0.213s\n",
      "[1592/3000] 1 tree, 31 leaves, max depth = 16, train loss: 0.01257, val loss: 0.06143, in 0.198s\n",
      "[1593/3000] 1 tree, 31 leaves, max depth = 12, train loss: 0.01256, val loss: 0.06142, in 0.191s\n",
      "[1594/3000] 1 tree, 31 leaves, max depth = 18, train loss: 0.01255, val loss: 0.06142, in 0.274s\n",
      "[1595/3000] 1 tree, 31 leaves, max depth = 18, train loss: 0.01253, val loss: 0.06142, in 0.209s\n",
      "[1596/3000] 1 tree, 31 leaves, max depth = 12, train loss: 0.01252, val loss: 0.06141, in 0.267s\n",
      "[1597/3000] 1 tree, 31 leaves, max depth = 10, train loss: 0.01250, val loss: 0.06141, in 0.234s\n",
      "[1598/3000] 1 tree, 31 leaves, max depth = 10, train loss: 0.01248, val loss: 0.06140, in 0.262s\n",
      "[1599/3000] 1 tree, 31 leaves, max depth = 13, train loss: 0.01246, val loss: 0.06141, in 0.272s\n",
      "[1600/3000] 1 tree, 31 leaves, max depth = 10, train loss: 0.01245, val loss: 0.06141, in 0.198s\n",
      "[1601/3000] 1 tree, 31 leaves, max depth = 13, train loss: 0.01244, val loss: 0.06140, in 0.260s\n",
      "[1602/3000] 1 tree, 31 leaves, max depth = 13, train loss: 0.01243, val loss: 0.06143, in 0.362s\n",
      "[1603/3000] 1 tree, 31 leaves, max depth = 14, train loss: 0.01242, val loss: 0.06142, in 0.292s\n",
      "[1604/3000] 1 tree, 31 leaves, max depth = 13, train loss: 0.01240, val loss: 0.06143, in 0.230s\n",
      "[1605/3000] 1 tree, 31 leaves, max depth = 12, train loss: 0.01238, val loss: 0.06146, in 0.287s\n",
      "[1606/3000] 1 tree, 31 leaves, max depth = 9, train loss: 0.01237, val loss: 0.06147, in 0.255s\n",
      "[1607/3000] 1 tree, 31 leaves, max depth = 9, train loss: 0.01236, val loss: 0.06148, in 0.241s\n",
      "[1608/3000] 1 tree, 31 leaves, max depth = 10, train loss: 0.01234, val loss: 0.06145, in 0.240s\n",
      "[1609/3000] 1 tree, 31 leaves, max depth = 17, train loss: 0.01233, val loss: 0.06146, in 0.235s\n",
      "[1610/3000] 1 tree, 31 leaves, max depth = 9, train loss: 0.01232, val loss: 0.06147, in 0.199s\n",
      "[1611/3000] 1 tree, 31 leaves, max depth = 10, train loss: 0.01231, val loss: 0.06144, in 0.217s\n",
      "[1612/3000] 1 tree, 31 leaves, max depth = 16, train loss: 0.01230, val loss: 0.06144, in 0.224s\n",
      "[1613/3000] 1 tree, 31 leaves, max depth = 14, train loss: 0.01229, val loss: 0.06142, in 0.224s\n",
      "[1614/3000] 1 tree, 31 leaves, max depth = 13, train loss: 0.01228, val loss: 0.06142, in 0.211s\n",
      "[1615/3000] 1 tree, 31 leaves, max depth = 10, train loss: 0.01227, val loss: 0.06141, in 0.231s\n",
      "[1616/3000] 1 tree, 31 leaves, max depth = 14, train loss: 0.01225, val loss: 0.06142, in 0.267s\n",
      "[1617/3000] 1 tree, 31 leaves, max depth = 10, train loss: 0.01223, val loss: 0.06141, in 0.254s\n",
      "[1618/3000] 1 tree, 31 leaves, max depth = 9, train loss: 0.01222, val loss: 0.06140, in 0.209s\n",
      "[1619/3000] 1 tree, 31 leaves, max depth = 14, train loss: 0.01221, val loss: 0.06140, in 0.217s\n",
      "[1620/3000] 1 tree, 31 leaves, max depth = 8, train loss: 0.01220, val loss: 0.06141, in 0.205s\n",
      "[1621/3000] 1 tree, 31 leaves, max depth = 10, train loss: 0.01219, val loss: 0.06140, in 0.210s\n",
      "[1622/3000] 1 tree, 31 leaves, max depth = 15, train loss: 0.01218, val loss: 0.06141, in 0.234s\n",
      "[1623/3000] 1 tree, 31 leaves, max depth = 14, train loss: 0.01217, val loss: 0.06141, in 0.351s\n",
      "[1624/3000] 1 tree, 31 leaves, max depth = 13, train loss: 0.01215, val loss: 0.06140, in 0.243s\n",
      "[1625/3000] 1 tree, 31 leaves, max depth = 12, train loss: 0.01214, val loss: 0.06141, in 0.259s\n",
      "[1626/3000] 1 tree, 31 leaves, max depth = 12, train loss: 0.01212, val loss: 0.06141, in 0.232s\n",
      "[1627/3000] 1 tree, 31 leaves, max depth = 11, train loss: 0.01211, val loss: 0.06140, in 0.205s\n",
      "[1628/3000] 1 tree, 31 leaves, max depth = 14, train loss: 0.01210, val loss: 0.06139, in 0.198s\n",
      "[1629/3000] 1 tree, 31 leaves, max depth = 10, train loss: 0.01208, val loss: 0.06140, in 0.296s\n",
      "[1630/3000] 1 tree, 31 leaves, max depth = 15, train loss: 0.01207, val loss: 0.06140, in 0.272s\n",
      "[1631/3000] 1 tree, 31 leaves, max depth = 13, train loss: 0.01205, val loss: 0.06141, in 0.261s\n",
      "[1632/3000] 1 tree, 31 leaves, max depth = 11, train loss: 0.01204, val loss: 0.06141, in 0.234s\n",
      "[1633/3000] 1 tree, 31 leaves, max depth = 12, train loss: 0.01203, val loss: 0.06142, in 0.217s\n",
      "[1634/3000] 1 tree, 31 leaves, max depth = 9, train loss: 0.01201, val loss: 0.06143, in 0.260s\n",
      "[1635/3000] 1 tree, 31 leaves, max depth = 15, train loss: 0.01200, val loss: 0.06144, in 0.272s\n",
      "[1636/3000] 1 tree, 31 leaves, max depth = 13, train loss: 0.01199, val loss: 0.06143, in 0.204s\n",
      "[1637/3000] 1 tree, 31 leaves, max depth = 11, train loss: 0.01197, val loss: 0.06140, in 0.234s\n",
      "[1638/3000] 1 tree, 31 leaves, max depth = 10, train loss: 0.01195, val loss: 0.06140, in 0.271s\n",
      "[1639/3000] 1 tree, 31 leaves, max depth = 10, train loss: 0.01194, val loss: 0.06140, in 0.243s\n",
      "[1640/3000] 1 tree, 31 leaves, max depth = 18, train loss: 0.01193, val loss: 0.06140, in 0.227s\n",
      "[1641/3000] 1 tree, 31 leaves, max depth = 9, train loss: 0.01192, val loss: 0.06139, in 0.206s\n",
      "[1642/3000] 1 tree, 31 leaves, max depth = 8, train loss: 0.01191, val loss: 0.06139, in 0.195s\n",
      "[1643/3000] 1 tree, 31 leaves, max depth = 12, train loss: 0.01189, val loss: 0.06139, in 0.358s\n",
      "[1644/3000] 1 tree, 31 leaves, max depth = 11, train loss: 0.01188, val loss: 0.06140, in 0.240s\n",
      "[1645/3000] 1 tree, 31 leaves, max depth = 14, train loss: 0.01187, val loss: 0.06139, in 0.233s\n",
      "[1646/3000] 1 tree, 31 leaves, max depth = 13, train loss: 0.01186, val loss: 0.06140, in 0.215s\n",
      "[1647/3000] 1 tree, 31 leaves, max depth = 10, train loss: 0.01185, val loss: 0.06141, in 0.259s\n",
      "[1648/3000] 1 tree, 31 leaves, max depth = 13, train loss: 0.01184, val loss: 0.06144, in 0.208s\n",
      "[1649/3000] 1 tree, 31 leaves, max depth = 9, train loss: 0.01183, val loss: 0.06145, in 0.236s\n",
      "[1650/3000] 1 tree, 31 leaves, max depth = 11, train loss: 0.01182, val loss: 0.06144, in 0.198s\n",
      "[1651/3000] 1 tree, 31 leaves, max depth = 11, train loss: 0.01181, val loss: 0.06144, in 0.271s\n",
      "[1652/3000] 1 tree, 31 leaves, max depth = 11, train loss: 0.01180, val loss: 0.06144, in 0.267s\n",
      "[1653/3000] 1 tree, 31 leaves, max depth = 9, train loss: 0.01178, val loss: 0.06143, in 0.204s\n",
      "[1654/3000] 1 tree, 31 leaves, max depth = 11, train loss: 0.01176, val loss: 0.06143, in 0.279s\n",
      "[1655/3000] 1 tree, 31 leaves, max depth = 17, train loss: 0.01175, val loss: 0.06143, in 0.263s\n",
      "[1656/3000] 1 tree, 31 leaves, max depth = 12, train loss: 0.01173, val loss: 0.06139, in 0.239s\n",
      "[1657/3000] 1 tree, 31 leaves, max depth = 12, train loss: 0.01172, val loss: 0.06139, in 0.209s\n",
      "[1658/3000] 1 tree, 31 leaves, max depth = 12, train loss: 0.01171, val loss: 0.06138, in 0.222s\n",
      "[1659/3000] 1 tree, 31 leaves, max depth = 13, train loss: 0.01169, val loss: 0.06137, in 0.206s\n",
      "[1660/3000] 1 tree, 31 leaves, max depth = 13, train loss: 0.01168, val loss: 0.06138, in 0.212s\n",
      "[1661/3000] 1 tree, 31 leaves, max depth = 10, train loss: 0.01167, val loss: 0.06136, in 0.207s\n",
      "[1662/3000] 1 tree, 31 leaves, max depth = 11, train loss: 0.01166, val loss: 0.06136, in 0.222s\n",
      "[1663/3000] 1 tree, 31 leaves, max depth = 17, train loss: 0.01165, val loss: 0.06135, in 0.384s\n",
      "[1664/3000] 1 tree, 31 leaves, max depth = 12, train loss: 0.01163, val loss: 0.06135, in 0.293s\n",
      "[1665/3000] 1 tree, 31 leaves, max depth = 11, train loss: 0.01162, val loss: 0.06136, in 0.290s\n",
      "[1666/3000] 1 tree, 31 leaves, max depth = 11, train loss: 0.01161, val loss: 0.06135, in 0.225s\n",
      "[1667/3000] 1 tree, 31 leaves, max depth = 11, train loss: 0.01159, val loss: 0.06136, in 0.237s\n",
      "[1668/3000] 1 tree, 31 leaves, max depth = 10, train loss: 0.01157, val loss: 0.06135, in 0.233s\n",
      "[1669/3000] 1 tree, 31 leaves, max depth = 11, train loss: 0.01156, val loss: 0.06135, in 0.232s\n",
      "[1670/3000] 1 tree, 31 leaves, max depth = 11, train loss: 0.01155, val loss: 0.06136, in 0.220s\n",
      "[1671/3000] 1 tree, 31 leaves, max depth = 12, train loss: 0.01153, val loss: 0.06136, in 0.215s\n",
      "[1672/3000] 1 tree, 31 leaves, max depth = 12, train loss: 0.01152, val loss: 0.06134, in 0.249s\n",
      "[1673/3000] 1 tree, 31 leaves, max depth = 13, train loss: 0.01150, val loss: 0.06135, in 0.283s\n",
      "[1674/3000] 1 tree, 31 leaves, max depth = 15, train loss: 0.01149, val loss: 0.06136, in 0.241s\n",
      "[1675/3000] 1 tree, 31 leaves, max depth = 11, train loss: 0.01148, val loss: 0.06135, in 0.235s\n",
      "[1676/3000] 1 tree, 31 leaves, max depth = 9, train loss: 0.01147, val loss: 0.06134, in 0.191s\n",
      "[1677/3000] 1 tree, 31 leaves, max depth = 9, train loss: 0.01146, val loss: 0.06134, in 0.262s\n",
      "[1678/3000] 1 tree, 31 leaves, max depth = 10, train loss: 0.01144, val loss: 0.06133, in 0.210s\n",
      "[1679/3000] 1 tree, 31 leaves, max depth = 10, train loss: 0.01143, val loss: 0.06134, in 0.225s\n",
      "[1680/3000] 1 tree, 31 leaves, max depth = 12, train loss: 0.01142, val loss: 0.06134, in 0.246s\n",
      "[1681/3000] 1 tree, 31 leaves, max depth = 10, train loss: 0.01140, val loss: 0.06133, in 0.266s\n",
      "[1682/3000] 1 tree, 31 leaves, max depth = 11, train loss: 0.01139, val loss: 0.06133, in 0.236s\n",
      "[1683/3000] 1 tree, 31 leaves, max depth = 11, train loss: 0.01137, val loss: 0.06134, in 0.328s\n",
      "[1684/3000] 1 tree, 31 leaves, max depth = 11, train loss: 0.01136, val loss: 0.06135, in 0.366s\n",
      "[1685/3000] 1 tree, 31 leaves, max depth = 10, train loss: 0.01135, val loss: 0.06135, in 0.290s\n",
      "[1686/3000] 1 tree, 31 leaves, max depth = 10, train loss: 0.01134, val loss: 0.06137, in 0.248s\n",
      "[1687/3000] 1 tree, 31 leaves, max depth = 10, train loss: 0.01133, val loss: 0.06137, in 0.250s\n",
      "[1688/3000] 1 tree, 31 leaves, max depth = 12, train loss: 0.01131, val loss: 0.06135, in 0.268s\n",
      "[1689/3000] 1 tree, 31 leaves, max depth = 11, train loss: 0.01130, val loss: 0.06134, in 0.216s\n",
      "[1690/3000] 1 tree, 31 leaves, max depth = 11, train loss: 0.01129, val loss: 0.06135, in 0.210s\n",
      "[1691/3000] 1 tree, 31 leaves, max depth = 10, train loss: 0.01128, val loss: 0.06135, in 0.226s\n",
      "[1692/3000] 1 tree, 31 leaves, max depth = 10, train loss: 0.01127, val loss: 0.06135, in 0.213s\n",
      "[1693/3000] 1 tree, 31 leaves, max depth = 10, train loss: 0.01125, val loss: 0.06135, in 0.221s\n",
      "[1694/3000] 1 tree, 31 leaves, max depth = 11, train loss: 0.01124, val loss: 0.06134, in 0.242s\n",
      "[1695/3000] 1 tree, 31 leaves, max depth = 8, train loss: 0.01123, val loss: 0.06134, in 0.208s\n",
      "[1696/3000] 1 tree, 31 leaves, max depth = 13, train loss: 0.01121, val loss: 0.06133, in 0.222s\n",
      "[1697/3000] 1 tree, 31 leaves, max depth = 11, train loss: 0.01119, val loss: 0.06130, in 0.230s\n",
      "[1698/3000] 1 tree, 31 leaves, max depth = 10, train loss: 0.01118, val loss: 0.06130, in 0.218s\n",
      "[1699/3000] 1 tree, 31 leaves, max depth = 10, train loss: 0.01117, val loss: 0.06130, in 0.237s\n",
      "[1700/3000] 1 tree, 31 leaves, max depth = 9, train loss: 0.01116, val loss: 0.06129, in 0.207s\n",
      "[1701/3000] 1 tree, 31 leaves, max depth = 10, train loss: 0.01115, val loss: 0.06129, in 0.189s\n",
      "[1702/3000] 1 tree, 31 leaves, max depth = 11, train loss: 0.01114, val loss: 0.06130, in 0.298s\n",
      "[1703/3000] 1 tree, 31 leaves, max depth = 15, train loss: 0.01113, val loss: 0.06129, in 0.228s\n",
      "[1704/3000] 1 tree, 31 leaves, max depth = 10, train loss: 0.01112, val loss: 0.06128, in 0.384s\n",
      "[1705/3000] 1 tree, 31 leaves, max depth = 11, train loss: 0.01111, val loss: 0.06129, in 0.212s\n",
      "[1706/3000] 1 tree, 31 leaves, max depth = 11, train loss: 0.01110, val loss: 0.06130, in 0.296s\n",
      "[1707/3000] 1 tree, 31 leaves, max depth = 12, train loss: 0.01109, val loss: 0.06127, in 0.246s\n",
      "[1708/3000] 1 tree, 31 leaves, max depth = 9, train loss: 0.01108, val loss: 0.06128, in 0.203s\n",
      "[1709/3000] 1 tree, 31 leaves, max depth = 13, train loss: 0.01107, val loss: 0.06127, in 0.286s\n",
      "[1710/3000] 1 tree, 31 leaves, max depth = 8, train loss: 0.01106, val loss: 0.06126, in 0.207s\n",
      "[1711/3000] 1 tree, 31 leaves, max depth = 12, train loss: 0.01104, val loss: 0.06128, in 0.209s\n",
      "[1712/3000] 1 tree, 31 leaves, max depth = 11, train loss: 0.01103, val loss: 0.06128, in 0.276s\n",
      "[1713/3000] 1 tree, 31 leaves, max depth = 13, train loss: 0.01101, val loss: 0.06127, in 0.225s\n",
      "[1714/3000] 1 tree, 31 leaves, max depth = 9, train loss: 0.01100, val loss: 0.06128, in 0.283s\n",
      "[1715/3000] 1 tree, 31 leaves, max depth = 17, train loss: 0.01099, val loss: 0.06127, in 0.236s\n",
      "[1716/3000] 1 tree, 31 leaves, max depth = 12, train loss: 0.01098, val loss: 0.06126, in 0.237s\n",
      "[1717/3000] 1 tree, 31 leaves, max depth = 13, train loss: 0.01096, val loss: 0.06126, in 0.237s\n",
      "[1718/3000] 1 tree, 31 leaves, max depth = 9, train loss: 0.01095, val loss: 0.06126, in 0.212s\n",
      "[1719/3000] 1 tree, 31 leaves, max depth = 12, train loss: 0.01093, val loss: 0.06127, in 0.269s\n",
      "[1720/3000] 1 tree, 31 leaves, max depth = 12, train loss: 0.01091, val loss: 0.06126, in 0.232s\n",
      "[1721/3000] 1 tree, 31 leaves, max depth = 13, train loss: 0.01090, val loss: 0.06127, in 0.211s\n",
      "[1722/3000] 1 tree, 31 leaves, max depth = 10, train loss: 0.01089, val loss: 0.06126, in 0.194s\n",
      "[1723/3000] 1 tree, 31 leaves, max depth = 9, train loss: 0.01088, val loss: 0.06125, in 0.227s\n",
      "[1724/3000] 1 tree, 31 leaves, max depth = 12, train loss: 0.01087, val loss: 0.06126, in 0.380s\n",
      "[1725/3000] 1 tree, 31 leaves, max depth = 8, train loss: 0.01085, val loss: 0.06126, in 0.314s\n",
      "[1726/3000] 1 tree, 31 leaves, max depth = 13, train loss: 0.01084, val loss: 0.06126, in 0.275s\n",
      "[1727/3000] 1 tree, 31 leaves, max depth = 10, train loss: 0.01083, val loss: 0.06126, in 0.196s\n",
      "[1728/3000] 1 tree, 31 leaves, max depth = 11, train loss: 0.01082, val loss: 0.06126, in 0.277s\n",
      "[1729/3000] 1 tree, 31 leaves, max depth = 14, train loss: 0.01081, val loss: 0.06126, in 0.181s\n",
      "[1730/3000] 1 tree, 31 leaves, max depth = 12, train loss: 0.01081, val loss: 0.06127, in 0.212s\n",
      "[1731/3000] 1 tree, 31 leaves, max depth = 13, train loss: 0.01079, val loss: 0.06128, in 0.280s\n",
      "[1732/3000] 1 tree, 31 leaves, max depth = 12, train loss: 0.01078, val loss: 0.06129, in 0.245s\n",
      "[1733/3000] 1 tree, 31 leaves, max depth = 15, train loss: 0.01077, val loss: 0.06128, in 0.243s\n",
      "[1734/3000] 1 tree, 31 leaves, max depth = 12, train loss: 0.01076, val loss: 0.06127, in 0.212s\n",
      "[1735/3000] 1 tree, 31 leaves, max depth = 14, train loss: 0.01075, val loss: 0.06127, in 0.208s\n",
      "[1736/3000] 1 tree, 31 leaves, max depth = 14, train loss: 0.01074, val loss: 0.06128, in 0.216s\n",
      "[1737/3000] 1 tree, 31 leaves, max depth = 12, train loss: 0.01074, val loss: 0.06128, in 0.196s\n",
      "[1738/3000] 1 tree, 31 leaves, max depth = 13, train loss: 0.01073, val loss: 0.06128, in 0.181s\n",
      "[1739/3000] 1 tree, 31 leaves, max depth = 13, train loss: 0.01071, val loss: 0.06128, in 0.260s\n",
      "[1740/3000] 1 tree, 31 leaves, max depth = 11, train loss: 0.01070, val loss: 0.06127, in 0.232s\n",
      "[1741/3000] 1 tree, 31 leaves, max depth = 9, train loss: 0.01068, val loss: 0.06127, in 0.214s\n",
      "[1742/3000] 1 tree, 31 leaves, max depth = 9, train loss: 0.01067, val loss: 0.06126, in 0.221s\n",
      "[1743/3000] 1 tree, 31 leaves, max depth = 11, train loss: 0.01065, val loss: 0.06125, in 0.257s\n",
      "[1744/3000] 1 tree, 31 leaves, max depth = 12, train loss: 0.01065, val loss: 0.06125, in 0.201s\n",
      "[1745/3000] 1 tree, 31 leaves, max depth = 13, train loss: 0.01064, val loss: 0.06126, in 0.377s\n",
      "[1746/3000] 1 tree, 31 leaves, max depth = 11, train loss: 0.01063, val loss: 0.06128, in 0.261s\n",
      "[1747/3000] 1 tree, 31 leaves, max depth = 10, train loss: 0.01062, val loss: 0.06128, in 0.268s\n",
      "[1748/3000] 1 tree, 31 leaves, max depth = 12, train loss: 0.01060, val loss: 0.06127, in 0.227s\n",
      "[1749/3000] 1 tree, 31 leaves, max depth = 11, train loss: 0.01058, val loss: 0.06126, in 0.325s\n",
      "[1750/3000] 1 tree, 31 leaves, max depth = 13, train loss: 0.01057, val loss: 0.06127, in 0.235s\n",
      "[1751/3000] 1 tree, 31 leaves, max depth = 13, train loss: 0.01056, val loss: 0.06129, in 0.262s\n",
      "[1752/3000] 1 tree, 31 leaves, max depth = 9, train loss: 0.01055, val loss: 0.06128, in 0.245s\n",
      "[1753/3000] 1 tree, 31 leaves, max depth = 13, train loss: 0.01054, val loss: 0.06129, in 0.206s\n",
      "[1754/3000] 1 tree, 31 leaves, max depth = 14, train loss: 0.01053, val loss: 0.06128, in 0.186s\n",
      "[1755/3000] 1 tree, 31 leaves, max depth = 9, train loss: 0.01052, val loss: 0.06126, in 0.275s\n",
      "[1756/3000] 1 tree, 31 leaves, max depth = 13, train loss: 0.01050, val loss: 0.06124, in 0.284s\n",
      "[1757/3000] 1 tree, 31 leaves, max depth = 14, train loss: 0.01049, val loss: 0.06125, in 0.231s\n",
      "[1758/3000] 1 tree, 31 leaves, max depth = 10, train loss: 0.01048, val loss: 0.06127, in 0.263s\n",
      "[1759/3000] 1 tree, 31 leaves, max depth = 8, train loss: 0.01047, val loss: 0.06128, in 0.220s\n",
      "[1760/3000] 1 tree, 31 leaves, max depth = 12, train loss: 0.01046, val loss: 0.06129, in 0.241s\n",
      "[1761/3000] 1 tree, 31 leaves, max depth = 11, train loss: 0.01044, val loss: 0.06128, in 0.249s\n",
      "[1762/3000] 1 tree, 31 leaves, max depth = 12, train loss: 0.01043, val loss: 0.06127, in 0.233s\n",
      "[1763/3000] 1 tree, 31 leaves, max depth = 11, train loss: 0.01042, val loss: 0.06128, in 0.227s\n",
      "[1764/3000] 1 tree, 31 leaves, max depth = 12, train loss: 0.01041, val loss: 0.06129, in 0.240s\n",
      "[1765/3000] 1 tree, 31 leaves, max depth = 15, train loss: 0.01040, val loss: 0.06130, in 0.353s\n",
      "[1766/3000] 1 tree, 31 leaves, max depth = 11, train loss: 0.01039, val loss: 0.06129, in 0.236s\n",
      "[1767/3000] 1 tree, 31 leaves, max depth = 12, train loss: 0.01038, val loss: 0.06131, in 0.538s\n",
      "[1768/3000] 1 tree, 31 leaves, max depth = 12, train loss: 0.01037, val loss: 0.06131, in 0.233s\n",
      "[1769/3000] 1 tree, 31 leaves, max depth = 11, train loss: 0.01036, val loss: 0.06132, in 0.253s\n",
      "[1770/3000] 1 tree, 31 leaves, max depth = 13, train loss: 0.01035, val loss: 0.06131, in 0.248s\n",
      "[1771/3000] 1 tree, 31 leaves, max depth = 17, train loss: 0.01034, val loss: 0.06131, in 0.253s\n",
      "[1772/3000] 1 tree, 31 leaves, max depth = 11, train loss: 0.01033, val loss: 0.06129, in 0.208s\n",
      "[1773/3000] 1 tree, 31 leaves, max depth = 12, train loss: 0.01032, val loss: 0.06129, in 0.251s\n",
      "[1774/3000] 1 tree, 31 leaves, max depth = 10, train loss: 0.01030, val loss: 0.06129, in 0.220s\n",
      "[1775/3000] 1 tree, 31 leaves, max depth = 11, train loss: 0.01029, val loss: 0.06128, in 0.207s\n",
      "[1776/3000] 1 tree, 31 leaves, max depth = 14, train loss: 0.01028, val loss: 0.06127, in 0.266s\n",
      "[1777/3000] 1 tree, 31 leaves, max depth = 9, train loss: 0.01028, val loss: 0.06127, in 0.193s\n",
      "[1778/3000] 1 tree, 31 leaves, max depth = 9, train loss: 0.01027, val loss: 0.06127, in 0.212s\n",
      "[1779/3000] 1 tree, 31 leaves, max depth = 11, train loss: 0.01026, val loss: 0.06125, in 0.287s\n",
      "[1780/3000] 1 tree, 31 leaves, max depth = 9, train loss: 0.01025, val loss: 0.06124, in 0.284s\n",
      "[1781/3000] 1 tree, 31 leaves, max depth = 10, train loss: 0.01023, val loss: 0.06123, in 0.240s\n",
      "[1782/3000] 1 tree, 31 leaves, max depth = 11, train loss: 0.01022, val loss: 0.06122, in 0.270s\n",
      "[1783/3000] 1 tree, 31 leaves, max depth = 16, train loss: 0.01021, val loss: 0.06121, in 0.259s\n",
      "[1784/3000] 1 tree, 31 leaves, max depth = 12, train loss: 0.01020, val loss: 0.06121, in 0.218s\n",
      "[1785/3000] 1 tree, 31 leaves, max depth = 14, train loss: 0.01019, val loss: 0.06122, in 0.234s\n",
      "[1786/3000] 1 tree, 31 leaves, max depth = 15, train loss: 0.01017, val loss: 0.06123, in 0.407s\n",
      "[1787/3000] 1 tree, 31 leaves, max depth = 10, train loss: 0.01016, val loss: 0.06124, in 0.318s\n",
      "[1788/3000] 1 tree, 31 leaves, max depth = 11, train loss: 0.01015, val loss: 0.06123, in 0.238s\n",
      "[1789/3000] 1 tree, 31 leaves, max depth = 9, train loss: 0.01013, val loss: 0.06121, in 0.262s\n",
      "[1790/3000] 1 tree, 31 leaves, max depth = 14, train loss: 0.01012, val loss: 0.06120, in 0.193s\n",
      "[1791/3000] 1 tree, 31 leaves, max depth = 12, train loss: 0.01010, val loss: 0.06121, in 0.242s\n",
      "[1792/3000] 1 tree, 31 leaves, max depth = 12, train loss: 0.01009, val loss: 0.06120, in 0.241s\n",
      "[1793/3000] 1 tree, 31 leaves, max depth = 10, train loss: 0.01008, val loss: 0.06119, in 0.268s\n",
      "[1794/3000] 1 tree, 31 leaves, max depth = 9, train loss: 0.01007, val loss: 0.06118, in 0.192s\n",
      "[1795/3000] 1 tree, 31 leaves, max depth = 13, train loss: 0.01005, val loss: 0.06117, in 0.245s\n",
      "[1796/3000] 1 tree, 31 leaves, max depth = 15, train loss: 0.01004, val loss: 0.06116, in 0.243s\n",
      "[1797/3000] 1 tree, 31 leaves, max depth = 12, train loss: 0.01003, val loss: 0.06116, in 0.240s\n",
      "[1798/3000] 1 tree, 31 leaves, max depth = 15, train loss: 0.01002, val loss: 0.06115, in 0.225s\n",
      "[1799/3000] 1 tree, 31 leaves, max depth = 11, train loss: 0.01001, val loss: 0.06114, in 0.220s\n",
      "[1800/3000] 1 tree, 31 leaves, max depth = 10, train loss: 0.01000, val loss: 0.06113, in 0.210s\n",
      "[1801/3000] 1 tree, 31 leaves, max depth = 9, train loss: 0.00999, val loss: 0.06115, in 0.211s\n",
      "[1802/3000] 1 tree, 31 leaves, max depth = 11, train loss: 0.00997, val loss: 0.06115, in 0.219s\n",
      "[1803/3000] 1 tree, 31 leaves, max depth = 11, train loss: 0.00996, val loss: 0.06114, in 0.261s\n",
      "[1804/3000] 1 tree, 31 leaves, max depth = 11, train loss: 0.00995, val loss: 0.06113, in 0.223s\n",
      "[1805/3000] 1 tree, 31 leaves, max depth = 12, train loss: 0.00994, val loss: 0.06111, in 0.238s\n",
      "[1806/3000] 1 tree, 31 leaves, max depth = 14, train loss: 0.00993, val loss: 0.06112, in 0.339s\n",
      "[1807/3000] 1 tree, 31 leaves, max depth = 10, train loss: 0.00992, val loss: 0.06112, in 0.258s\n",
      "[1808/3000] 1 tree, 31 leaves, max depth = 9, train loss: 0.00991, val loss: 0.06114, in 0.268s\n",
      "[1809/3000] 1 tree, 31 leaves, max depth = 8, train loss: 0.00990, val loss: 0.06115, in 0.223s\n",
      "[1810/3000] 1 tree, 31 leaves, max depth = 10, train loss: 0.00989, val loss: 0.06113, in 0.208s\n",
      "[1811/3000] 1 tree, 31 leaves, max depth = 11, train loss: 0.00988, val loss: 0.06112, in 0.217s\n",
      "[1812/3000] 1 tree, 31 leaves, max depth = 13, train loss: 0.00987, val loss: 0.06112, in 0.234s\n",
      "[1813/3000] 1 tree, 31 leaves, max depth = 11, train loss: 0.00986, val loss: 0.06112, in 0.207s\n",
      "[1814/3000] 1 tree, 31 leaves, max depth = 11, train loss: 0.00985, val loss: 0.06111, in 0.194s\n",
      "[1815/3000] 1 tree, 31 leaves, max depth = 11, train loss: 0.00984, val loss: 0.06111, in 0.250s\n",
      "[1816/3000] 1 tree, 31 leaves, max depth = 10, train loss: 0.00982, val loss: 0.06107, in 0.282s\n",
      "[1817/3000] 1 tree, 31 leaves, max depth = 10, train loss: 0.00981, val loss: 0.06109, in 0.238s\n",
      "[1818/3000] 1 tree, 31 leaves, max depth = 11, train loss: 0.00979, val loss: 0.06109, in 0.243s\n",
      "[1819/3000] 1 tree, 31 leaves, max depth = 11, train loss: 0.00978, val loss: 0.06107, in 0.212s\n",
      "[1820/3000] 1 tree, 31 leaves, max depth = 13, train loss: 0.00977, val loss: 0.06107, in 0.223s\n",
      "[1821/3000] 1 tree, 31 leaves, max depth = 8, train loss: 0.00976, val loss: 0.06106, in 0.215s\n",
      "[1822/3000] 1 tree, 31 leaves, max depth = 12, train loss: 0.00975, val loss: 0.06109, in 0.230s\n",
      "[1823/3000] 1 tree, 31 leaves, max depth = 10, train loss: 0.00974, val loss: 0.06110, in 0.207s\n",
      "[1824/3000] 1 tree, 31 leaves, max depth = 12, train loss: 0.00973, val loss: 0.06109, in 0.232s\n",
      "[1825/3000] 1 tree, 31 leaves, max depth = 13, train loss: 0.00971, val loss: 0.06110, in 0.241s\n",
      "[1826/3000] 1 tree, 31 leaves, max depth = 10, train loss: 0.00970, val loss: 0.06109, in 0.210s\n",
      "[1827/3000] 1 tree, 31 leaves, max depth = 12, train loss: 0.00969, val loss: 0.06108, in 0.403s\n",
      "[1828/3000] 1 tree, 31 leaves, max depth = 11, train loss: 0.00967, val loss: 0.06110, in 0.278s\n",
      "[1829/3000] 1 tree, 31 leaves, max depth = 10, train loss: 0.00966, val loss: 0.06109, in 0.299s\n",
      "[1830/3000] 1 tree, 31 leaves, max depth = 11, train loss: 0.00965, val loss: 0.06109, in 0.239s\n",
      "[1831/3000] 1 tree, 31 leaves, max depth = 10, train loss: 0.00964, val loss: 0.06108, in 0.279s\n",
      "[1832/3000] 1 tree, 31 leaves, max depth = 10, train loss: 0.00963, val loss: 0.06110, in 0.292s\n",
      "[1833/3000] 1 tree, 31 leaves, max depth = 13, train loss: 0.00963, val loss: 0.06110, in 0.194s\n",
      "[1834/3000] 1 tree, 31 leaves, max depth = 11, train loss: 0.00962, val loss: 0.06111, in 0.253s\n",
      "[1835/3000] 1 tree, 31 leaves, max depth = 12, train loss: 0.00961, val loss: 0.06111, in 0.191s\n",
      "[1836/3000] 1 tree, 31 leaves, max depth = 9, train loss: 0.00960, val loss: 0.06110, in 0.269s\n",
      "[1837/3000] 1 tree, 31 leaves, max depth = 11, train loss: 0.00959, val loss: 0.06110, in 0.229s\n",
      "[1838/3000] 1 tree, 31 leaves, max depth = 12, train loss: 0.00958, val loss: 0.06111, in 0.220s\n",
      "[1839/3000] 1 tree, 31 leaves, max depth = 10, train loss: 0.00956, val loss: 0.06110, in 0.293s\n",
      "[1840/3000] 1 tree, 31 leaves, max depth = 9, train loss: 0.00955, val loss: 0.06110, in 0.217s\n",
      "[1841/3000] 1 tree, 31 leaves, max depth = 10, train loss: 0.00954, val loss: 0.06110, in 0.208s\n",
      "[1842/3000] 1 tree, 31 leaves, max depth = 10, train loss: 0.00953, val loss: 0.06109, in 0.210s\n",
      "[1843/3000] 1 tree, 31 leaves, max depth = 13, train loss: 0.00952, val loss: 0.06108, in 0.213s\n",
      "[1844/3000] 1 tree, 31 leaves, max depth = 10, train loss: 0.00951, val loss: 0.06108, in 0.211s\n",
      "[1845/3000] 1 tree, 31 leaves, max depth = 14, train loss: 0.00950, val loss: 0.06108, in 0.248s\n",
      "[1846/3000] 1 tree, 31 leaves, max depth = 7, train loss: 0.00949, val loss: 0.06107, in 0.278s\n",
      "[1847/3000] 1 tree, 31 leaves, max depth = 14, train loss: 0.00948, val loss: 0.06108, in 0.436s\n",
      "[1848/3000] 1 tree, 31 leaves, max depth = 14, train loss: 0.00946, val loss: 0.06106, in 0.280s\n",
      "[1849/3000] 1 tree, 31 leaves, max depth = 8, train loss: 0.00946, val loss: 0.06106, in 0.224s\n",
      "[1850/3000] 1 tree, 31 leaves, max depth = 13, train loss: 0.00945, val loss: 0.06107, in 0.301s\n",
      "[1851/3000] 1 tree, 31 leaves, max depth = 11, train loss: 0.00943, val loss: 0.06106, in 0.243s\n",
      "[1852/3000] 1 tree, 31 leaves, max depth = 16, train loss: 0.00942, val loss: 0.06108, in 0.224s\n",
      "[1853/3000] 1 tree, 31 leaves, max depth = 11, train loss: 0.00941, val loss: 0.06109, in 0.268s\n",
      "[1854/3000] 1 tree, 31 leaves, max depth = 12, train loss: 0.00940, val loss: 0.06110, in 0.281s\n",
      "[1855/3000] 1 tree, 31 leaves, max depth = 15, train loss: 0.00940, val loss: 0.06110, in 0.220s\n",
      "[1856/3000] 1 tree, 31 leaves, max depth = 12, train loss: 0.00938, val loss: 0.06110, in 0.264s\n",
      "[1857/3000] 1 tree, 31 leaves, max depth = 15, train loss: 0.00937, val loss: 0.06111, in 0.240s\n",
      "[1858/3000] 1 tree, 31 leaves, max depth = 12, train loss: 0.00936, val loss: 0.06109, in 0.233s\n",
      "[1859/3000] 1 tree, 31 leaves, max depth = 13, train loss: 0.00935, val loss: 0.06109, in 0.200s\n",
      "[1860/3000] 1 tree, 31 leaves, max depth = 9, train loss: 0.00934, val loss: 0.06109, in 0.227s\n",
      "[1861/3000] 1 tree, 31 leaves, max depth = 14, train loss: 0.00933, val loss: 0.06108, in 0.242s\n",
      "[1862/3000] 1 tree, 31 leaves, max depth = 9, train loss: 0.00932, val loss: 0.06109, in 0.212s\n",
      "[1863/3000] 1 tree, 31 leaves, max depth = 8, train loss: 0.00931, val loss: 0.06110, in 0.255s\n",
      "[1864/3000] 1 tree, 31 leaves, max depth = 13, train loss: 0.00930, val loss: 0.06112, in 0.278s\n",
      "[1865/3000] 1 tree, 31 leaves, max depth = 11, train loss: 0.00929, val loss: 0.06111, in 0.196s\n",
      "[1866/3000] 1 tree, 31 leaves, max depth = 18, train loss: 0.00928, val loss: 0.06112, in 0.255s\n",
      "[1867/3000] 1 tree, 31 leaves, max depth = 12, train loss: 0.00927, val loss: 0.06111, in 0.246s\n",
      "[1868/3000] 1 tree, 31 leaves, max depth = 12, train loss: 0.00926, val loss: 0.06113, in 0.399s\n",
      "[1869/3000] 1 tree, 31 leaves, max depth = 12, train loss: 0.00925, val loss: 0.06113, in 0.245s\n",
      "[1870/3000] 1 tree, 31 leaves, max depth = 11, train loss: 0.00924, val loss: 0.06113, in 0.218s\n",
      "[1871/3000] 1 tree, 31 leaves, max depth = 16, train loss: 0.00923, val loss: 0.06114, in 0.230s\n",
      "[1872/3000] 1 tree, 31 leaves, max depth = 11, train loss: 0.00922, val loss: 0.06114, in 0.256s\n",
      "[1873/3000] 1 tree, 31 leaves, max depth = 15, train loss: 0.00921, val loss: 0.06111, in 0.248s\n",
      "[1874/3000] 1 tree, 31 leaves, max depth = 10, train loss: 0.00920, val loss: 0.06110, in 0.211s\n",
      "[1875/3000] 1 tree, 31 leaves, max depth = 11, train loss: 0.00919, val loss: 0.06109, in 0.284s\n",
      "[1876/3000] 1 tree, 31 leaves, max depth = 12, train loss: 0.00919, val loss: 0.06109, in 0.206s\n",
      "[1877/3000] 1 tree, 31 leaves, max depth = 11, train loss: 0.00918, val loss: 0.06110, in 0.207s\n",
      "[1878/3000] 1 tree, 31 leaves, max depth = 11, train loss: 0.00916, val loss: 0.06111, in 0.268s\n",
      "[1879/3000] 1 tree, 31 leaves, max depth = 12, train loss: 0.00915, val loss: 0.06109, in 0.238s\n",
      "[1880/3000] 1 tree, 31 leaves, max depth = 10, train loss: 0.00914, val loss: 0.06109, in 0.246s\n",
      "[1881/3000] 1 tree, 31 leaves, max depth = 14, train loss: 0.00913, val loss: 0.06108, in 0.203s\n",
      "[1882/3000] 1 tree, 31 leaves, max depth = 13, train loss: 0.00913, val loss: 0.06109, in 0.205s\n",
      "[1883/3000] 1 tree, 31 leaves, max depth = 8, train loss: 0.00912, val loss: 0.06110, in 0.220s\n",
      "[1884/3000] 1 tree, 31 leaves, max depth = 14, train loss: 0.00911, val loss: 0.06110, in 0.210s\n",
      "[1885/3000] 1 tree, 31 leaves, max depth = 12, train loss: 0.00910, val loss: 0.06111, in 0.216s\n",
      "[1886/3000] 1 tree, 31 leaves, max depth = 14, train loss: 0.00910, val loss: 0.06111, in 0.176s\n",
      "[1887/3000] 1 tree, 31 leaves, max depth = 10, train loss: 0.00908, val loss: 0.06109, in 0.280s\n",
      "[1888/3000] 1 tree, 31 leaves, max depth = 13, train loss: 0.00908, val loss: 0.06108, in 0.341s\n",
      "[1889/3000] 1 tree, 31 leaves, max depth = 11, train loss: 0.00907, val loss: 0.06109, in 0.276s\n",
      "[1890/3000] 1 tree, 31 leaves, max depth = 12, train loss: 0.00906, val loss: 0.06108, in 0.298s\n",
      "[1891/3000] 1 tree, 31 leaves, max depth = 10, train loss: 0.00905, val loss: 0.06106, in 0.226s\n",
      "[1892/3000] 1 tree, 31 leaves, max depth = 9, train loss: 0.00904, val loss: 0.06106, in 0.214s\n",
      "[1893/3000] 1 tree, 31 leaves, max depth = 11, train loss: 0.00903, val loss: 0.06105, in 0.290s\n",
      "[1894/3000] 1 tree, 31 leaves, max depth = 12, train loss: 0.00902, val loss: 0.06106, in 0.214s\n",
      "[1895/3000] 1 tree, 31 leaves, max depth = 11, train loss: 0.00901, val loss: 0.06105, in 0.264s\n",
      "[1896/3000] 1 tree, 31 leaves, max depth = 12, train loss: 0.00900, val loss: 0.06105, in 0.241s\n",
      "[1897/3000] 1 tree, 31 leaves, max depth = 11, train loss: 0.00900, val loss: 0.06105, in 0.301s\n",
      "[1898/3000] 1 tree, 31 leaves, max depth = 11, train loss: 0.00899, val loss: 0.06105, in 0.194s\n",
      "[1899/3000] 1 tree, 31 leaves, max depth = 14, train loss: 0.00898, val loss: 0.06103, in 0.228s\n",
      "[1900/3000] 1 tree, 31 leaves, max depth = 10, train loss: 0.00897, val loss: 0.06104, in 0.210s\n",
      "[1901/3000] 1 tree, 31 leaves, max depth = 17, train loss: 0.00897, val loss: 0.06105, in 0.215s\n",
      "[1902/3000] 1 tree, 31 leaves, max depth = 14, train loss: 0.00896, val loss: 0.06105, in 0.196s\n",
      "[1903/3000] 1 tree, 31 leaves, max depth = 11, train loss: 0.00895, val loss: 0.06107, in 0.248s\n",
      "[1904/3000] 1 tree, 31 leaves, max depth = 13, train loss: 0.00894, val loss: 0.06105, in 0.265s\n",
      "[1905/3000] 1 tree, 31 leaves, max depth = 12, train loss: 0.00892, val loss: 0.06104, in 0.229s\n",
      "[1906/3000] 1 tree, 31 leaves, max depth = 14, train loss: 0.00891, val loss: 0.06104, in 0.190s\n",
      "[1907/3000] 1 tree, 31 leaves, max depth = 13, train loss: 0.00890, val loss: 0.06104, in 0.247s\n",
      "[1908/3000] 1 tree, 31 leaves, max depth = 13, train loss: 0.00889, val loss: 0.06105, in 0.223s\n",
      "[1909/3000] 1 tree, 31 leaves, max depth = 10, train loss: 0.00888, val loss: 0.06106, in 0.354s\n",
      "[1910/3000] 1 tree, 31 leaves, max depth = 9, train loss: 0.00887, val loss: 0.06107, in 0.264s\n",
      "[1911/3000] 1 tree, 31 leaves, max depth = 11, train loss: 0.00886, val loss: 0.06107, in 0.280s\n",
      "[1912/3000] 1 tree, 31 leaves, max depth = 11, train loss: 0.00885, val loss: 0.06107, in 0.250s\n",
      "[1913/3000] 1 tree, 31 leaves, max depth = 11, train loss: 0.00884, val loss: 0.06107, in 0.188s\n",
      "[1914/3000] 1 tree, 31 leaves, max depth = 10, train loss: 0.00883, val loss: 0.06107, in 0.280s\n",
      "[1915/3000] 1 tree, 31 leaves, max depth = 10, train loss: 0.00883, val loss: 0.06107, in 0.196s\n",
      "[1916/3000] 1 tree, 31 leaves, max depth = 9, train loss: 0.00882, val loss: 0.06107, in 0.208s\n",
      "[1917/3000] 1 tree, 31 leaves, max depth = 12, train loss: 0.00881, val loss: 0.06107, in 0.253s\n",
      "[1918/3000] 1 tree, 31 leaves, max depth = 12, train loss: 0.00879, val loss: 0.06107, in 0.229s\n",
      "[1919/3000] 1 tree, 31 leaves, max depth = 12, train loss: 0.00879, val loss: 0.06107, in 0.252s\n",
      "[1920/3000] 1 tree, 31 leaves, max depth = 10, train loss: 0.00878, val loss: 0.06106, in 0.195s\n",
      "[1921/3000] 1 tree, 31 leaves, max depth = 11, train loss: 0.00877, val loss: 0.06106, in 0.214s\n",
      "[1922/3000] 1 tree, 31 leaves, max depth = 15, train loss: 0.00876, val loss: 0.06108, in 0.229s\n",
      "[1923/3000] 1 tree, 31 leaves, max depth = 11, train loss: 0.00874, val loss: 0.06110, in 0.280s\n",
      "[1924/3000] 1 tree, 31 leaves, max depth = 12, train loss: 0.00873, val loss: 0.06109, in 0.260s\n",
      "[1925/3000] 1 tree, 31 leaves, max depth = 10, train loss: 0.00873, val loss: 0.06107, in 0.227s\n",
      "[1926/3000] 1 tree, 31 leaves, max depth = 14, train loss: 0.00872, val loss: 0.06106, in 0.235s\n",
      "[1927/3000] 1 tree, 31 leaves, max depth = 13, train loss: 0.00871, val loss: 0.06106, in 0.259s\n",
      "[1928/3000] 1 tree, 31 leaves, max depth = 12, train loss: 0.00870, val loss: 0.06107, in 0.257s\n",
      "[1929/3000] 1 tree, 31 leaves, max depth = 12, train loss: 0.00869, val loss: 0.06108, in 0.351s\n",
      "[1930/3000] 1 tree, 31 leaves, max depth = 10, train loss: 0.00868, val loss: 0.06108, in 0.238s\n",
      "[1931/3000] 1 tree, 31 leaves, max depth = 11, train loss: 0.00867, val loss: 0.06108, in 0.259s\n",
      "[1932/3000] 1 tree, 31 leaves, max depth = 11, train loss: 0.00866, val loss: 0.06108, in 0.225s\n",
      "[1933/3000] 1 tree, 31 leaves, max depth = 19, train loss: 0.00865, val loss: 0.06109, in 0.195s\n",
      "[1934/3000] 1 tree, 31 leaves, max depth = 13, train loss: 0.00865, val loss: 0.06108, in 0.207s\n",
      "[1935/3000] 1 tree, 31 leaves, max depth = 11, train loss: 0.00864, val loss: 0.06109, in 0.196s\n",
      "[1936/3000] 1 tree, 31 leaves, max depth = 13, train loss: 0.00863, val loss: 0.06108, in 0.249s\n",
      "[1937/3000] 1 tree, 31 leaves, max depth = 11, train loss: 0.00862, val loss: 0.06110, in 0.242s\n",
      "[1938/3000] 1 tree, 31 leaves, max depth = 16, train loss: 0.00861, val loss: 0.06111, in 0.220s\n",
      "[1939/3000] 1 tree, 31 leaves, max depth = 12, train loss: 0.00860, val loss: 0.06110, in 0.261s\n",
      "[1940/3000] 1 tree, 31 leaves, max depth = 16, train loss: 0.00860, val loss: 0.06110, in 0.226s\n",
      "[1941/3000] 1 tree, 31 leaves, max depth = 10, train loss: 0.00859, val loss: 0.06111, in 0.207s\n",
      "[1942/3000] 1 tree, 31 leaves, max depth = 12, train loss: 0.00858, val loss: 0.06110, in 0.241s\n",
      "[1943/3000] 1 tree, 31 leaves, max depth = 11, train loss: 0.00857, val loss: 0.06111, in 0.248s\n",
      "[1944/3000] 1 tree, 31 leaves, max depth = 12, train loss: 0.00856, val loss: 0.06112, in 0.258s\n",
      "[1945/3000] 1 tree, 31 leaves, max depth = 12, train loss: 0.00856, val loss: 0.06114, in 0.234s\n",
      "[1946/3000] 1 tree, 31 leaves, max depth = 11, train loss: 0.00855, val loss: 0.06113, in 0.221s\n",
      "[1947/3000] 1 tree, 31 leaves, max depth = 11, train loss: 0.00853, val loss: 0.06113, in 0.284s\n",
      "[1948/3000] 1 tree, 31 leaves, max depth = 11, train loss: 0.00852, val loss: 0.06114, in 0.241s\n",
      "[1949/3000] 1 tree, 31 leaves, max depth = 11, train loss: 0.00852, val loss: 0.06114, in 0.221s\n",
      "[1950/3000] 1 tree, 31 leaves, max depth = 9, train loss: 0.00851, val loss: 0.06113, in 0.334s\n",
      "[1951/3000] 1 tree, 31 leaves, max depth = 10, train loss: 0.00850, val loss: 0.06113, in 0.282s\n",
      "[1952/3000] 1 tree, 31 leaves, max depth = 15, train loss: 0.00849, val loss: 0.06115, in 0.269s\n",
      "[1953/3000] 1 tree, 31 leaves, max depth = 14, train loss: 0.00848, val loss: 0.06116, in 0.258s\n",
      "[1954/3000] 1 tree, 31 leaves, max depth = 13, train loss: 0.00847, val loss: 0.06116, in 0.227s\n",
      "[1955/3000] 1 tree, 31 leaves, max depth = 13, train loss: 0.00846, val loss: 0.06116, in 0.269s\n",
      "[1956/3000] 1 tree, 31 leaves, max depth = 12, train loss: 0.00845, val loss: 0.06114, in 0.213s\n",
      "[1957/3000] 1 tree, 31 leaves, max depth = 13, train loss: 0.00844, val loss: 0.06115, in 0.223s\n",
      "[1958/3000] 1 tree, 31 leaves, max depth = 13, train loss: 0.00843, val loss: 0.06115, in 0.241s\n",
      "[1959/3000] 1 tree, 31 leaves, max depth = 13, train loss: 0.00843, val loss: 0.06115, in 0.196s\n",
      "[1960/3000] 1 tree, 31 leaves, max depth = 12, train loss: 0.00841, val loss: 0.06116, in 0.248s\n",
      "[1961/3000] 1 tree, 31 leaves, max depth = 15, train loss: 0.00841, val loss: 0.06118, in 0.227s\n",
      "[1962/3000] 1 tree, 31 leaves, max depth = 11, train loss: 0.00840, val loss: 0.06119, in 0.257s\n",
      "[1963/3000] 1 tree, 31 leaves, max depth = 11, train loss: 0.00839, val loss: 0.06119, in 0.323s\n",
      "[1964/3000] 1 tree, 31 leaves, max depth = 11, train loss: 0.00838, val loss: 0.06120, in 0.252s\n",
      "[1965/3000] 1 tree, 31 leaves, max depth = 16, train loss: 0.00837, val loss: 0.06121, in 0.272s\n",
      "[1966/3000] 1 tree, 31 leaves, max depth = 10, train loss: 0.00836, val loss: 0.06119, in 0.279s\n",
      "[1967/3000] 1 tree, 31 leaves, max depth = 11, train loss: 0.00835, val loss: 0.06120, in 0.215s\n",
      "[1968/3000] 1 tree, 31 leaves, max depth = 11, train loss: 0.00834, val loss: 0.06120, in 0.237s\n",
      "[1969/3000] 1 tree, 31 leaves, max depth = 9, train loss: 0.00833, val loss: 0.06119, in 0.212s\n",
      "[1970/3000] 1 tree, 31 leaves, max depth = 12, train loss: 0.00832, val loss: 0.06119, in 0.348s\n",
      "[1971/3000] 1 tree, 31 leaves, max depth = 13, train loss: 0.00832, val loss: 0.06119, in 0.316s\n",
      "[1972/3000] 1 tree, 31 leaves, max depth = 11, train loss: 0.00830, val loss: 0.06120, in 0.254s\n",
      "[1973/3000] 1 tree, 31 leaves, max depth = 14, train loss: 0.00830, val loss: 0.06120, in 0.206s\n",
      "[1974/3000] 1 tree, 31 leaves, max depth = 12, train loss: 0.00829, val loss: 0.06120, in 0.202s\n",
      "[1975/3000] 1 tree, 31 leaves, max depth = 10, train loss: 0.00828, val loss: 0.06119, in 0.276s\n",
      "[1976/3000] 1 tree, 31 leaves, max depth = 10, train loss: 0.00827, val loss: 0.06120, in 0.220s\n",
      "[1977/3000] 1 tree, 31 leaves, max depth = 13, train loss: 0.00826, val loss: 0.06119, in 0.235s\n",
      "[1978/3000] 1 tree, 31 leaves, max depth = 9, train loss: 0.00826, val loss: 0.06118, in 0.240s\n",
      "[1979/3000] 1 tree, 31 leaves, max depth = 12, train loss: 0.00825, val loss: 0.06118, in 0.320s\n",
      "[1980/3000] 1 tree, 31 leaves, max depth = 11, train loss: 0.00824, val loss: 0.06119, in 0.197s\n",
      "[1981/3000] 1 tree, 31 leaves, max depth = 9, train loss: 0.00823, val loss: 0.06119, in 0.191s\n",
      "[1982/3000] 1 tree, 31 leaves, max depth = 10, train loss: 0.00822, val loss: 0.06118, in 0.250s\n",
      "[1983/3000] 1 tree, 31 leaves, max depth = 10, train loss: 0.00822, val loss: 0.06119, in 0.211s\n",
      "[1984/3000] 1 tree, 31 leaves, max depth = 11, train loss: 0.00821, val loss: 0.06120, in 0.280s\n",
      "[1985/3000] 1 tree, 31 leaves, max depth = 10, train loss: 0.00820, val loss: 0.06118, in 0.305s\n",
      "[1986/3000] 1 tree, 31 leaves, max depth = 12, train loss: 0.00819, val loss: 0.06118, in 0.222s\n",
      "[1987/3000] 1 tree, 31 leaves, max depth = 13, train loss: 0.00818, val loss: 0.06119, in 0.193s\n",
      "[1988/3000] 1 tree, 31 leaves, max depth = 12, train loss: 0.00817, val loss: 0.06118, in 0.203s\n",
      "[1989/3000] 1 tree, 31 leaves, max depth = 10, train loss: 0.00816, val loss: 0.06120, in 0.285s\n",
      "[1990/3000] 1 tree, 31 leaves, max depth = 10, train loss: 0.00816, val loss: 0.06120, in 0.341s\n",
      "[1991/3000] 1 tree, 31 leaves, max depth = 9, train loss: 0.00815, val loss: 0.06120, in 0.236s\n",
      "[1992/3000] 1 tree, 31 leaves, max depth = 11, train loss: 0.00814, val loss: 0.06120, in 0.243s\n",
      "[1993/3000] 1 tree, 31 leaves, max depth = 12, train loss: 0.00813, val loss: 0.06121, in 0.239s\n",
      "[1994/3000] 1 tree, 31 leaves, max depth = 11, train loss: 0.00812, val loss: 0.06122, in 0.285s\n",
      "[1995/3000] 1 tree, 31 leaves, max depth = 11, train loss: 0.00811, val loss: 0.06122, in 0.230s\n",
      "[1996/3000] 1 tree, 31 leaves, max depth = 10, train loss: 0.00810, val loss: 0.06123, in 0.189s\n",
      "[1997/3000] 1 tree, 31 leaves, max depth = 10, train loss: 0.00809, val loss: 0.06124, in 0.270s\n",
      "[1998/3000] 1 tree, 31 leaves, max depth = 13, train loss: 0.00808, val loss: 0.06123, in 0.230s\n",
      "[1999/3000] 1 tree, 31 leaves, max depth = 11, train loss: 0.00807, val loss: 0.06122, in 0.232s\n",
      "Fit 1999 trees in 500.967 s, (61969 total leaves)\n",
      "Time spent computing histograms: 224.499s\n",
      "Time spent finding best splits:  20.998s\n",
      "Time spent applying splits:      24.953s\n",
      "Time spent predicting:           0.851s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "HistGradientBoostingClassifier(max_iter=3000, min_samples_leaf=50,\n",
       "                               n_iter_no_change=100, verbose=1)"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "histgbc = ensemble.HistGradientBoostingClassifier(max_iter=3000, learning_rate=0.10, \n",
    "                                                  min_samples_leaf=50,\n",
    "                                                  warm_start=False, verbose=1, n_iter_no_change=100)\n",
    "histgbc.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true,
     "source_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing results:\n",
      "accuracy = 0.9975799902015907\n",
      "adjusted balanced accuracy = 0.9910800317618493\n",
      "balanced accuracy = 0.9955400158809247\n",
      "average precision = 0.9882032270658644\n",
      "f1 score = 0.9933771258885989\n",
      "precision = 0.9944330342665658\n",
      "recall = 0.9923234575003596\n",
      "roc = 0.9955400158809247\n",
      "confusion matrix\n",
      " = [[248198    309]\n",
      " [   427  55197]]\n",
      "Testing results:\n",
      "accuracy = 0.9764448258515077\n",
      "adjusted balanced accuracy = 0.9109433069659969\n",
      "balanced accuracy = 0.9554716534829985\n",
      "average precision = 0.8877404590221393\n",
      "f1 score = 0.9345825115055885\n",
      "precision = 0.9470353097934711\n",
      "recall = 0.9224529526281635\n",
      "roc = 0.9554716534829985\n",
      "confusion matrix\n",
      " = [[27311   318]\n",
      " [  478  5686]]\n"
     ]
    }
   ],
   "source": [
    "evaluate_classifier(histgbc, X_train, y_train)\n",
    "evaluate_classifier(histgbc, X_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true,
     "source_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 3 folds for each of 10 candidates, totalling 30 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done   2 tasks      | elapsed:  5.0min\n",
      "[Parallel(n_jobs=-1)]: Done  22 out of  30 | elapsed: 19.8min remaining:  7.2min\n",
      "[Parallel(n_jobs=-1)]: Done  30 out of  30 | elapsed: 27.6min finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RF grid search saved\n"
     ]
    }
   ],
   "source": [
    "rf_search = model_selection.RandomizedSearchCV(\n",
    "    ensemble.RandomForestClassifier(), \n",
    "    param_distributions={\n",
    "        'n_estimators': [10, 100, 300],\n",
    "        'criterion': ['gini', 'entropy'],\n",
    "        'class_weight': [{True: 2, False: 1}, {True: 3, False: 1}, 'balanced', None],\n",
    "        'max_depth': [None],\n",
    "        'min_samples_leaf': [20, 50],\n",
    "        'min_samples_split': [20, 50]\n",
    "    },\n",
    "    n_jobs=-1,\n",
    "    cv=3,\n",
    "    refit='f1',\n",
    "    scoring=['neg_log_loss', 'f1', 'roc_auc'],\n",
    "    verbose=5)\n",
    "rf_search.fit(X_train, y_train)\n",
    "\n",
    "with open(f'./model_gridsearch/{loss_type}/rf', 'wb') as f:\n",
    "    pickle.dump(rf_search, f)\n",
    "    print('RF grid search saved')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true,
     "source_hidden": true
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'class_weight': {True: 2, False: 1},\n",
       " 'criterion': 'entropy',\n",
       " 'max_depth': None,\n",
       " 'min_samples_leaf': 20,\n",
       " 'min_samples_split': 20,\n",
       " 'n_estimators': 300}"
      ]
     },
     "execution_count": 94,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_search_result(rf_search.cv_results_).iloc[0].params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true,
     "source_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend ThreadingBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done  34 tasks      | elapsed:   19.0s\n",
      "[Parallel(n_jobs=-1)]: Done 184 tasks      | elapsed:  1.5min\n",
      "[Parallel(n_jobs=-1)]: Done 300 out of 300 | elapsed:  2.4min finished\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "RandomForestClassifier(class_weight={False: 1, True: 2}, criterion='entropy',\n",
       "                       min_samples_leaf=20, min_samples_split=20,\n",
       "                       n_estimators=300, n_jobs=-1, verbose=1)"
      ]
     },
     "execution_count": 145,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rfc = ensemble.RandomForestClassifier(n_estimators=300, criterion='entropy', min_samples_split=20, min_samples_leaf=20,\n",
    "                                      class_weight={True: 2, False: 1}, max_depth=None,\n",
    "                                      verbose=1, n_jobs=-1)\n",
    "rfc.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true,
     "source_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing results:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=8)]: Using backend ThreadingBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=8)]: Done  34 tasks      | elapsed:    0.6s\n",
      "[Parallel(n_jobs=8)]: Done 184 tasks      | elapsed:    3.7s\n",
      "[Parallel(n_jobs=8)]: Done 300 out of 300 | elapsed:    6.0s finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy = 0.9654260828393028\n",
      "adjusted balanced accuracy = 0.9096303533156793\n",
      "balanced accuracy = 0.9548151766578397\n",
      "average precision = 0.8374575967973328\n",
      "f1 score = 0.9084657236126225\n",
      "precision = 0.8806602420212317\n",
      "recall = 0.9380842801668344\n",
      "roc = 0.9548151766578398\n",
      "confusion matrix\n",
      " = [[241436   7071]\n",
      " [  3444  52180]]\n",
      "Testing results:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=8)]: Using backend ThreadingBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=8)]: Done  34 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=8)]: Done 184 tasks      | elapsed:    0.3s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy = 0.9522978131565709\n",
      "adjusted balanced accuracy = 0.8598565448012625\n",
      "balanced accuracy = 0.9299282724006313\n",
      "average precision = 0.7809113068674557\n",
      "f1 score = 0.8724885302958393\n",
      "precision = 0.8513430071009571\n",
      "recall = 0.8947112264763141\n",
      "roc = 0.9299282724006313\n",
      "confusion matrix\n",
      " = [[26666   963]\n",
      " [  649  5515]]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=8)]: Done 300 out of 300 | elapsed:    0.6s finished\n"
     ]
    }
   ],
   "source": [
    "evaluate_classifier(rfc, X_train, y_train)\n",
    "evaluate_classifier(rfc, X_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Angles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Preparing angle_loss training/test split:\n",
      "Train size: 270339; test size: 67585\n"
     ]
    }
   ],
   "source": [
    "# Preparing train / test split\n",
    "loss_target = 'angle_loss'\n",
    "print(f'Preparing {loss_target} training/test split:')\n",
    "X_train, X_test, y_train, y_test = sklearn.model_selection.train_test_split(\n",
    "    X, y[loss_target], test_size=0.2)\n",
    "print(f'Train size: {len(y_train)}; test size: {len(y_test)}')\n",
    "\n",
    "loss_type = 'angle'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cross Validation of Classifiers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true,
     "source_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logistic Regression: \n",
      "Cross validation results:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done   2 out of   5 | elapsed:  5.0min remaining:  7.5min\n",
      "[Parallel(n_jobs=-1)]: Done   5 out of   5 | elapsed:  5.1min finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fit_time = 299.6866195201874 +- 3.6873566130088293\n",
      "score_time = 0.2133716583251953 +- 0.06860711511724538\n",
      "test_accuracy = 0.9751009059193979 +- 0.0022692552396563113\n",
      "test_adjusted balanced accuracy = 0.7779150710054915 +- 0.010494498078427755\n",
      "test_balanced accuracy = 0.8889575355027457 +- 0.005247249039213878\n",
      "test_average precision = 0.5817958259802498 +- 0.026324119796813157\n",
      "test_f1 score = 0.7552243897757013 +- 0.018117124738683297\n",
      "test_precision = 0.7204832811437185 +- 0.027362749233350324\n",
      "test_recall = 0.793584938038036 +- 0.009006719359555443\n",
      "test_roc = 0.8889575355027457 +- 0.0052472490392140355\n",
      "\n",
      "Neural Network: \n",
      "Cross validation results:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done   2 out of   5 | elapsed:  9.2min remaining: 13.8min\n",
      "[Parallel(n_jobs=-1)]: Done   5 out of   5 | elapsed:  9.8min finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fit_time = 558.1072418689728 +- 36.118177700573995\n",
      "score_time = 0.9879521369934082 +- 0.20136082886929668\n",
      "test_accuracy = 0.9808448020824926 +- 0.0012516006424344666\n",
      "test_adjusted balanced accuracy = 0.7471205136734534 +- 0.038805591812587296\n",
      "test_balanced accuracy = 0.8735602568367268 +- 0.019402795906293648\n",
      "test_average precision = 0.6412064386785936 +- 0.022665448062494414\n",
      "test_f1 score = 0.7921331712034723 +- 0.016407911159906002\n",
      "test_precision = 0.8340588429618728 +- 0.02668714489659445\n",
      "test_recall = 0.7547861989116764 +- 0.040253414038365465\n",
      "test_roc = 0.8735602568367268 +- 0.01940279590629363\n",
      "\n",
      "Decision Tree: \n",
      "Cross validation results:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done   2 out of   5 | elapsed:   39.7s remaining:   59.5s\n",
      "[Parallel(n_jobs=-1)]: Done   5 out of   5 | elapsed:   42.9s finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fit_time = 37.27674903869629 +- 3.4811802008398742\n",
      "score_time = 0.23500680923461914 +- 0.05118937484906137\n",
      "test_accuracy = 0.9685550555655504 +- 0.0025173009025912805\n",
      "test_adjusted balanced accuracy = 0.7062070292750003 +- 0.020035493927589968\n",
      "test_balanced accuracy = 0.8531035146375002 +- 0.010017746963794986\n",
      "test_average precision = 0.4915021213890867 +- 0.025388073521462523\n",
      "test_f1 score = 0.6906560722369144 +- 0.019094023513103194\n",
      "test_precision = 0.6592545884695508 +- 0.023244090131637224\n",
      "test_recall = 0.7252822688709165 +- 0.01925414782426952\n",
      "test_roc = 0.8531035146375 +- 0.010017746963794944\n",
      "\n",
      "Gradient Boosting 3000: \n",
      "Cross validation results:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done   2 out of   5 | elapsed:  3.8min remaining:  5.6min\n",
      "[Parallel(n_jobs=-1)]: Done   5 out of   5 | elapsed: 10.0min finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fit_time = 339.567313337326 +- 292.99106724140415\n",
      "score_time = 6.008761024475097 +- 3.4506987296768936\n",
      "test_accuracy = 0.980980925604906 +- 0.0021965409505142193\n",
      "test_adjusted balanced accuracy = 0.7349139634655735 +- 0.019206006860734193\n",
      "test_balanced accuracy = 0.8674569817327867 +- 0.009603003430367097\n",
      "test_average precision = 0.6403724822675523 +- 0.034388022139072084\n",
      "test_f1 score = 0.7905818170915571 +- 0.021950015241106798\n",
      "test_precision = 0.8463120153406607 +- 0.028985290978058853\n",
      "test_recall = 0.7417710207184334 +- 0.01808240994225528\n",
      "test_roc = 0.8674569817327867 +- 0.009603003430367053\n",
      "\n",
      "Gradient Boosting 500: \n",
      "Cross validation results:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done   2 out of   5 | elapsed:  4.7min remaining:  7.1min\n",
      "[Parallel(n_jobs=-1)]: Done   5 out of   5 | elapsed:  7.4min finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fit_time = 360.5136022567749 +- 165.51192264066242\n",
      "score_time = 6.76694073677063 +- 1.4413513902206838\n",
      "test_accuracy = 0.9815934923803862 +- 0.0012685261069632244\n",
      "test_adjusted balanced accuracy = 0.7409820946381566 +- 0.009927293669996791\n",
      "test_balanced accuracy = 0.8704910473190782 +- 0.0049636468349983955\n",
      "test_average precision = 0.6505820566023814 +- 0.017566010055221733\n",
      "test_f1 score = 0.7971800354164873 +- 0.011066135519999657\n",
      "test_precision = 0.8539893416337684 +- 0.016909428422129582\n",
      "test_recall = 0.7474845793867273 +- 0.009484294576902126\n",
      "test_roc = 0.8704910473190782 +- 0.004963646834998381\n",
      "\n",
      "Random Forest: \n",
      "Cross validation results:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done   2 out of   5 | elapsed:  3.7min remaining:  5.5min\n",
      "[Parallel(n_jobs=-1)]: Done   5 out of   5 | elapsed:  3.7min finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fit_time = 213.15396857261658 +- 4.870501880180108\n",
      "score_time = 1.8173917293548585 +- 0.3860468215429233\n",
      "test_accuracy = 0.9727216750746395 +- 0.0015769003956553503\n",
      "test_adjusted balanced accuracy = 0.7196115434617898 +- 0.007719264818850066\n",
      "test_balanced accuracy = 0.8598057717308949 +- 0.003859632409425033\n",
      "test_average precision = 0.5354336468941999 +- 0.016244833910658\n",
      "test_f1 score = 0.7227725177761641 +- 0.011395975230538546\n",
      "test_precision = 0.7112229687843621 +- 0.020965528881061114\n",
      "test_recall = 0.7347932320551543 +- 0.007751643754368502\n",
      "test_roc = 0.8598057717308949 +- 0.003859632409424984\n",
      "\n"
     ]
    }
   ],
   "source": [
    "loss_target = 'angle_loss'\n",
    "energy_models = {\n",
    "    'Logistic Regression': linear_model.LogisticRegression(C=10.0, class_weight={True: 2, False: 1}, max_iter=1000),\n",
    "    'Neural Network': neural_network.MLPClassifier(alpha=1e-3,\n",
    "                                  hidden_layer_sizes=(150,),\n",
    "                                  max_iter=1000,\n",
    "                                  learning_rate_init=1e-4,\n",
    "                                  early_stopping=True,\n",
    "                                  n_iter_no_change=25,\n",
    "                                  verbose=True),\n",
    "    'Decision Tree': tree.DecisionTreeClassifier(class_weight={True: 2, False: 1}, \n",
    "                                       max_depth=None, min_samples_leaf=50, min_samples_split=50,\n",
    "                                       splitter='random', criterion='entropy'),\n",
    "    'Gradient Boosting 3000': ensemble.HistGradientBoostingClassifier(max_iter=3000, n_iter_no_change=100,\n",
    "                                                  learning_rate=0.1, min_samples_leaf=20, \n",
    "                                                  warm_start=False),\n",
    "    'Gradient Boosting 500': ensemble.HistGradientBoostingClassifier(max_iter=500, n_iter_no_change=100,\n",
    "                                                  learning_rate=0.1, min_samples_leaf=20, \n",
    "                                                  warm_start=False),\n",
    "    'Random Forest': ensemble.RandomForestClassifier(n_estimators=100, criterion='entropy', min_samples_split=50, min_samples_leaf=20,\n",
    "                                      class_weight={True: 3, False: 1}, max_depth=None)\n",
    "}\n",
    "\n",
    "for name, model in energy_models.items():\n",
    "    print(f'{name}: ')\n",
    "    cross_validate_classifier(model, X, y[loss_target], cv=sklearn.model_selection.KFold(n_splits=5, shuffle=True, random_state=42), verbose=5, n_jobs=-1)\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Undersampling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "loss_target = 'angle_loss'\n",
    "angle_models = {\n",
    "    'Decision Tree': tree.DecisionTreeClassifier(max_depth=None, min_samples_leaf=50, min_samples_split=50,\n",
    "                                       splitter='random', criterion='entropy'),\n",
    "    'Logistic Regression': linear_model.LogisticRegression(C=10.0, max_iter=1000),\n",
    "    'Neural Network': neural_network.MLPClassifier(alpha=1e-3,\n",
    "                                  hidden_layer_sizes=(150,),\n",
    "                                  max_iter=1000,\n",
    "                                  learning_rate_init=1e-4,\n",
    "                                  early_stopping=True,\n",
    "                                  n_iter_no_change=25,\n",
    "                                  verbose=True),\n",
    "    'Gradient Boosting 3000': ensemble.HistGradientBoostingClassifier(max_iter=3000, n_iter_no_change=100,\n",
    "                                                  learning_rate=0.1, min_samples_leaf=20, \n",
    "                                                  warm_start=False),\n",
    "    'Gradient Boosting 500': ensemble.HistGradientBoostingClassifier(max_iter=500, n_iter_no_change=100,\n",
    "                                                  learning_rate=0.1, min_samples_leaf=20, \n",
    "                                                  warm_start=False),\n",
    "    'Random Forest': ensemble.RandomForestClassifier(n_estimators=100, criterion='entropy', min_samples_split=50, min_samples_leaf=20, max_depth=None)\n",
    "}\n",
    "\n",
    "undersample_rates = [0.1, 0.2, 0.3, 0.4, 0.5]\n",
    "repeats = 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true,
     "source_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Decision Tree: \n",
      "Decision Tree: undersampling at rate 0.1 from the most popular class:\n",
      "All 5 repeats done! \n",
      "\n",
      "Decision Tree: undersampling at rate 0.2 from the most popular class:\n",
      "All 5 repeats done! \n",
      "\n",
      "Decision Tree: undersampling at rate 0.3 from the most popular class:\n",
      "All 5 repeats done! \n",
      "\n",
      "Decision Tree: undersampling at rate 0.4 from the most popular class:\n",
      "All 5 repeats done! \n",
      "\n",
      "Decision Tree: undersampling at rate 0.5 from the most popular class:\n",
      "All 5 repeats done! \n",
      "\n",
      "Logistic Regression: \n",
      "Logistic Regression: undersampling at rate 0.1 from the most popular class:\n",
      "All 5 repeats done! \n",
      "\n",
      "Logistic Regression: undersampling at rate 0.2 from the most popular class:\n",
      "All 5 repeats done! \n",
      "\n",
      "Logistic Regression: undersampling at rate 0.3 from the most popular class:\n",
      "All 5 repeats done! \n",
      "\n",
      "Logistic Regression: undersampling at rate 0.4 from the most popular class:\n",
      "All 5 repeats done! \n",
      "\n",
      "Logistic Regression: undersampling at rate 0.5 from the most popular class:\n",
      "All 5 repeats done! \n",
      "\n",
      "Neural Network: \n",
      "Neural Network: undersampling at rate 0.1 from the most popular class:\n",
      "Iteration 1, loss = 0.41517406\n",
      "Validation score: 0.903010\n",
      "Iteration 2, loss = 0.24616590\n",
      "Validation score: 0.918189\n",
      "Iteration 3, loss = 0.20480165\n",
      "Validation score: 0.928222\n",
      "Iteration 4, loss = 0.18316369\n",
      "Validation score: 0.933625\n",
      "Iteration 5, loss = 0.16933302\n",
      "Validation score: 0.936455\n",
      "Iteration 6, loss = 0.15938590\n",
      "Validation score: 0.937227\n",
      "Iteration 7, loss = 0.15203974\n",
      "Validation score: 0.937741\n",
      "Iteration 8, loss = 0.14587098\n",
      "Validation score: 0.937227\n",
      "Iteration 9, loss = 0.14090117\n",
      "Validation score: 0.938770\n",
      "Iteration 10, loss = 0.13657110\n",
      "Validation score: 0.939799\n",
      "Iteration 11, loss = 0.13300123\n",
      "Validation score: 0.941343\n",
      "Iteration 12, loss = 0.12959469\n",
      "Validation score: 0.942115\n",
      "Iteration 13, loss = 0.12657560\n",
      "Validation score: 0.943401\n",
      "Iteration 14, loss = 0.12381715\n",
      "Validation score: 0.942372\n",
      "Iteration 15, loss = 0.12130002\n",
      "Validation score: 0.943658\n",
      "Iteration 16, loss = 0.11900350\n",
      "Validation score: 0.944687\n",
      "Iteration 17, loss = 0.11655754\n",
      "Validation score: 0.943144\n",
      "Iteration 18, loss = 0.11484750\n",
      "Validation score: 0.944430\n",
      "Iteration 19, loss = 0.11276691\n",
      "Validation score: 0.944687\n",
      "Iteration 20, loss = 0.11069885\n",
      "Validation score: 0.944430\n",
      "Iteration 21, loss = 0.10885928\n",
      "Validation score: 0.945974\n",
      "Iteration 22, loss = 0.10724622\n",
      "Validation score: 0.946488\n",
      "Iteration 23, loss = 0.10527995\n",
      "Validation score: 0.945716\n",
      "Iteration 24, loss = 0.10371990\n",
      "Validation score: 0.945974\n",
      "Iteration 25, loss = 0.10211209\n",
      "Validation score: 0.945459\n",
      "Iteration 26, loss = 0.10065591\n",
      "Validation score: 0.945974\n",
      "Iteration 27, loss = 0.09908111\n",
      "Validation score: 0.945974\n",
      "Iteration 28, loss = 0.09748979\n",
      "Validation score: 0.945974\n",
      "Iteration 29, loss = 0.09623346\n",
      "Validation score: 0.947260\n",
      "Iteration 30, loss = 0.09480212\n",
      "Validation score: 0.947003\n",
      "Iteration 31, loss = 0.09363724\n",
      "Validation score: 0.947003\n",
      "Iteration 32, loss = 0.09187896\n",
      "Validation score: 0.947003\n",
      "Iteration 33, loss = 0.09075886\n",
      "Validation score: 0.948289\n",
      "Iteration 34, loss = 0.08947126\n",
      "Validation score: 0.948289\n",
      "Iteration 35, loss = 0.08810598\n",
      "Validation score: 0.947775\n",
      "Iteration 36, loss = 0.08674103\n",
      "Validation score: 0.948546\n",
      "Iteration 37, loss = 0.08569065\n",
      "Validation score: 0.947260\n",
      "Iteration 38, loss = 0.08445039\n",
      "Validation score: 0.948804\n",
      "Iteration 39, loss = 0.08341790\n",
      "Validation score: 0.948289\n",
      "Iteration 40, loss = 0.08203256\n",
      "Validation score: 0.950090\n",
      "Iteration 41, loss = 0.08092725\n",
      "Validation score: 0.950347\n",
      "Iteration 42, loss = 0.08001814\n",
      "Validation score: 0.948804\n",
      "Iteration 43, loss = 0.07860889\n",
      "Validation score: 0.950605\n",
      "Iteration 44, loss = 0.07779859\n",
      "Validation score: 0.949061\n",
      "Iteration 45, loss = 0.07664054\n",
      "Validation score: 0.950605\n",
      "Iteration 46, loss = 0.07583525\n",
      "Validation score: 0.949576\n",
      "Iteration 47, loss = 0.07477266\n",
      "Validation score: 0.950347\n",
      "Iteration 48, loss = 0.07358453\n",
      "Validation score: 0.949576\n",
      "Iteration 49, loss = 0.07268404\n",
      "Validation score: 0.948546\n",
      "Iteration 50, loss = 0.07155902\n",
      "Validation score: 0.948804\n",
      "Iteration 51, loss = 0.07091220\n",
      "Validation score: 0.948546\n",
      "Iteration 52, loss = 0.06982164\n",
      "Validation score: 0.947260\n",
      "Iteration 53, loss = 0.06889327\n",
      "Validation score: 0.950090\n",
      "Iteration 54, loss = 0.06778027\n",
      "Validation score: 0.949576\n",
      "Iteration 55, loss = 0.06686380\n",
      "Validation score: 0.949318\n",
      "Iteration 56, loss = 0.06582825\n",
      "Validation score: 0.944945\n",
      "Iteration 57, loss = 0.06512260\n",
      "Validation score: 0.951119\n",
      "Iteration 58, loss = 0.06428769\n",
      "Validation score: 0.949833\n",
      "Iteration 59, loss = 0.06330880\n",
      "Validation score: 0.949833\n",
      "Iteration 60, loss = 0.06246428\n",
      "Validation score: 0.949318\n",
      "Iteration 61, loss = 0.06166403\n",
      "Validation score: 0.948289\n",
      "Iteration 62, loss = 0.06070570\n",
      "Validation score: 0.949833\n",
      "Iteration 63, loss = 0.05995427\n",
      "Validation score: 0.947517\n",
      "Iteration 64, loss = 0.05918794\n",
      "Validation score: 0.949576\n",
      "Iteration 65, loss = 0.05821871\n",
      "Validation score: 0.949833\n",
      "Iteration 66, loss = 0.05758156\n",
      "Validation score: 0.950347\n",
      "Iteration 67, loss = 0.05692972\n",
      "Validation score: 0.949833\n",
      "Iteration 68, loss = 0.05600116\n",
      "Validation score: 0.947775\n",
      "Iteration 69, loss = 0.05538229\n",
      "Validation score: 0.948546\n",
      "Iteration 70, loss = 0.05448199\n",
      "Validation score: 0.947260\n",
      "Iteration 71, loss = 0.05377481\n",
      "Validation score: 0.949061\n",
      "Iteration 72, loss = 0.05328179\n",
      "Validation score: 0.950605\n",
      "Iteration 73, loss = 0.05231066\n",
      "Validation score: 0.948289\n",
      "Iteration 74, loss = 0.05169543\n",
      "Validation score: 0.947260\n",
      "Iteration 75, loss = 0.05101005\n",
      "Validation score: 0.950347\n",
      "Iteration 76, loss = 0.05021446\n",
      "Validation score: 0.949576\n",
      "Iteration 77, loss = 0.04956029\n",
      "Validation score: 0.949061\n",
      "Iteration 78, loss = 0.04890331\n",
      "Validation score: 0.949833\n",
      "Iteration 79, loss = 0.04832415\n",
      "Validation score: 0.949576\n",
      "Iteration 80, loss = 0.04770672\n",
      "Validation score: 0.949833\n",
      "Iteration 81, loss = 0.04707609\n",
      "Validation score: 0.948804\n",
      "Iteration 82, loss = 0.04635561\n",
      "Validation score: 0.948546\n",
      "Iteration 83, loss = 0.04562468\n",
      "Validation score: 0.947260\n",
      "Validation score did not improve more than tol=0.000100 for 25 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.43052155\n",
      "Validation score: 0.882171\n",
      "Iteration 2, loss = 0.25537081\n",
      "Validation score: 0.909442\n",
      "Iteration 3, loss = 0.20977102\n",
      "Validation score: 0.922048\n",
      "Iteration 4, loss = 0.18579182\n",
      "Validation score: 0.925392\n",
      "Iteration 5, loss = 0.17035797\n",
      "Validation score: 0.930280\n",
      "Iteration 6, loss = 0.15971125\n",
      "Validation score: 0.933625\n",
      "Iteration 7, loss = 0.15200270\n",
      "Validation score: 0.934654\n",
      "Iteration 8, loss = 0.14564979\n",
      "Validation score: 0.936712\n",
      "Iteration 9, loss = 0.14045267\n",
      "Validation score: 0.938256\n",
      "Iteration 10, loss = 0.13624181\n",
      "Validation score: 0.937998\n",
      "Iteration 11, loss = 0.13230389\n",
      "Validation score: 0.939028\n",
      "Iteration 12, loss = 0.12885282\n",
      "Validation score: 0.939028\n",
      "Iteration 13, loss = 0.12596523\n",
      "Validation score: 0.941343\n",
      "Iteration 14, loss = 0.12315934\n",
      "Validation score: 0.940828\n",
      "Iteration 15, loss = 0.12062006\n",
      "Validation score: 0.941600\n",
      "Iteration 16, loss = 0.11814787\n",
      "Validation score: 0.942887\n",
      "Iteration 17, loss = 0.11599988\n",
      "Validation score: 0.942115\n",
      "Iteration 18, loss = 0.11392867\n",
      "Validation score: 0.942887\n",
      "Iteration 19, loss = 0.11182304\n",
      "Validation score: 0.944945\n",
      "Iteration 20, loss = 0.10992088\n",
      "Validation score: 0.942115\n",
      "Iteration 21, loss = 0.10822822\n",
      "Validation score: 0.942887\n",
      "Iteration 22, loss = 0.10645048\n",
      "Validation score: 0.944687\n",
      "Iteration 23, loss = 0.10477733\n",
      "Validation score: 0.944430\n",
      "Iteration 24, loss = 0.10296929\n",
      "Validation score: 0.946488\n",
      "Iteration 25, loss = 0.10142476\n",
      "Validation score: 0.945202\n",
      "Iteration 26, loss = 0.09987626\n",
      "Validation score: 0.945716\n",
      "Iteration 27, loss = 0.09855993\n",
      "Validation score: 0.946746\n",
      "Iteration 28, loss = 0.09715897\n",
      "Validation score: 0.947260\n",
      "Iteration 29, loss = 0.09563249\n",
      "Validation score: 0.945716\n",
      "Iteration 30, loss = 0.09397933\n",
      "Validation score: 0.947003\n",
      "Iteration 31, loss = 0.09284001\n",
      "Validation score: 0.946488\n",
      "Iteration 32, loss = 0.09155139\n",
      "Validation score: 0.947003\n",
      "Iteration 33, loss = 0.09027139\n",
      "Validation score: 0.947775\n",
      "Iteration 34, loss = 0.08902921\n",
      "Validation score: 0.946488\n",
      "Iteration 35, loss = 0.08771867\n",
      "Validation score: 0.947003\n",
      "Iteration 36, loss = 0.08653361\n",
      "Validation score: 0.949061\n",
      "Iteration 37, loss = 0.08533337\n",
      "Validation score: 0.948032\n",
      "Iteration 38, loss = 0.08402199\n",
      "Validation score: 0.947260\n",
      "Iteration 39, loss = 0.08281450\n",
      "Validation score: 0.949061\n",
      "Iteration 40, loss = 0.08175432\n",
      "Validation score: 0.947775\n",
      "Iteration 41, loss = 0.08048555\n",
      "Validation score: 0.947003\n",
      "Iteration 42, loss = 0.07961703\n",
      "Validation score: 0.948804\n",
      "Iteration 43, loss = 0.07842861\n",
      "Validation score: 0.948289\n",
      "Iteration 44, loss = 0.07730795\n",
      "Validation score: 0.950347\n",
      "Iteration 45, loss = 0.07628537\n",
      "Validation score: 0.948804\n",
      "Iteration 46, loss = 0.07520680\n",
      "Validation score: 0.948546\n",
      "Iteration 47, loss = 0.07423434\n",
      "Validation score: 0.950090\n",
      "Iteration 48, loss = 0.07312644\n",
      "Validation score: 0.949833\n",
      "Iteration 49, loss = 0.07212632\n",
      "Validation score: 0.950347\n",
      "Iteration 50, loss = 0.07103572\n",
      "Validation score: 0.950090\n",
      "Iteration 51, loss = 0.07028055\n",
      "Validation score: 0.950347\n",
      "Iteration 52, loss = 0.06902777\n",
      "Validation score: 0.948546\n",
      "Iteration 53, loss = 0.06865139\n",
      "Validation score: 0.949833\n",
      "Iteration 54, loss = 0.06726050\n",
      "Validation score: 0.948546\n",
      "Iteration 55, loss = 0.06643805\n",
      "Validation score: 0.949576\n",
      "Iteration 56, loss = 0.06551516\n",
      "Validation score: 0.949833\n",
      "Iteration 57, loss = 0.06465356\n",
      "Validation score: 0.949833\n",
      "Iteration 58, loss = 0.06378803\n",
      "Validation score: 0.949061\n",
      "Iteration 59, loss = 0.06275417\n",
      "Validation score: 0.949833\n",
      "Iteration 60, loss = 0.06203345\n",
      "Validation score: 0.949318\n",
      "Iteration 61, loss = 0.06145770\n",
      "Validation score: 0.949318\n",
      "Iteration 62, loss = 0.06025381\n",
      "Validation score: 0.950090\n",
      "Iteration 63, loss = 0.05951624\n",
      "Validation score: 0.950347\n",
      "Iteration 64, loss = 0.05862786\n",
      "Validation score: 0.950347\n",
      "Iteration 65, loss = 0.05788033\n",
      "Validation score: 0.949576\n",
      "Iteration 66, loss = 0.05701670\n",
      "Validation score: 0.949061\n",
      "Iteration 67, loss = 0.05641654\n",
      "Validation score: 0.952405\n",
      "Iteration 68, loss = 0.05571042\n",
      "Validation score: 0.951376\n",
      "Iteration 69, loss = 0.05495318\n",
      "Validation score: 0.948804\n",
      "Iteration 70, loss = 0.05413696\n",
      "Validation score: 0.949833\n",
      "Iteration 71, loss = 0.05319181\n",
      "Validation score: 0.951119\n",
      "Iteration 72, loss = 0.05272188\n",
      "Validation score: 0.951634\n",
      "Iteration 73, loss = 0.05179232\n",
      "Validation score: 0.949833\n",
      "Iteration 74, loss = 0.05134472\n",
      "Validation score: 0.949576\n",
      "Iteration 75, loss = 0.05043888\n",
      "Validation score: 0.950090\n",
      "Iteration 76, loss = 0.04967936\n",
      "Validation score: 0.950862\n",
      "Iteration 77, loss = 0.04909198\n",
      "Validation score: 0.950605\n",
      "Iteration 78, loss = 0.04843677\n",
      "Validation score: 0.949318\n",
      "Iteration 79, loss = 0.04782254\n",
      "Validation score: 0.948804\n",
      "Iteration 80, loss = 0.04708274\n",
      "Validation score: 0.950605\n",
      "Iteration 81, loss = 0.04650835\n",
      "Validation score: 0.952148\n",
      "Iteration 82, loss = 0.04601753\n",
      "Validation score: 0.949318\n",
      "Iteration 83, loss = 0.04515186\n",
      "Validation score: 0.950090\n",
      "Iteration 84, loss = 0.04445035\n",
      "Validation score: 0.949318\n",
      "Iteration 85, loss = 0.04388073\n",
      "Validation score: 0.949318\n",
      "Iteration 86, loss = 0.04332273\n",
      "Validation score: 0.950347\n",
      "Iteration 87, loss = 0.04279980\n",
      "Validation score: 0.949061\n",
      "Iteration 88, loss = 0.04224453\n",
      "Validation score: 0.949833\n",
      "Iteration 89, loss = 0.04169843\n",
      "Validation score: 0.950347\n",
      "Iteration 90, loss = 0.04103893\n",
      "Validation score: 0.949318\n",
      "Iteration 91, loss = 0.04040081\n",
      "Validation score: 0.949061\n",
      "Iteration 92, loss = 0.03975339\n",
      "Validation score: 0.950605\n",
      "Iteration 93, loss = 0.03916270\n",
      "Validation score: 0.950605\n",
      "Validation score did not improve more than tol=0.000100 for 25 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.37442725\n",
      "Validation score: 0.900695\n",
      "Iteration 2, loss = 0.23522866\n",
      "Validation score: 0.918703\n",
      "Iteration 3, loss = 0.19735812\n",
      "Validation score: 0.926164\n",
      "Iteration 4, loss = 0.17712236\n",
      "Validation score: 0.930538\n",
      "Iteration 5, loss = 0.16378571\n",
      "Validation score: 0.932596\n",
      "Iteration 6, loss = 0.15435418\n",
      "Validation score: 0.935169\n",
      "Iteration 7, loss = 0.14702342\n",
      "Validation score: 0.938256\n",
      "Iteration 8, loss = 0.14114367\n",
      "Validation score: 0.941600\n",
      "Iteration 9, loss = 0.13632911\n",
      "Validation score: 0.941600\n",
      "Iteration 10, loss = 0.13198464\n",
      "Validation score: 0.942115\n",
      "Iteration 11, loss = 0.12819234\n",
      "Validation score: 0.942629\n",
      "Iteration 12, loss = 0.12512622\n",
      "Validation score: 0.943916\n",
      "Iteration 13, loss = 0.12201261\n",
      "Validation score: 0.943658\n",
      "Iteration 14, loss = 0.11945120\n",
      "Validation score: 0.945202\n",
      "Iteration 15, loss = 0.11667842\n",
      "Validation score: 0.944430\n",
      "Iteration 16, loss = 0.11441981\n",
      "Validation score: 0.944430\n",
      "Iteration 17, loss = 0.11209097\n",
      "Validation score: 0.945202\n",
      "Iteration 18, loss = 0.10988403\n",
      "Validation score: 0.946746\n",
      "Iteration 19, loss = 0.10789185\n",
      "Validation score: 0.944687\n",
      "Iteration 20, loss = 0.10616109\n",
      "Validation score: 0.946488\n",
      "Iteration 21, loss = 0.10407093\n",
      "Validation score: 0.945716\n",
      "Iteration 22, loss = 0.10247616\n",
      "Validation score: 0.945974\n",
      "Iteration 23, loss = 0.10062604\n",
      "Validation score: 0.947517\n",
      "Iteration 24, loss = 0.09893705\n",
      "Validation score: 0.947517\n",
      "Iteration 25, loss = 0.09738020\n",
      "Validation score: 0.947517\n",
      "Iteration 26, loss = 0.09584013\n",
      "Validation score: 0.947517\n",
      "Iteration 27, loss = 0.09429631\n",
      "Validation score: 0.949833\n",
      "Iteration 28, loss = 0.09285695\n",
      "Validation score: 0.948804\n",
      "Iteration 29, loss = 0.09133855\n",
      "Validation score: 0.948546\n",
      "Iteration 30, loss = 0.08982787\n",
      "Validation score: 0.948289\n",
      "Iteration 31, loss = 0.08859567\n",
      "Validation score: 0.947775\n",
      "Iteration 32, loss = 0.08712170\n",
      "Validation score: 0.949833\n",
      "Iteration 33, loss = 0.08595240\n",
      "Validation score: 0.949833\n",
      "Iteration 34, loss = 0.08438204\n",
      "Validation score: 0.948289\n",
      "Iteration 35, loss = 0.08339073\n",
      "Validation score: 0.949318\n",
      "Iteration 36, loss = 0.08198784\n",
      "Validation score: 0.949318\n",
      "Iteration 37, loss = 0.08072927\n",
      "Validation score: 0.948032\n",
      "Iteration 38, loss = 0.07961007\n",
      "Validation score: 0.950605\n",
      "Iteration 39, loss = 0.07836262\n",
      "Validation score: 0.948289\n",
      "Iteration 40, loss = 0.07722588\n",
      "Validation score: 0.948289\n",
      "Iteration 41, loss = 0.07608507\n",
      "Validation score: 0.949833\n",
      "Iteration 42, loss = 0.07498611\n",
      "Validation score: 0.949833\n",
      "Iteration 43, loss = 0.07380810\n",
      "Validation score: 0.949061\n",
      "Iteration 44, loss = 0.07272782\n",
      "Validation score: 0.950862\n",
      "Iteration 45, loss = 0.07186553\n",
      "Validation score: 0.950605\n",
      "Iteration 46, loss = 0.07062575\n",
      "Validation score: 0.950862\n",
      "Iteration 47, loss = 0.06979162\n",
      "Validation score: 0.950862\n",
      "Iteration 48, loss = 0.06862355\n",
      "Validation score: 0.951376\n",
      "Iteration 49, loss = 0.06769019\n",
      "Validation score: 0.950090\n",
      "Iteration 50, loss = 0.06683824\n",
      "Validation score: 0.949833\n",
      "Iteration 51, loss = 0.06578900\n",
      "Validation score: 0.950605\n",
      "Iteration 52, loss = 0.06471245\n",
      "Validation score: 0.949576\n",
      "Iteration 53, loss = 0.06395714\n",
      "Validation score: 0.949833\n",
      "Iteration 54, loss = 0.06299540\n",
      "Validation score: 0.950605\n",
      "Iteration 55, loss = 0.06202875\n",
      "Validation score: 0.950862\n",
      "Iteration 56, loss = 0.06121281\n",
      "Validation score: 0.950347\n",
      "Iteration 57, loss = 0.06031945\n",
      "Validation score: 0.951119\n",
      "Iteration 58, loss = 0.05930412\n",
      "Validation score: 0.952148\n",
      "Iteration 59, loss = 0.05854281\n",
      "Validation score: 0.950862\n",
      "Iteration 60, loss = 0.05777894\n",
      "Validation score: 0.950862\n",
      "Iteration 61, loss = 0.05698413\n",
      "Validation score: 0.950862\n",
      "Iteration 62, loss = 0.05628055\n",
      "Validation score: 0.953177\n",
      "Iteration 63, loss = 0.05523539\n",
      "Validation score: 0.952148\n",
      "Iteration 64, loss = 0.05450787\n",
      "Validation score: 0.952920\n",
      "Iteration 65, loss = 0.05387797\n",
      "Validation score: 0.951634\n",
      "Iteration 66, loss = 0.05294318\n",
      "Validation score: 0.952920\n",
      "Iteration 67, loss = 0.05227039\n",
      "Validation score: 0.951891\n",
      "Iteration 68, loss = 0.05127918\n",
      "Validation score: 0.952405\n",
      "Iteration 69, loss = 0.05061454\n",
      "Validation score: 0.951119\n",
      "Iteration 70, loss = 0.04991579\n",
      "Validation score: 0.952405\n",
      "Iteration 71, loss = 0.04927541\n",
      "Validation score: 0.951376\n",
      "Iteration 72, loss = 0.04855100\n",
      "Validation score: 0.950862\n",
      "Iteration 73, loss = 0.04809915\n",
      "Validation score: 0.952148\n",
      "Iteration 74, loss = 0.04707402\n",
      "Validation score: 0.951891\n",
      "Iteration 75, loss = 0.04626394\n",
      "Validation score: 0.949576\n",
      "Iteration 76, loss = 0.04581577\n",
      "Validation score: 0.952920\n",
      "Iteration 77, loss = 0.04519045\n",
      "Validation score: 0.953692\n",
      "Iteration 78, loss = 0.04451727\n",
      "Validation score: 0.951376\n",
      "Iteration 79, loss = 0.04374835\n",
      "Validation score: 0.952405\n",
      "Iteration 80, loss = 0.04293445\n",
      "Validation score: 0.951891\n",
      "Iteration 81, loss = 0.04253947\n",
      "Validation score: 0.951891\n",
      "Iteration 82, loss = 0.04200192\n",
      "Validation score: 0.952405\n",
      "Iteration 83, loss = 0.04126649\n",
      "Validation score: 0.951891\n",
      "Iteration 84, loss = 0.04066592\n",
      "Validation score: 0.951891\n",
      "Iteration 85, loss = 0.03990205\n",
      "Validation score: 0.952405\n",
      "Iteration 86, loss = 0.03952105\n",
      "Validation score: 0.952148\n",
      "Iteration 87, loss = 0.03893166\n",
      "Validation score: 0.952148\n",
      "Iteration 88, loss = 0.03823297\n",
      "Validation score: 0.951634\n",
      "Iteration 89, loss = 0.03773592\n",
      "Validation score: 0.953177\n",
      "Iteration 90, loss = 0.03707209\n",
      "Validation score: 0.951891\n",
      "Iteration 91, loss = 0.03644760\n",
      "Validation score: 0.953435\n",
      "Iteration 92, loss = 0.03609610\n",
      "Validation score: 0.953435\n",
      "Iteration 93, loss = 0.03556737\n",
      "Validation score: 0.954464\n",
      "Iteration 94, loss = 0.03490084\n",
      "Validation score: 0.952405\n",
      "Iteration 95, loss = 0.03447033\n",
      "Validation score: 0.953949\n",
      "Iteration 96, loss = 0.03392458\n",
      "Validation score: 0.953692\n",
      "Iteration 97, loss = 0.03319792\n",
      "Validation score: 0.953177\n",
      "Iteration 98, loss = 0.03293039\n",
      "Validation score: 0.953949\n",
      "Iteration 99, loss = 0.03226667\n",
      "Validation score: 0.950862\n",
      "Iteration 100, loss = 0.03205142\n",
      "Validation score: 0.953435\n",
      "Iteration 101, loss = 0.03132355\n",
      "Validation score: 0.952663\n",
      "Iteration 102, loss = 0.03098931\n",
      "Validation score: 0.953177\n",
      "Iteration 103, loss = 0.03044189\n",
      "Validation score: 0.954206\n",
      "Iteration 104, loss = 0.02998378\n",
      "Validation score: 0.953692\n",
      "Iteration 105, loss = 0.02961939\n",
      "Validation score: 0.952663\n",
      "Iteration 106, loss = 0.02894728\n",
      "Validation score: 0.952663\n",
      "Iteration 107, loss = 0.02850731\n",
      "Validation score: 0.953949\n",
      "Iteration 108, loss = 0.02809662\n",
      "Validation score: 0.955235\n",
      "Iteration 109, loss = 0.02772792\n",
      "Validation score: 0.952663\n",
      "Iteration 110, loss = 0.02727917\n",
      "Validation score: 0.954206\n",
      "Iteration 111, loss = 0.02689365\n",
      "Validation score: 0.954464\n",
      "Iteration 112, loss = 0.02636922\n",
      "Validation score: 0.953177\n",
      "Iteration 113, loss = 0.02606185\n",
      "Validation score: 0.952920\n",
      "Iteration 114, loss = 0.02552420\n",
      "Validation score: 0.953692\n",
      "Iteration 115, loss = 0.02519536\n",
      "Validation score: 0.953435\n",
      "Iteration 116, loss = 0.02484327\n",
      "Validation score: 0.954206\n",
      "Iteration 117, loss = 0.02439632\n",
      "Validation score: 0.953692\n",
      "Iteration 118, loss = 0.02396303\n",
      "Validation score: 0.951891\n",
      "Iteration 119, loss = 0.02374201\n",
      "Validation score: 0.953692\n",
      "Iteration 120, loss = 0.02328235\n",
      "Validation score: 0.954206\n",
      "Iteration 121, loss = 0.02288900\n",
      "Validation score: 0.953949\n",
      "Iteration 122, loss = 0.02242233\n",
      "Validation score: 0.954978\n",
      "Iteration 123, loss = 0.02215379\n",
      "Validation score: 0.953435\n",
      "Iteration 124, loss = 0.02165725\n",
      "Validation score: 0.952405\n",
      "Iteration 125, loss = 0.02134463\n",
      "Validation score: 0.953177\n",
      "Iteration 126, loss = 0.02106844\n",
      "Validation score: 0.952405\n",
      "Iteration 127, loss = 0.02067255\n",
      "Validation score: 0.954206\n",
      "Iteration 128, loss = 0.02050066\n",
      "Validation score: 0.953949\n",
      "Iteration 129, loss = 0.02004595\n",
      "Validation score: 0.953692\n",
      "Iteration 130, loss = 0.01982667\n",
      "Validation score: 0.953949\n",
      "Iteration 131, loss = 0.01941029\n",
      "Validation score: 0.953177\n",
      "Iteration 132, loss = 0.01912307\n",
      "Validation score: 0.953949\n",
      "Iteration 133, loss = 0.01880663\n",
      "Validation score: 0.953177\n",
      "Iteration 134, loss = 0.01843180\n",
      "Validation score: 0.952920\n",
      "Validation score did not improve more than tol=0.000100 for 25 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.39894449\n",
      "Validation score: 0.897093\n",
      "Iteration 2, loss = 0.23593178\n",
      "Validation score: 0.917674\n",
      "Iteration 3, loss = 0.19681660\n",
      "Validation score: 0.926421\n",
      "Iteration 4, loss = 0.17670500\n",
      "Validation score: 0.930023\n",
      "Iteration 5, loss = 0.16360535\n",
      "Validation score: 0.933625\n",
      "Iteration 6, loss = 0.15411947\n",
      "Validation score: 0.936198\n",
      "Iteration 7, loss = 0.14689037\n",
      "Validation score: 0.937998\n",
      "Iteration 8, loss = 0.14110430\n",
      "Validation score: 0.937741\n",
      "Iteration 9, loss = 0.13629704\n",
      "Validation score: 0.939285\n",
      "Iteration 10, loss = 0.13233682\n",
      "Validation score: 0.941857\n",
      "Iteration 11, loss = 0.12861192\n",
      "Validation score: 0.941086\n",
      "Iteration 12, loss = 0.12553349\n",
      "Validation score: 0.941600\n",
      "Iteration 13, loss = 0.12255577\n",
      "Validation score: 0.944173\n",
      "Iteration 14, loss = 0.11979281\n",
      "Validation score: 0.941857\n",
      "Iteration 15, loss = 0.11721320\n",
      "Validation score: 0.941857\n",
      "Iteration 16, loss = 0.11499728\n",
      "Validation score: 0.943401\n",
      "Iteration 17, loss = 0.11277288\n",
      "Validation score: 0.942629\n",
      "Iteration 18, loss = 0.11085989\n",
      "Validation score: 0.942887\n",
      "Iteration 19, loss = 0.10868566\n",
      "Validation score: 0.945716\n",
      "Iteration 20, loss = 0.10676728\n",
      "Validation score: 0.943658\n",
      "Iteration 21, loss = 0.10503877\n",
      "Validation score: 0.944173\n",
      "Iteration 22, loss = 0.10320901\n",
      "Validation score: 0.943658\n",
      "Iteration 23, loss = 0.10139321\n",
      "Validation score: 0.946231\n",
      "Iteration 24, loss = 0.09972091\n",
      "Validation score: 0.947260\n",
      "Iteration 25, loss = 0.09813861\n",
      "Validation score: 0.944687\n",
      "Iteration 26, loss = 0.09656393\n",
      "Validation score: 0.945716\n",
      "Iteration 27, loss = 0.09495203\n",
      "Validation score: 0.945974\n",
      "Iteration 28, loss = 0.09352338\n",
      "Validation score: 0.948289\n",
      "Iteration 29, loss = 0.09183166\n",
      "Validation score: 0.948032\n",
      "Iteration 30, loss = 0.09054079\n",
      "Validation score: 0.947775\n",
      "Iteration 31, loss = 0.08913988\n",
      "Validation score: 0.946746\n",
      "Iteration 32, loss = 0.08780998\n",
      "Validation score: 0.947003\n",
      "Iteration 33, loss = 0.08637544\n",
      "Validation score: 0.947517\n",
      "Iteration 34, loss = 0.08514730\n",
      "Validation score: 0.948032\n",
      "Iteration 35, loss = 0.08391040\n",
      "Validation score: 0.948546\n",
      "Iteration 36, loss = 0.08241082\n",
      "Validation score: 0.950347\n",
      "Iteration 37, loss = 0.08129187\n",
      "Validation score: 0.948289\n",
      "Iteration 38, loss = 0.08025717\n",
      "Validation score: 0.949318\n",
      "Iteration 39, loss = 0.07887983\n",
      "Validation score: 0.948289\n",
      "Iteration 40, loss = 0.07763988\n",
      "Validation score: 0.950090\n",
      "Iteration 41, loss = 0.07661535\n",
      "Validation score: 0.949318\n",
      "Iteration 42, loss = 0.07545980\n",
      "Validation score: 0.950347\n",
      "Iteration 43, loss = 0.07443162\n",
      "Validation score: 0.950347\n",
      "Iteration 44, loss = 0.07323089\n",
      "Validation score: 0.948546\n",
      "Iteration 45, loss = 0.07200455\n",
      "Validation score: 0.949061\n",
      "Iteration 46, loss = 0.07120400\n",
      "Validation score: 0.950862\n",
      "Iteration 47, loss = 0.07015515\n",
      "Validation score: 0.949576\n",
      "Iteration 48, loss = 0.06887297\n",
      "Validation score: 0.949576\n",
      "Iteration 49, loss = 0.06796842\n",
      "Validation score: 0.948289\n",
      "Iteration 50, loss = 0.06704655\n",
      "Validation score: 0.951634\n",
      "Iteration 51, loss = 0.06587081\n",
      "Validation score: 0.948804\n",
      "Iteration 52, loss = 0.06508028\n",
      "Validation score: 0.951376\n",
      "Iteration 53, loss = 0.06411209\n",
      "Validation score: 0.948032\n",
      "Iteration 54, loss = 0.06314024\n",
      "Validation score: 0.951634\n",
      "Iteration 55, loss = 0.06208267\n",
      "Validation score: 0.949833\n",
      "Iteration 56, loss = 0.06138763\n",
      "Validation score: 0.952148\n",
      "Iteration 57, loss = 0.06026029\n",
      "Validation score: 0.950605\n",
      "Iteration 58, loss = 0.05942997\n",
      "Validation score: 0.950347\n",
      "Iteration 59, loss = 0.05851828\n",
      "Validation score: 0.949318\n",
      "Iteration 60, loss = 0.05761647\n",
      "Validation score: 0.949061\n",
      "Iteration 61, loss = 0.05688826\n",
      "Validation score: 0.949061\n",
      "Iteration 62, loss = 0.05606673\n",
      "Validation score: 0.949576\n",
      "Iteration 63, loss = 0.05515799\n",
      "Validation score: 0.949833\n",
      "Iteration 64, loss = 0.05447601\n",
      "Validation score: 0.951634\n",
      "Iteration 65, loss = 0.05357048\n",
      "Validation score: 0.951376\n",
      "Iteration 66, loss = 0.05284406\n",
      "Validation score: 0.948546\n",
      "Iteration 67, loss = 0.05194598\n",
      "Validation score: 0.949061\n",
      "Iteration 68, loss = 0.05131297\n",
      "Validation score: 0.949318\n",
      "Iteration 69, loss = 0.05049221\n",
      "Validation score: 0.950090\n",
      "Iteration 70, loss = 0.04968149\n",
      "Validation score: 0.950347\n",
      "Iteration 71, loss = 0.04898155\n",
      "Validation score: 0.951634\n",
      "Iteration 72, loss = 0.04828637\n",
      "Validation score: 0.949061\n",
      "Iteration 73, loss = 0.04751873\n",
      "Validation score: 0.951119\n",
      "Iteration 74, loss = 0.04687774\n",
      "Validation score: 0.950347\n",
      "Iteration 75, loss = 0.04613545\n",
      "Validation score: 0.950347\n",
      "Iteration 76, loss = 0.04552950\n",
      "Validation score: 0.950347\n",
      "Iteration 77, loss = 0.04470413\n",
      "Validation score: 0.950347\n",
      "Iteration 78, loss = 0.04393615\n",
      "Validation score: 0.950347\n",
      "Iteration 79, loss = 0.04348207\n",
      "Validation score: 0.951634\n",
      "Iteration 80, loss = 0.04271384\n",
      "Validation score: 0.952663\n",
      "Iteration 81, loss = 0.04209395\n",
      "Validation score: 0.949318\n",
      "Iteration 82, loss = 0.04139793\n",
      "Validation score: 0.951634\n",
      "Iteration 83, loss = 0.04091257\n",
      "Validation score: 0.949833\n",
      "Iteration 84, loss = 0.04018781\n",
      "Validation score: 0.949318\n",
      "Iteration 85, loss = 0.03970080\n",
      "Validation score: 0.950605\n",
      "Iteration 86, loss = 0.03893602\n",
      "Validation score: 0.950862\n",
      "Iteration 87, loss = 0.03846890\n",
      "Validation score: 0.950090\n",
      "Iteration 88, loss = 0.03792800\n",
      "Validation score: 0.949833\n",
      "Iteration 89, loss = 0.03732294\n",
      "Validation score: 0.948804\n",
      "Iteration 90, loss = 0.03675393\n",
      "Validation score: 0.950090\n",
      "Iteration 91, loss = 0.03596478\n",
      "Validation score: 0.949833\n",
      "Iteration 92, loss = 0.03578438\n",
      "Validation score: 0.951634\n",
      "Iteration 93, loss = 0.03512861\n",
      "Validation score: 0.950090\n",
      "Iteration 94, loss = 0.03459951\n",
      "Validation score: 0.950347\n",
      "Iteration 95, loss = 0.03404190\n",
      "Validation score: 0.950862\n",
      "Iteration 96, loss = 0.03351270\n",
      "Validation score: 0.949318\n",
      "Iteration 97, loss = 0.03298527\n",
      "Validation score: 0.950605\n",
      "Iteration 98, loss = 0.03247134\n",
      "Validation score: 0.951634\n",
      "Iteration 99, loss = 0.03220601\n",
      "Validation score: 0.950862\n",
      "Iteration 100, loss = 0.03156571\n",
      "Validation score: 0.949833\n",
      "Iteration 101, loss = 0.03103021\n",
      "Validation score: 0.950605\n",
      "Iteration 102, loss = 0.03039111\n",
      "Validation score: 0.950862\n",
      "Iteration 103, loss = 0.03012682\n",
      "Validation score: 0.950347\n",
      "Iteration 104, loss = 0.02959241\n",
      "Validation score: 0.951376\n",
      "Iteration 105, loss = 0.02905088\n",
      "Validation score: 0.949833\n",
      "Iteration 106, loss = 0.02863217\n",
      "Validation score: 0.949833\n",
      "Validation score did not improve more than tol=0.000100 for 25 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.38904672\n",
      "Validation score: 0.902753\n",
      "Iteration 2, loss = 0.24098962\n",
      "Validation score: 0.920247\n",
      "Iteration 3, loss = 0.20183681\n",
      "Validation score: 0.928994\n",
      "Iteration 4, loss = 0.18124456\n",
      "Validation score: 0.933110\n",
      "Iteration 5, loss = 0.16772734\n",
      "Validation score: 0.936969\n",
      "Iteration 6, loss = 0.15815083\n",
      "Validation score: 0.940571\n",
      "Iteration 7, loss = 0.15082184\n",
      "Validation score: 0.941343\n",
      "Iteration 8, loss = 0.14502508\n",
      "Validation score: 0.942115\n",
      "Iteration 9, loss = 0.14013164\n",
      "Validation score: 0.942115\n",
      "Iteration 10, loss = 0.13596393\n",
      "Validation score: 0.945202\n",
      "Iteration 11, loss = 0.13226279\n",
      "Validation score: 0.945974\n",
      "Iteration 12, loss = 0.12922380\n",
      "Validation score: 0.946231\n",
      "Iteration 13, loss = 0.12602637\n",
      "Validation score: 0.946231\n",
      "Iteration 14, loss = 0.12339490\n",
      "Validation score: 0.947517\n",
      "Iteration 15, loss = 0.12078297\n",
      "Validation score: 0.948032\n",
      "Iteration 16, loss = 0.11833225\n",
      "Validation score: 0.947260\n",
      "Iteration 17, loss = 0.11624349\n",
      "Validation score: 0.949833\n",
      "Iteration 18, loss = 0.11422245\n",
      "Validation score: 0.948804\n",
      "Iteration 19, loss = 0.11208242\n",
      "Validation score: 0.950605\n",
      "Iteration 20, loss = 0.11021012\n",
      "Validation score: 0.950090\n",
      "Iteration 21, loss = 0.10828878\n",
      "Validation score: 0.950090\n",
      "Iteration 22, loss = 0.10639727\n",
      "Validation score: 0.949833\n",
      "Iteration 23, loss = 0.10462538\n",
      "Validation score: 0.949833\n",
      "Iteration 24, loss = 0.10308302\n",
      "Validation score: 0.951119\n",
      "Iteration 25, loss = 0.10124765\n",
      "Validation score: 0.951634\n",
      "Iteration 26, loss = 0.09977834\n",
      "Validation score: 0.951376\n",
      "Iteration 27, loss = 0.09791589\n",
      "Validation score: 0.952148\n",
      "Iteration 28, loss = 0.09670075\n",
      "Validation score: 0.953177\n",
      "Iteration 29, loss = 0.09516705\n",
      "Validation score: 0.951119\n",
      "Iteration 30, loss = 0.09367732\n",
      "Validation score: 0.950862\n",
      "Iteration 31, loss = 0.09232799\n",
      "Validation score: 0.952663\n",
      "Iteration 32, loss = 0.09092878\n",
      "Validation score: 0.951376\n",
      "Iteration 33, loss = 0.08946006\n",
      "Validation score: 0.952663\n",
      "Iteration 34, loss = 0.08812032\n",
      "Validation score: 0.953435\n",
      "Iteration 35, loss = 0.08685590\n",
      "Validation score: 0.954206\n",
      "Iteration 36, loss = 0.08549305\n",
      "Validation score: 0.952663\n",
      "Iteration 37, loss = 0.08443980\n",
      "Validation score: 0.953692\n",
      "Iteration 38, loss = 0.08315880\n",
      "Validation score: 0.954206\n",
      "Iteration 39, loss = 0.08181304\n",
      "Validation score: 0.954206\n",
      "Iteration 40, loss = 0.08073152\n",
      "Validation score: 0.954464\n",
      "Iteration 41, loss = 0.07949411\n",
      "Validation score: 0.954206\n",
      "Iteration 42, loss = 0.07841106\n",
      "Validation score: 0.953177\n",
      "Iteration 43, loss = 0.07727744\n",
      "Validation score: 0.954721\n",
      "Iteration 44, loss = 0.07600440\n",
      "Validation score: 0.953177\n",
      "Iteration 45, loss = 0.07500607\n",
      "Validation score: 0.953435\n",
      "Iteration 46, loss = 0.07391313\n",
      "Validation score: 0.954206\n",
      "Iteration 47, loss = 0.07290450\n",
      "Validation score: 0.953177\n",
      "Iteration 48, loss = 0.07201888\n",
      "Validation score: 0.954721\n",
      "Iteration 49, loss = 0.07107475\n",
      "Validation score: 0.954978\n",
      "Iteration 50, loss = 0.06978050\n",
      "Validation score: 0.954721\n",
      "Iteration 51, loss = 0.06895062\n",
      "Validation score: 0.955750\n",
      "Iteration 52, loss = 0.06785698\n",
      "Validation score: 0.954206\n",
      "Iteration 53, loss = 0.06699828\n",
      "Validation score: 0.956007\n",
      "Iteration 54, loss = 0.06590779\n",
      "Validation score: 0.955750\n",
      "Iteration 55, loss = 0.06504205\n",
      "Validation score: 0.955493\n",
      "Iteration 56, loss = 0.06416483\n",
      "Validation score: 0.954206\n",
      "Iteration 57, loss = 0.06338614\n",
      "Validation score: 0.953949\n",
      "Iteration 58, loss = 0.06236869\n",
      "Validation score: 0.953949\n",
      "Iteration 59, loss = 0.06153009\n",
      "Validation score: 0.955235\n",
      "Iteration 60, loss = 0.06037880\n",
      "Validation score: 0.954464\n",
      "Iteration 61, loss = 0.05981106\n",
      "Validation score: 0.956522\n",
      "Iteration 62, loss = 0.05874919\n",
      "Validation score: 0.955750\n",
      "Iteration 63, loss = 0.05800991\n",
      "Validation score: 0.954721\n",
      "Iteration 64, loss = 0.05721696\n",
      "Validation score: 0.955235\n",
      "Iteration 65, loss = 0.05646098\n",
      "Validation score: 0.955750\n",
      "Iteration 66, loss = 0.05553350\n",
      "Validation score: 0.953435\n",
      "Iteration 67, loss = 0.05485298\n",
      "Validation score: 0.954464\n",
      "Iteration 68, loss = 0.05406128\n",
      "Validation score: 0.955493\n",
      "Iteration 69, loss = 0.05341885\n",
      "Validation score: 0.956522\n",
      "Iteration 70, loss = 0.05251239\n",
      "Validation score: 0.955750\n",
      "Iteration 71, loss = 0.05194431\n",
      "Validation score: 0.955235\n",
      "Iteration 72, loss = 0.05124800\n",
      "Validation score: 0.954206\n",
      "Iteration 73, loss = 0.05033022\n",
      "Validation score: 0.954978\n",
      "Iteration 74, loss = 0.04972883\n",
      "Validation score: 0.954206\n",
      "Iteration 75, loss = 0.04883911\n",
      "Validation score: 0.953692\n",
      "Iteration 76, loss = 0.04814029\n",
      "Validation score: 0.955750\n",
      "Iteration 77, loss = 0.04748553\n",
      "Validation score: 0.954721\n",
      "Iteration 78, loss = 0.04678280\n",
      "Validation score: 0.954721\n",
      "Iteration 79, loss = 0.04606015\n",
      "Validation score: 0.954721\n",
      "Iteration 80, loss = 0.04547928\n",
      "Validation score: 0.954464\n",
      "Iteration 81, loss = 0.04483156\n",
      "Validation score: 0.954978\n",
      "Iteration 82, loss = 0.04422443\n",
      "Validation score: 0.955493\n",
      "Iteration 83, loss = 0.04353466\n",
      "Validation score: 0.954978\n",
      "Iteration 84, loss = 0.04300516\n",
      "Validation score: 0.952663\n",
      "Iteration 85, loss = 0.04242567\n",
      "Validation score: 0.954978\n",
      "Iteration 86, loss = 0.04157198\n",
      "Validation score: 0.954721\n",
      "Iteration 87, loss = 0.04111896\n",
      "Validation score: 0.953177\n",
      "Validation score did not improve more than tol=0.000100 for 25 consecutive epochs. Stopping.\n",
      "All 5 repeats done! \n",
      "\n",
      "Neural Network: undersampling at rate 0.2 from the most popular class:\n",
      "Iteration 1, loss = 0.31590898\n",
      "Validation score: 0.915002\n",
      "Iteration 2, loss = 0.18124504\n",
      "Validation score: 0.928007\n",
      "Iteration 3, loss = 0.15362482\n",
      "Validation score: 0.935749\n",
      "Iteration 4, loss = 0.13937905\n",
      "Validation score: 0.938381\n",
      "Iteration 5, loss = 0.13050759\n",
      "Validation score: 0.943645\n",
      "Iteration 6, loss = 0.12406549\n",
      "Validation score: 0.943490\n",
      "Iteration 7, loss = 0.11899584\n",
      "Validation score: 0.945038\n",
      "Iteration 8, loss = 0.11484136\n",
      "Validation score: 0.946277\n",
      "Iteration 9, loss = 0.11145120\n",
      "Validation score: 0.946586\n",
      "Iteration 10, loss = 0.10813601\n",
      "Validation score: 0.947360\n",
      "Iteration 11, loss = 0.10564512\n",
      "Validation score: 0.947825\n",
      "Iteration 12, loss = 0.10314520\n",
      "Validation score: 0.949528\n",
      "Iteration 13, loss = 0.10101830\n",
      "Validation score: 0.950302\n",
      "Iteration 14, loss = 0.09885526\n",
      "Validation score: 0.951540\n",
      "Iteration 15, loss = 0.09683120\n",
      "Validation score: 0.951231\n",
      "Iteration 16, loss = 0.09494594\n",
      "Validation score: 0.951695\n",
      "Iteration 17, loss = 0.09318601\n",
      "Validation score: 0.953708\n",
      "Iteration 18, loss = 0.09152290\n",
      "Validation score: 0.952005\n",
      "Iteration 19, loss = 0.09011475\n",
      "Validation score: 0.952160\n",
      "Iteration 20, loss = 0.08831351\n",
      "Validation score: 0.953708\n",
      "Iteration 21, loss = 0.08676016\n",
      "Validation score: 0.955101\n",
      "Iteration 22, loss = 0.08544415\n",
      "Validation score: 0.953553\n",
      "Iteration 23, loss = 0.08412261\n",
      "Validation score: 0.953089\n",
      "Iteration 24, loss = 0.08273152\n",
      "Validation score: 0.953244\n",
      "Iteration 25, loss = 0.08149873\n",
      "Validation score: 0.954482\n",
      "Iteration 26, loss = 0.08014209\n",
      "Validation score: 0.954482\n",
      "Iteration 27, loss = 0.07881280\n",
      "Validation score: 0.954327\n",
      "Iteration 28, loss = 0.07760966\n",
      "Validation score: 0.953863\n",
      "Iteration 29, loss = 0.07639836\n",
      "Validation score: 0.956030\n",
      "Iteration 30, loss = 0.07530900\n",
      "Validation score: 0.954482\n",
      "Iteration 31, loss = 0.07404394\n",
      "Validation score: 0.954947\n",
      "Iteration 32, loss = 0.07282242\n",
      "Validation score: 0.955101\n",
      "Iteration 33, loss = 0.07201066\n",
      "Validation score: 0.954482\n",
      "Iteration 34, loss = 0.07089793\n",
      "Validation score: 0.953244\n",
      "Iteration 35, loss = 0.06983678\n",
      "Validation score: 0.954018\n",
      "Iteration 36, loss = 0.06886916\n",
      "Validation score: 0.956030\n",
      "Iteration 37, loss = 0.06772130\n",
      "Validation score: 0.954018\n",
      "Iteration 38, loss = 0.06679413\n",
      "Validation score: 0.954947\n",
      "Iteration 39, loss = 0.06570629\n",
      "Validation score: 0.954172\n",
      "Iteration 40, loss = 0.06479393\n",
      "Validation score: 0.955566\n",
      "Iteration 41, loss = 0.06393282\n",
      "Validation score: 0.953863\n",
      "Iteration 42, loss = 0.06303008\n",
      "Validation score: 0.954482\n",
      "Iteration 43, loss = 0.06205725\n",
      "Validation score: 0.956030\n",
      "Iteration 44, loss = 0.06104143\n",
      "Validation score: 0.956804\n",
      "Iteration 45, loss = 0.06054580\n",
      "Validation score: 0.953244\n",
      "Iteration 46, loss = 0.05945568\n",
      "Validation score: 0.954947\n",
      "Iteration 47, loss = 0.05845136\n",
      "Validation score: 0.953089\n",
      "Iteration 48, loss = 0.05770730\n",
      "Validation score: 0.955721\n",
      "Iteration 49, loss = 0.05703629\n",
      "Validation score: 0.955411\n",
      "Iteration 50, loss = 0.05600574\n",
      "Validation score: 0.955411\n",
      "Iteration 51, loss = 0.05532585\n",
      "Validation score: 0.956185\n",
      "Iteration 52, loss = 0.05454852\n",
      "Validation score: 0.955721\n",
      "Iteration 53, loss = 0.05367860\n",
      "Validation score: 0.956030\n",
      "Iteration 54, loss = 0.05295005\n",
      "Validation score: 0.956804\n",
      "Iteration 55, loss = 0.05215093\n",
      "Validation score: 0.956495\n",
      "Iteration 56, loss = 0.05162064\n",
      "Validation score: 0.956650\n",
      "Iteration 57, loss = 0.05076052\n",
      "Validation score: 0.954327\n",
      "Iteration 58, loss = 0.04993468\n",
      "Validation score: 0.957269\n",
      "Iteration 59, loss = 0.04931172\n",
      "Validation score: 0.957114\n",
      "Iteration 60, loss = 0.04865068\n",
      "Validation score: 0.958198\n",
      "Iteration 61, loss = 0.04790389\n",
      "Validation score: 0.957269\n",
      "Iteration 62, loss = 0.04737233\n",
      "Validation score: 0.957269\n",
      "Iteration 63, loss = 0.04654296\n",
      "Validation score: 0.956650\n",
      "Iteration 64, loss = 0.04588520\n",
      "Validation score: 0.955566\n",
      "Iteration 65, loss = 0.04536019\n",
      "Validation score: 0.957269\n",
      "Iteration 66, loss = 0.04461220\n",
      "Validation score: 0.957888\n",
      "Iteration 67, loss = 0.04398550\n",
      "Validation score: 0.956030\n",
      "Iteration 68, loss = 0.04340945\n",
      "Validation score: 0.956340\n",
      "Iteration 69, loss = 0.04276171\n",
      "Validation score: 0.956804\n",
      "Iteration 70, loss = 0.04220066\n",
      "Validation score: 0.957114\n",
      "Iteration 71, loss = 0.04152017\n",
      "Validation score: 0.956185\n",
      "Iteration 72, loss = 0.04108339\n",
      "Validation score: 0.957579\n",
      "Iteration 73, loss = 0.04057971\n",
      "Validation score: 0.957733\n",
      "Iteration 74, loss = 0.03990246\n",
      "Validation score: 0.956959\n",
      "Iteration 75, loss = 0.03933466\n",
      "Validation score: 0.956185\n",
      "Iteration 76, loss = 0.03869807\n",
      "Validation score: 0.957424\n",
      "Iteration 77, loss = 0.03817609\n",
      "Validation score: 0.957269\n",
      "Iteration 78, loss = 0.03773938\n",
      "Validation score: 0.955721\n",
      "Iteration 79, loss = 0.03705586\n",
      "Validation score: 0.957424\n",
      "Iteration 80, loss = 0.03657507\n",
      "Validation score: 0.957269\n",
      "Iteration 81, loss = 0.03605640\n",
      "Validation score: 0.955411\n",
      "Iteration 82, loss = 0.03565439\n",
      "Validation score: 0.956030\n",
      "Iteration 83, loss = 0.03487431\n",
      "Validation score: 0.957733\n",
      "Iteration 84, loss = 0.03458830\n",
      "Validation score: 0.954637\n",
      "Iteration 85, loss = 0.03396272\n",
      "Validation score: 0.957888\n",
      "Iteration 86, loss = 0.03355159\n",
      "Validation score: 0.958043\n",
      "Validation score did not improve more than tol=0.000100 for 25 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.33638341\n",
      "Validation score: 0.914538\n",
      "Iteration 2, loss = 0.18374455\n",
      "Validation score: 0.930330\n",
      "Iteration 3, loss = 0.15451023\n",
      "Validation score: 0.935749\n",
      "Iteration 4, loss = 0.13988704\n",
      "Validation score: 0.938690\n",
      "Iteration 5, loss = 0.13047666\n",
      "Validation score: 0.943799\n",
      "Iteration 6, loss = 0.12391382\n",
      "Validation score: 0.945348\n",
      "Iteration 7, loss = 0.11884414\n",
      "Validation score: 0.945193\n",
      "Iteration 8, loss = 0.11451779\n",
      "Validation score: 0.945812\n",
      "Iteration 9, loss = 0.11118521\n",
      "Validation score: 0.946586\n",
      "Iteration 10, loss = 0.10804796\n",
      "Validation score: 0.948289\n",
      "Iteration 11, loss = 0.10532207\n",
      "Validation score: 0.947205\n",
      "Iteration 12, loss = 0.10287526\n",
      "Validation score: 0.948754\n",
      "Iteration 13, loss = 0.10065075\n",
      "Validation score: 0.949373\n",
      "Iteration 14, loss = 0.09858760\n",
      "Validation score: 0.948289\n",
      "Iteration 15, loss = 0.09660923\n",
      "Validation score: 0.949528\n",
      "Iteration 16, loss = 0.09466692\n",
      "Validation score: 0.948754\n",
      "Iteration 17, loss = 0.09302692\n",
      "Validation score: 0.949837\n",
      "Iteration 18, loss = 0.09139157\n",
      "Validation score: 0.948908\n",
      "Iteration 19, loss = 0.08979005\n",
      "Validation score: 0.950457\n",
      "Iteration 20, loss = 0.08796222\n",
      "Validation score: 0.949837\n",
      "Iteration 21, loss = 0.08638106\n",
      "Validation score: 0.949992\n",
      "Iteration 22, loss = 0.08493113\n",
      "Validation score: 0.949063\n",
      "Iteration 23, loss = 0.08355471\n",
      "Validation score: 0.949837\n",
      "Iteration 24, loss = 0.08232489\n",
      "Validation score: 0.951540\n",
      "Iteration 25, loss = 0.08086367\n",
      "Validation score: 0.950921\n",
      "Iteration 26, loss = 0.07971517\n",
      "Validation score: 0.952160\n",
      "Iteration 27, loss = 0.07826373\n",
      "Validation score: 0.950302\n",
      "Iteration 28, loss = 0.07707740\n",
      "Validation score: 0.950457\n",
      "Iteration 29, loss = 0.07595290\n",
      "Validation score: 0.950766\n",
      "Iteration 30, loss = 0.07458749\n",
      "Validation score: 0.950766\n",
      "Iteration 31, loss = 0.07344851\n",
      "Validation score: 0.950457\n",
      "Iteration 32, loss = 0.07232831\n",
      "Validation score: 0.950302\n",
      "Iteration 33, loss = 0.07111541\n",
      "Validation score: 0.951386\n",
      "Iteration 34, loss = 0.07019032\n",
      "Validation score: 0.951695\n",
      "Iteration 35, loss = 0.06909904\n",
      "Validation score: 0.951386\n",
      "Iteration 36, loss = 0.06791803\n",
      "Validation score: 0.951386\n",
      "Iteration 37, loss = 0.06697947\n",
      "Validation score: 0.951231\n",
      "Iteration 38, loss = 0.06604553\n",
      "Validation score: 0.951231\n",
      "Iteration 39, loss = 0.06489352\n",
      "Validation score: 0.951231\n",
      "Iteration 40, loss = 0.06410086\n",
      "Validation score: 0.951231\n",
      "Iteration 41, loss = 0.06292015\n",
      "Validation score: 0.951386\n",
      "Iteration 42, loss = 0.06206184\n",
      "Validation score: 0.951076\n",
      "Iteration 43, loss = 0.06124167\n",
      "Validation score: 0.951540\n",
      "Iteration 44, loss = 0.06024153\n",
      "Validation score: 0.952005\n",
      "Iteration 45, loss = 0.05935544\n",
      "Validation score: 0.952315\n",
      "Iteration 46, loss = 0.05843272\n",
      "Validation score: 0.951540\n",
      "Iteration 47, loss = 0.05754528\n",
      "Validation score: 0.950147\n",
      "Iteration 48, loss = 0.05672619\n",
      "Validation score: 0.951231\n",
      "Iteration 49, loss = 0.05597000\n",
      "Validation score: 0.951540\n",
      "Iteration 50, loss = 0.05512309\n",
      "Validation score: 0.951540\n",
      "Iteration 51, loss = 0.05447384\n",
      "Validation score: 0.952005\n",
      "Iteration 52, loss = 0.05348073\n",
      "Validation score: 0.951076\n",
      "Iteration 53, loss = 0.05265041\n",
      "Validation score: 0.952624\n",
      "Iteration 54, loss = 0.05200657\n",
      "Validation score: 0.950921\n",
      "Iteration 55, loss = 0.05119400\n",
      "Validation score: 0.951695\n",
      "Iteration 56, loss = 0.05053653\n",
      "Validation score: 0.952934\n",
      "Iteration 57, loss = 0.04981191\n",
      "Validation score: 0.953089\n",
      "Iteration 58, loss = 0.04902292\n",
      "Validation score: 0.952779\n",
      "Iteration 59, loss = 0.04824982\n",
      "Validation score: 0.953089\n",
      "Iteration 60, loss = 0.04770009\n",
      "Validation score: 0.953244\n",
      "Iteration 61, loss = 0.04705929\n",
      "Validation score: 0.953863\n",
      "Iteration 62, loss = 0.04627161\n",
      "Validation score: 0.950921\n",
      "Iteration 63, loss = 0.04566567\n",
      "Validation score: 0.952934\n",
      "Iteration 64, loss = 0.04484718\n",
      "Validation score: 0.952160\n",
      "Iteration 65, loss = 0.04432350\n",
      "Validation score: 0.953398\n",
      "Iteration 66, loss = 0.04383520\n",
      "Validation score: 0.951695\n",
      "Iteration 67, loss = 0.04303700\n",
      "Validation score: 0.953398\n",
      "Iteration 68, loss = 0.04247999\n",
      "Validation score: 0.952934\n",
      "Iteration 69, loss = 0.04178344\n",
      "Validation score: 0.953398\n",
      "Iteration 70, loss = 0.04113344\n",
      "Validation score: 0.952469\n",
      "Iteration 71, loss = 0.04060385\n",
      "Validation score: 0.952469\n",
      "Iteration 72, loss = 0.04005421\n",
      "Validation score: 0.952934\n",
      "Iteration 73, loss = 0.03939647\n",
      "Validation score: 0.954018\n",
      "Iteration 74, loss = 0.03889527\n",
      "Validation score: 0.953398\n",
      "Iteration 75, loss = 0.03822991\n",
      "Validation score: 0.953089\n",
      "Iteration 76, loss = 0.03776683\n",
      "Validation score: 0.952779\n",
      "Iteration 77, loss = 0.03713380\n",
      "Validation score: 0.951850\n",
      "Iteration 78, loss = 0.03665111\n",
      "Validation score: 0.952779\n",
      "Iteration 79, loss = 0.03608273\n",
      "Validation score: 0.952934\n",
      "Iteration 80, loss = 0.03549686\n",
      "Validation score: 0.953398\n",
      "Iteration 81, loss = 0.03504242\n",
      "Validation score: 0.952160\n",
      "Iteration 82, loss = 0.03452835\n",
      "Validation score: 0.952160\n",
      "Iteration 83, loss = 0.03411714\n",
      "Validation score: 0.952469\n",
      "Iteration 84, loss = 0.03356609\n",
      "Validation score: 0.953553\n",
      "Iteration 85, loss = 0.03301714\n",
      "Validation score: 0.952315\n",
      "Iteration 86, loss = 0.03250035\n",
      "Validation score: 0.951850\n",
      "Iteration 87, loss = 0.03208054\n",
      "Validation score: 0.952624\n",
      "Iteration 88, loss = 0.03148587\n",
      "Validation score: 0.953553\n",
      "Iteration 89, loss = 0.03111713\n",
      "Validation score: 0.951695\n",
      "Iteration 90, loss = 0.03055786\n",
      "Validation score: 0.951695\n",
      "Iteration 91, loss = 0.03030755\n",
      "Validation score: 0.953244\n",
      "Iteration 92, loss = 0.02961993\n",
      "Validation score: 0.952624\n",
      "Iteration 93, loss = 0.02919247\n",
      "Validation score: 0.951695\n",
      "Iteration 94, loss = 0.02870675\n",
      "Validation score: 0.953553\n",
      "Iteration 95, loss = 0.02827355\n",
      "Validation score: 0.952779\n",
      "Iteration 96, loss = 0.02786414\n",
      "Validation score: 0.952779\n",
      "Iteration 97, loss = 0.02753967\n",
      "Validation score: 0.953708\n",
      "Iteration 98, loss = 0.02704236\n",
      "Validation score: 0.953398\n",
      "Iteration 99, loss = 0.02663600\n",
      "Validation score: 0.952934\n",
      "Validation score did not improve more than tol=0.000100 for 25 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.45819003\n",
      "Validation score: 0.912525\n",
      "Iteration 2, loss = 0.19875289\n",
      "Validation score: 0.930330\n",
      "Iteration 3, loss = 0.16281870\n",
      "Validation score: 0.935129\n",
      "Iteration 4, loss = 0.14584786\n",
      "Validation score: 0.939774\n",
      "Iteration 5, loss = 0.13521923\n",
      "Validation score: 0.943335\n",
      "Iteration 6, loss = 0.12798470\n",
      "Validation score: 0.945348\n",
      "Iteration 7, loss = 0.12229184\n",
      "Validation score: 0.946741\n",
      "Iteration 8, loss = 0.11795113\n",
      "Validation score: 0.947980\n",
      "Iteration 9, loss = 0.11419286\n",
      "Validation score: 0.948444\n",
      "Iteration 10, loss = 0.11090176\n",
      "Validation score: 0.949373\n",
      "Iteration 11, loss = 0.10810520\n",
      "Validation score: 0.949992\n",
      "Iteration 12, loss = 0.10526895\n",
      "Validation score: 0.949528\n",
      "Iteration 13, loss = 0.10309783\n",
      "Validation score: 0.950766\n",
      "Iteration 14, loss = 0.10060601\n",
      "Validation score: 0.950766\n",
      "Iteration 15, loss = 0.09872531\n",
      "Validation score: 0.952005\n",
      "Iteration 16, loss = 0.09660701\n",
      "Validation score: 0.954018\n",
      "Iteration 17, loss = 0.09479359\n",
      "Validation score: 0.952469\n",
      "Iteration 18, loss = 0.09285495\n",
      "Validation score: 0.952005\n",
      "Iteration 19, loss = 0.09137316\n",
      "Validation score: 0.952779\n",
      "Iteration 20, loss = 0.08955453\n",
      "Validation score: 0.952469\n",
      "Iteration 21, loss = 0.08780910\n",
      "Validation score: 0.952934\n",
      "Iteration 22, loss = 0.08656109\n",
      "Validation score: 0.953553\n",
      "Iteration 23, loss = 0.08499445\n",
      "Validation score: 0.953089\n",
      "Iteration 24, loss = 0.08355387\n",
      "Validation score: 0.954018\n",
      "Iteration 25, loss = 0.08194648\n",
      "Validation score: 0.953863\n",
      "Iteration 26, loss = 0.08056401\n",
      "Validation score: 0.954637\n",
      "Iteration 27, loss = 0.07926186\n",
      "Validation score: 0.954947\n",
      "Iteration 28, loss = 0.07797286\n",
      "Validation score: 0.954947\n",
      "Iteration 29, loss = 0.07688550\n",
      "Validation score: 0.955876\n",
      "Iteration 30, loss = 0.07548833\n",
      "Validation score: 0.955101\n",
      "Iteration 31, loss = 0.07428188\n",
      "Validation score: 0.955101\n",
      "Iteration 32, loss = 0.07297629\n",
      "Validation score: 0.955411\n",
      "Iteration 33, loss = 0.07203809\n",
      "Validation score: 0.956650\n",
      "Iteration 34, loss = 0.07068107\n",
      "Validation score: 0.955876\n",
      "Iteration 35, loss = 0.06976289\n",
      "Validation score: 0.956185\n",
      "Iteration 36, loss = 0.06859723\n",
      "Validation score: 0.956185\n",
      "Iteration 37, loss = 0.06752456\n",
      "Validation score: 0.955566\n",
      "Iteration 38, loss = 0.06647977\n",
      "Validation score: 0.956185\n",
      "Iteration 39, loss = 0.06525220\n",
      "Validation score: 0.955876\n",
      "Iteration 40, loss = 0.06452012\n",
      "Validation score: 0.957114\n",
      "Iteration 41, loss = 0.06349443\n",
      "Validation score: 0.956340\n",
      "Iteration 42, loss = 0.06237575\n",
      "Validation score: 0.957579\n",
      "Iteration 43, loss = 0.06158601\n",
      "Validation score: 0.957114\n",
      "Iteration 44, loss = 0.06063363\n",
      "Validation score: 0.956185\n",
      "Iteration 45, loss = 0.05968885\n",
      "Validation score: 0.956495\n",
      "Iteration 46, loss = 0.05872045\n",
      "Validation score: 0.956650\n",
      "Iteration 47, loss = 0.05811869\n",
      "Validation score: 0.957424\n",
      "Iteration 48, loss = 0.05695703\n",
      "Validation score: 0.955876\n",
      "Iteration 49, loss = 0.05629490\n",
      "Validation score: 0.956495\n",
      "Iteration 50, loss = 0.05531672\n",
      "Validation score: 0.955256\n",
      "Iteration 51, loss = 0.05449026\n",
      "Validation score: 0.957579\n",
      "Iteration 52, loss = 0.05370126\n",
      "Validation score: 0.958198\n",
      "Iteration 53, loss = 0.05318553\n",
      "Validation score: 0.957424\n",
      "Iteration 54, loss = 0.05214216\n",
      "Validation score: 0.958198\n",
      "Iteration 55, loss = 0.05137948\n",
      "Validation score: 0.957888\n",
      "Iteration 56, loss = 0.05055996\n",
      "Validation score: 0.958043\n",
      "Iteration 57, loss = 0.04976421\n",
      "Validation score: 0.957579\n",
      "Iteration 58, loss = 0.04925317\n",
      "Validation score: 0.957424\n",
      "Iteration 59, loss = 0.04852807\n",
      "Validation score: 0.956340\n",
      "Iteration 60, loss = 0.04780516\n",
      "Validation score: 0.957424\n",
      "Iteration 61, loss = 0.04703965\n",
      "Validation score: 0.956959\n",
      "Iteration 62, loss = 0.04644449\n",
      "Validation score: 0.958043\n",
      "Iteration 63, loss = 0.04558463\n",
      "Validation score: 0.957114\n",
      "Iteration 64, loss = 0.04505581\n",
      "Validation score: 0.958043\n",
      "Iteration 65, loss = 0.04429165\n",
      "Validation score: 0.957579\n",
      "Iteration 66, loss = 0.04381979\n",
      "Validation score: 0.957733\n",
      "Iteration 67, loss = 0.04303759\n",
      "Validation score: 0.957733\n",
      "Iteration 68, loss = 0.04256479\n",
      "Validation score: 0.957579\n",
      "Iteration 69, loss = 0.04197403\n",
      "Validation score: 0.957114\n",
      "Iteration 70, loss = 0.04112346\n",
      "Validation score: 0.957733\n",
      "Iteration 71, loss = 0.04062878\n",
      "Validation score: 0.957114\n",
      "Iteration 72, loss = 0.04007285\n",
      "Validation score: 0.956340\n",
      "Iteration 73, loss = 0.03953678\n",
      "Validation score: 0.956495\n",
      "Iteration 74, loss = 0.03894824\n",
      "Validation score: 0.957114\n",
      "Iteration 75, loss = 0.03828887\n",
      "Validation score: 0.956959\n",
      "Iteration 76, loss = 0.03786375\n",
      "Validation score: 0.958353\n",
      "Iteration 77, loss = 0.03726595\n",
      "Validation score: 0.957114\n",
      "Iteration 78, loss = 0.03668975\n",
      "Validation score: 0.956804\n",
      "Iteration 79, loss = 0.03609346\n",
      "Validation score: 0.957733\n",
      "Iteration 80, loss = 0.03572359\n",
      "Validation score: 0.957888\n",
      "Iteration 81, loss = 0.03503197\n",
      "Validation score: 0.956804\n",
      "Iteration 82, loss = 0.03469285\n",
      "Validation score: 0.957269\n",
      "Iteration 83, loss = 0.03397476\n",
      "Validation score: 0.956340\n",
      "Iteration 84, loss = 0.03355143\n",
      "Validation score: 0.956959\n",
      "Iteration 85, loss = 0.03312816\n",
      "Validation score: 0.957114\n",
      "Iteration 86, loss = 0.03252737\n",
      "Validation score: 0.956495\n",
      "Iteration 87, loss = 0.03200749\n",
      "Validation score: 0.957733\n",
      "Iteration 88, loss = 0.03163892\n",
      "Validation score: 0.957269\n",
      "Iteration 89, loss = 0.03110529\n",
      "Validation score: 0.956185\n",
      "Iteration 90, loss = 0.03061928\n",
      "Validation score: 0.956340\n",
      "Iteration 91, loss = 0.03016534\n",
      "Validation score: 0.957114\n",
      "Iteration 92, loss = 0.02980060\n",
      "Validation score: 0.955256\n",
      "Iteration 93, loss = 0.02936621\n",
      "Validation score: 0.955721\n",
      "Iteration 94, loss = 0.02896983\n",
      "Validation score: 0.957114\n",
      "Iteration 95, loss = 0.02845497\n",
      "Validation score: 0.956804\n",
      "Iteration 96, loss = 0.02792507\n",
      "Validation score: 0.957114\n",
      "Iteration 97, loss = 0.02763896\n",
      "Validation score: 0.955101\n",
      "Iteration 98, loss = 0.02727387\n",
      "Validation score: 0.955566\n",
      "Iteration 99, loss = 0.02687156\n",
      "Validation score: 0.956185\n",
      "Iteration 100, loss = 0.02648123\n",
      "Validation score: 0.956650\n",
      "Iteration 101, loss = 0.02602721\n",
      "Validation score: 0.957579\n",
      "Iteration 102, loss = 0.02566928\n",
      "Validation score: 0.956030\n",
      "Validation score did not improve more than tol=0.000100 for 25 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.35442325\n",
      "Validation score: 0.911906\n",
      "Iteration 2, loss = 0.18640035\n",
      "Validation score: 0.927853\n",
      "Iteration 3, loss = 0.15553096\n",
      "Validation score: 0.932652\n",
      "Iteration 4, loss = 0.14052559\n",
      "Validation score: 0.936678\n",
      "Iteration 5, loss = 0.13151263\n",
      "Validation score: 0.939619\n",
      "Iteration 6, loss = 0.12493602\n",
      "Validation score: 0.940858\n",
      "Iteration 7, loss = 0.12008251\n",
      "Validation score: 0.943025\n",
      "Iteration 8, loss = 0.11607889\n",
      "Validation score: 0.943180\n",
      "Iteration 9, loss = 0.11261865\n",
      "Validation score: 0.945502\n",
      "Iteration 10, loss = 0.10986459\n",
      "Validation score: 0.945812\n",
      "Iteration 11, loss = 0.10690331\n",
      "Validation score: 0.947051\n",
      "Iteration 12, loss = 0.10469678\n",
      "Validation score: 0.948908\n",
      "Iteration 13, loss = 0.10240352\n",
      "Validation score: 0.949063\n",
      "Iteration 14, loss = 0.10050133\n",
      "Validation score: 0.949992\n",
      "Iteration 15, loss = 0.09844042\n",
      "Validation score: 0.949528\n",
      "Iteration 16, loss = 0.09655254\n",
      "Validation score: 0.949837\n",
      "Iteration 17, loss = 0.09487552\n",
      "Validation score: 0.950921\n",
      "Iteration 18, loss = 0.09296029\n",
      "Validation score: 0.950457\n",
      "Iteration 19, loss = 0.09152417\n",
      "Validation score: 0.951386\n",
      "Iteration 20, loss = 0.08975420\n",
      "Validation score: 0.951386\n",
      "Iteration 21, loss = 0.08832116\n",
      "Validation score: 0.951076\n",
      "Iteration 22, loss = 0.08682086\n",
      "Validation score: 0.952005\n",
      "Iteration 23, loss = 0.08534425\n",
      "Validation score: 0.951386\n",
      "Iteration 24, loss = 0.08420631\n",
      "Validation score: 0.951076\n",
      "Iteration 25, loss = 0.08276850\n",
      "Validation score: 0.952469\n",
      "Iteration 26, loss = 0.08138171\n",
      "Validation score: 0.951540\n",
      "Iteration 27, loss = 0.08013852\n",
      "Validation score: 0.953553\n",
      "Iteration 28, loss = 0.07885083\n",
      "Validation score: 0.952469\n",
      "Iteration 29, loss = 0.07768999\n",
      "Validation score: 0.953553\n",
      "Iteration 30, loss = 0.07660611\n",
      "Validation score: 0.952779\n",
      "Iteration 31, loss = 0.07536776\n",
      "Validation score: 0.953398\n",
      "Iteration 32, loss = 0.07422857\n",
      "Validation score: 0.953863\n",
      "Iteration 33, loss = 0.07300849\n",
      "Validation score: 0.952779\n",
      "Iteration 34, loss = 0.07195869\n",
      "Validation score: 0.953089\n",
      "Iteration 35, loss = 0.07093252\n",
      "Validation score: 0.953398\n",
      "Iteration 36, loss = 0.07001983\n",
      "Validation score: 0.954792\n",
      "Iteration 37, loss = 0.06868567\n",
      "Validation score: 0.954482\n",
      "Iteration 38, loss = 0.06767866\n",
      "Validation score: 0.955101\n",
      "Iteration 39, loss = 0.06678447\n",
      "Validation score: 0.953708\n",
      "Iteration 40, loss = 0.06582558\n",
      "Validation score: 0.954482\n",
      "Iteration 41, loss = 0.06481664\n",
      "Validation score: 0.954637\n",
      "Iteration 42, loss = 0.06369389\n",
      "Validation score: 0.954947\n",
      "Iteration 43, loss = 0.06281639\n",
      "Validation score: 0.955876\n",
      "Iteration 44, loss = 0.06203219\n",
      "Validation score: 0.955721\n",
      "Iteration 45, loss = 0.06105245\n",
      "Validation score: 0.954637\n",
      "Iteration 46, loss = 0.06020287\n",
      "Validation score: 0.953863\n",
      "Iteration 47, loss = 0.05916590\n",
      "Validation score: 0.954947\n",
      "Iteration 48, loss = 0.05840328\n",
      "Validation score: 0.955101\n",
      "Iteration 49, loss = 0.05752602\n",
      "Validation score: 0.954637\n",
      "Iteration 50, loss = 0.05674350\n",
      "Validation score: 0.956340\n",
      "Iteration 51, loss = 0.05592031\n",
      "Validation score: 0.956030\n",
      "Iteration 52, loss = 0.05517310\n",
      "Validation score: 0.955411\n",
      "Iteration 53, loss = 0.05447133\n",
      "Validation score: 0.956650\n",
      "Iteration 54, loss = 0.05364299\n",
      "Validation score: 0.954947\n",
      "Iteration 55, loss = 0.05282013\n",
      "Validation score: 0.956340\n",
      "Iteration 56, loss = 0.05198580\n",
      "Validation score: 0.956030\n",
      "Iteration 57, loss = 0.05144901\n",
      "Validation score: 0.954947\n",
      "Iteration 58, loss = 0.05047857\n",
      "Validation score: 0.956340\n",
      "Iteration 59, loss = 0.04967906\n",
      "Validation score: 0.956340\n",
      "Iteration 60, loss = 0.04927481\n",
      "Validation score: 0.956495\n",
      "Iteration 61, loss = 0.04839981\n",
      "Validation score: 0.955566\n",
      "Iteration 62, loss = 0.04781697\n",
      "Validation score: 0.956495\n",
      "Iteration 63, loss = 0.04704316\n",
      "Validation score: 0.956340\n",
      "Iteration 64, loss = 0.04644609\n",
      "Validation score: 0.957424\n",
      "Iteration 65, loss = 0.04569557\n",
      "Validation score: 0.955256\n",
      "Iteration 66, loss = 0.04504895\n",
      "Validation score: 0.956185\n",
      "Iteration 67, loss = 0.04438149\n",
      "Validation score: 0.957269\n",
      "Iteration 68, loss = 0.04386444\n",
      "Validation score: 0.956804\n",
      "Iteration 69, loss = 0.04322918\n",
      "Validation score: 0.954947\n",
      "Iteration 70, loss = 0.04260003\n",
      "Validation score: 0.956030\n",
      "Iteration 71, loss = 0.04176681\n",
      "Validation score: 0.956185\n",
      "Iteration 72, loss = 0.04139792\n",
      "Validation score: 0.956804\n",
      "Iteration 73, loss = 0.04063980\n",
      "Validation score: 0.956650\n",
      "Iteration 74, loss = 0.04010986\n",
      "Validation score: 0.955876\n",
      "Iteration 75, loss = 0.03951194\n",
      "Validation score: 0.955101\n",
      "Iteration 76, loss = 0.03904231\n",
      "Validation score: 0.955411\n",
      "Iteration 77, loss = 0.03837128\n",
      "Validation score: 0.956185\n",
      "Iteration 78, loss = 0.03800293\n",
      "Validation score: 0.956959\n",
      "Iteration 79, loss = 0.03728911\n",
      "Validation score: 0.956495\n",
      "Iteration 80, loss = 0.03682977\n",
      "Validation score: 0.956804\n",
      "Iteration 81, loss = 0.03623435\n",
      "Validation score: 0.956340\n",
      "Iteration 82, loss = 0.03581079\n",
      "Validation score: 0.955411\n",
      "Iteration 83, loss = 0.03526888\n",
      "Validation score: 0.955876\n",
      "Iteration 84, loss = 0.03481516\n",
      "Validation score: 0.957114\n",
      "Iteration 85, loss = 0.03425060\n",
      "Validation score: 0.956340\n",
      "Iteration 86, loss = 0.03377753\n",
      "Validation score: 0.956959\n",
      "Iteration 87, loss = 0.03332638\n",
      "Validation score: 0.956185\n",
      "Iteration 88, loss = 0.03261360\n",
      "Validation score: 0.955411\n",
      "Iteration 89, loss = 0.03221984\n",
      "Validation score: 0.956340\n",
      "Iteration 90, loss = 0.03183852\n",
      "Validation score: 0.956959\n",
      "Validation score did not improve more than tol=0.000100 for 25 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.28078084\n",
      "Validation score: 0.922589\n",
      "Iteration 2, loss = 0.17164125\n",
      "Validation score: 0.934355\n",
      "Iteration 3, loss = 0.14664814\n",
      "Validation score: 0.939155\n",
      "Iteration 4, loss = 0.13371537\n",
      "Validation score: 0.943025\n",
      "Iteration 5, loss = 0.12554595\n",
      "Validation score: 0.943335\n",
      "Iteration 6, loss = 0.11944931\n",
      "Validation score: 0.945502\n",
      "Iteration 7, loss = 0.11460871\n",
      "Validation score: 0.945812\n",
      "Iteration 8, loss = 0.11090681\n",
      "Validation score: 0.947670\n",
      "Iteration 9, loss = 0.10752595\n",
      "Validation score: 0.946431\n",
      "Iteration 10, loss = 0.10464818\n",
      "Validation score: 0.949373\n",
      "Iteration 11, loss = 0.10199197\n",
      "Validation score: 0.948134\n",
      "Iteration 12, loss = 0.09971673\n",
      "Validation score: 0.949992\n",
      "Iteration 13, loss = 0.09732768\n",
      "Validation score: 0.948599\n",
      "Iteration 14, loss = 0.09527608\n",
      "Validation score: 0.949992\n",
      "Iteration 15, loss = 0.09331473\n",
      "Validation score: 0.949528\n",
      "Iteration 16, loss = 0.09142701\n",
      "Validation score: 0.950147\n",
      "Iteration 17, loss = 0.08977379\n",
      "Validation score: 0.950766\n",
      "Iteration 18, loss = 0.08813242\n",
      "Validation score: 0.950302\n",
      "Iteration 19, loss = 0.08647735\n",
      "Validation score: 0.950921\n",
      "Iteration 20, loss = 0.08487904\n",
      "Validation score: 0.951695\n",
      "Iteration 21, loss = 0.08340922\n",
      "Validation score: 0.951695\n",
      "Iteration 22, loss = 0.08191132\n",
      "Validation score: 0.951695\n",
      "Iteration 23, loss = 0.08074164\n",
      "Validation score: 0.952005\n",
      "Iteration 24, loss = 0.07925930\n",
      "Validation score: 0.951695\n",
      "Iteration 25, loss = 0.07782744\n",
      "Validation score: 0.952005\n",
      "Iteration 26, loss = 0.07668695\n",
      "Validation score: 0.951695\n",
      "Iteration 27, loss = 0.07538811\n",
      "Validation score: 0.953089\n",
      "Iteration 28, loss = 0.07422346\n",
      "Validation score: 0.952315\n",
      "Iteration 29, loss = 0.07291268\n",
      "Validation score: 0.952315\n",
      "Iteration 30, loss = 0.07191877\n",
      "Validation score: 0.952624\n",
      "Iteration 31, loss = 0.07085982\n",
      "Validation score: 0.953863\n",
      "Iteration 32, loss = 0.06966226\n",
      "Validation score: 0.952160\n",
      "Iteration 33, loss = 0.06865791\n",
      "Validation score: 0.953708\n",
      "Iteration 34, loss = 0.06770234\n",
      "Validation score: 0.952624\n",
      "Iteration 35, loss = 0.06669754\n",
      "Validation score: 0.953708\n",
      "Iteration 36, loss = 0.06565720\n",
      "Validation score: 0.954947\n",
      "Iteration 37, loss = 0.06450772\n",
      "Validation score: 0.954637\n",
      "Iteration 38, loss = 0.06345598\n",
      "Validation score: 0.954792\n",
      "Iteration 39, loss = 0.06241045\n",
      "Validation score: 0.953863\n",
      "Iteration 40, loss = 0.06178268\n",
      "Validation score: 0.954482\n",
      "Iteration 41, loss = 0.06076524\n",
      "Validation score: 0.954327\n",
      "Iteration 42, loss = 0.05974824\n",
      "Validation score: 0.953398\n",
      "Iteration 43, loss = 0.05893736\n",
      "Validation score: 0.954637\n",
      "Iteration 44, loss = 0.05807450\n",
      "Validation score: 0.954327\n",
      "Iteration 45, loss = 0.05717720\n",
      "Validation score: 0.954792\n",
      "Iteration 46, loss = 0.05622126\n",
      "Validation score: 0.954947\n",
      "Iteration 47, loss = 0.05537988\n",
      "Validation score: 0.953863\n",
      "Iteration 48, loss = 0.05473701\n",
      "Validation score: 0.954172\n",
      "Iteration 49, loss = 0.05384215\n",
      "Validation score: 0.955566\n",
      "Iteration 50, loss = 0.05315408\n",
      "Validation score: 0.954018\n",
      "Iteration 51, loss = 0.05224437\n",
      "Validation score: 0.955256\n",
      "Iteration 52, loss = 0.05147183\n",
      "Validation score: 0.956804\n",
      "Iteration 53, loss = 0.05069093\n",
      "Validation score: 0.955411\n",
      "Iteration 54, loss = 0.05020387\n",
      "Validation score: 0.954327\n",
      "Iteration 55, loss = 0.04934311\n",
      "Validation score: 0.954327\n",
      "Iteration 56, loss = 0.04854491\n",
      "Validation score: 0.956030\n",
      "Iteration 57, loss = 0.04773501\n",
      "Validation score: 0.956185\n",
      "Iteration 58, loss = 0.04722299\n",
      "Validation score: 0.954172\n",
      "Iteration 59, loss = 0.04653165\n",
      "Validation score: 0.956340\n",
      "Iteration 60, loss = 0.04586580\n",
      "Validation score: 0.954172\n",
      "Iteration 61, loss = 0.04505589\n",
      "Validation score: 0.956340\n",
      "Iteration 62, loss = 0.04452162\n",
      "Validation score: 0.955721\n",
      "Iteration 63, loss = 0.04385679\n",
      "Validation score: 0.955256\n",
      "Iteration 64, loss = 0.04305647\n",
      "Validation score: 0.956495\n",
      "Iteration 65, loss = 0.04257019\n",
      "Validation score: 0.955411\n",
      "Iteration 66, loss = 0.04191777\n",
      "Validation score: 0.956959\n",
      "Iteration 67, loss = 0.04118704\n",
      "Validation score: 0.956340\n",
      "Iteration 68, loss = 0.04097887\n",
      "Validation score: 0.956959\n",
      "Iteration 69, loss = 0.04004789\n",
      "Validation score: 0.956340\n",
      "Iteration 70, loss = 0.03936688\n",
      "Validation score: 0.955411\n",
      "Iteration 71, loss = 0.03896124\n",
      "Validation score: 0.955101\n",
      "Iteration 72, loss = 0.03866637\n",
      "Validation score: 0.954947\n",
      "Iteration 73, loss = 0.03779377\n",
      "Validation score: 0.955101\n",
      "Iteration 74, loss = 0.03724512\n",
      "Validation score: 0.956959\n",
      "Iteration 75, loss = 0.03672959\n",
      "Validation score: 0.956185\n",
      "Iteration 76, loss = 0.03610761\n",
      "Validation score: 0.956030\n",
      "Iteration 77, loss = 0.03553501\n",
      "Validation score: 0.955101\n",
      "Iteration 78, loss = 0.03507013\n",
      "Validation score: 0.955411\n",
      "Iteration 79, loss = 0.03459610\n",
      "Validation score: 0.956650\n",
      "Iteration 80, loss = 0.03408236\n",
      "Validation score: 0.955566\n",
      "Iteration 81, loss = 0.03365585\n",
      "Validation score: 0.956185\n",
      "Iteration 82, loss = 0.03312992\n",
      "Validation score: 0.955411\n",
      "Iteration 83, loss = 0.03254735\n",
      "Validation score: 0.955566\n",
      "Iteration 84, loss = 0.03214701\n",
      "Validation score: 0.954482\n",
      "Iteration 85, loss = 0.03179795\n",
      "Validation score: 0.955721\n",
      "Iteration 86, loss = 0.03128484\n",
      "Validation score: 0.955101\n",
      "Iteration 87, loss = 0.03058238\n",
      "Validation score: 0.954018\n",
      "Iteration 88, loss = 0.03024487\n",
      "Validation score: 0.954637\n",
      "Iteration 89, loss = 0.02974938\n",
      "Validation score: 0.955876\n",
      "Iteration 90, loss = 0.02925887\n",
      "Validation score: 0.955721\n",
      "Iteration 91, loss = 0.02883878\n",
      "Validation score: 0.954947\n",
      "Iteration 92, loss = 0.02839795\n",
      "Validation score: 0.955256\n",
      "Validation score did not improve more than tol=0.000100 for 25 consecutive epochs. Stopping.\n",
      "All 5 repeats done! \n",
      "\n",
      "Neural Network: undersampling at rate 0.3 from the most popular class:\n",
      "Iteration 1, loss = 0.24913888\n",
      "Validation score: 0.938434\n",
      "Iteration 2, loss = 0.14123323\n",
      "Validation score: 0.950061\n",
      "Iteration 3, loss = 0.12196339\n",
      "Validation score: 0.954047\n",
      "Iteration 4, loss = 0.11240514\n",
      "Validation score: 0.955819\n",
      "Iteration 5, loss = 0.10610962\n",
      "Validation score: 0.956926\n",
      "Iteration 6, loss = 0.10150921\n",
      "Validation score: 0.957923\n",
      "Iteration 7, loss = 0.09795980\n",
      "Validation score: 0.959251\n",
      "Iteration 8, loss = 0.09478551\n",
      "Validation score: 0.959030\n",
      "Iteration 9, loss = 0.09227869\n",
      "Validation score: 0.960580\n",
      "Iteration 10, loss = 0.08984524\n",
      "Validation score: 0.959584\n",
      "Iteration 11, loss = 0.08778024\n",
      "Validation score: 0.961245\n",
      "Iteration 12, loss = 0.08575677\n",
      "Validation score: 0.961466\n",
      "Iteration 13, loss = 0.08405050\n",
      "Validation score: 0.962684\n",
      "Iteration 14, loss = 0.08227125\n",
      "Validation score: 0.962241\n",
      "Iteration 15, loss = 0.08041174\n",
      "Validation score: 0.961798\n",
      "Iteration 16, loss = 0.07897253\n",
      "Validation score: 0.962906\n",
      "Iteration 17, loss = 0.07750933\n",
      "Validation score: 0.962352\n",
      "Iteration 18, loss = 0.07596851\n",
      "Validation score: 0.962573\n",
      "Iteration 19, loss = 0.07453824\n",
      "Validation score: 0.962352\n",
      "Iteration 20, loss = 0.07327849\n",
      "Validation score: 0.963902\n",
      "Iteration 21, loss = 0.07180739\n",
      "Validation score: 0.962352\n",
      "Iteration 22, loss = 0.07060838\n",
      "Validation score: 0.964345\n",
      "Iteration 23, loss = 0.06950581\n",
      "Validation score: 0.962573\n",
      "Iteration 24, loss = 0.06818594\n",
      "Validation score: 0.964456\n",
      "Iteration 25, loss = 0.06708648\n",
      "Validation score: 0.964677\n",
      "Iteration 26, loss = 0.06597579\n",
      "Validation score: 0.963681\n",
      "Iteration 27, loss = 0.06498819\n",
      "Validation score: 0.965342\n",
      "Iteration 28, loss = 0.06397858\n",
      "Validation score: 0.966117\n",
      "Iteration 29, loss = 0.06288339\n",
      "Validation score: 0.964788\n",
      "Iteration 30, loss = 0.06181902\n",
      "Validation score: 0.963570\n",
      "Iteration 31, loss = 0.06090124\n",
      "Validation score: 0.964234\n",
      "Iteration 32, loss = 0.05997903\n",
      "Validation score: 0.963902\n",
      "Iteration 33, loss = 0.05909524\n",
      "Validation score: 0.965231\n",
      "Iteration 34, loss = 0.05804122\n",
      "Validation score: 0.964788\n",
      "Iteration 35, loss = 0.05725707\n",
      "Validation score: 0.964345\n",
      "Iteration 36, loss = 0.05624756\n",
      "Validation score: 0.964899\n",
      "Iteration 37, loss = 0.05542210\n",
      "Validation score: 0.964899\n",
      "Iteration 38, loss = 0.05465321\n",
      "Validation score: 0.965452\n",
      "Iteration 39, loss = 0.05391604\n",
      "Validation score: 0.964566\n",
      "Iteration 40, loss = 0.05313025\n",
      "Validation score: 0.965231\n",
      "Iteration 41, loss = 0.05221216\n",
      "Validation score: 0.965674\n",
      "Iteration 42, loss = 0.05175320\n",
      "Validation score: 0.965231\n",
      "Iteration 43, loss = 0.05080594\n",
      "Validation score: 0.964566\n",
      "Iteration 44, loss = 0.05009853\n",
      "Validation score: 0.965563\n",
      "Iteration 45, loss = 0.04920131\n",
      "Validation score: 0.965120\n",
      "Iteration 46, loss = 0.04849416\n",
      "Validation score: 0.964788\n",
      "Iteration 47, loss = 0.04802016\n",
      "Validation score: 0.965231\n",
      "Iteration 48, loss = 0.04717862\n",
      "Validation score: 0.965009\n",
      "Iteration 49, loss = 0.04663296\n",
      "Validation score: 0.965120\n",
      "Iteration 50, loss = 0.04593949\n",
      "Validation score: 0.965785\n",
      "Iteration 51, loss = 0.04533592\n",
      "Validation score: 0.964677\n",
      "Iteration 52, loss = 0.04460674\n",
      "Validation score: 0.964788\n",
      "Iteration 53, loss = 0.04409454\n",
      "Validation score: 0.964677\n",
      "Iteration 54, loss = 0.04337690\n",
      "Validation score: 0.964788\n",
      "Validation score did not improve more than tol=0.000100 for 25 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.25972645\n",
      "Validation score: 0.931237\n",
      "Iteration 2, loss = 0.14498218\n",
      "Validation score: 0.944967\n",
      "Iteration 3, loss = 0.12423957\n",
      "Validation score: 0.949839\n",
      "Iteration 4, loss = 0.11396706\n",
      "Validation score: 0.953272\n",
      "Iteration 5, loss = 0.10762171\n",
      "Validation score: 0.953604\n",
      "Iteration 6, loss = 0.10268016\n",
      "Validation score: 0.954933\n",
      "Iteration 7, loss = 0.09880145\n",
      "Validation score: 0.954933\n",
      "Iteration 8, loss = 0.09569009\n",
      "Validation score: 0.956594\n",
      "Iteration 9, loss = 0.09289460\n",
      "Validation score: 0.957591\n",
      "Iteration 10, loss = 0.09060166\n",
      "Validation score: 0.958366\n",
      "Iteration 11, loss = 0.08832359\n",
      "Validation score: 0.956594\n",
      "Iteration 12, loss = 0.08633190\n",
      "Validation score: 0.958587\n",
      "Iteration 13, loss = 0.08433237\n",
      "Validation score: 0.960137\n",
      "Iteration 14, loss = 0.08257106\n",
      "Validation score: 0.957923\n",
      "Iteration 15, loss = 0.08102544\n",
      "Validation score: 0.958919\n",
      "Iteration 16, loss = 0.07925343\n",
      "Validation score: 0.959916\n",
      "Iteration 17, loss = 0.07770623\n",
      "Validation score: 0.959251\n",
      "Iteration 18, loss = 0.07631371\n",
      "Validation score: 0.959584\n",
      "Iteration 19, loss = 0.07499179\n",
      "Validation score: 0.959916\n",
      "Iteration 20, loss = 0.07363433\n",
      "Validation score: 0.960802\n",
      "Iteration 21, loss = 0.07232112\n",
      "Validation score: 0.960580\n",
      "Iteration 22, loss = 0.07120669\n",
      "Validation score: 0.961023\n",
      "Iteration 23, loss = 0.06982794\n",
      "Validation score: 0.959362\n",
      "Iteration 24, loss = 0.06860368\n",
      "Validation score: 0.960912\n",
      "Iteration 25, loss = 0.06741900\n",
      "Validation score: 0.959805\n",
      "Iteration 26, loss = 0.06653741\n",
      "Validation score: 0.960802\n",
      "Iteration 27, loss = 0.06544409\n",
      "Validation score: 0.961023\n",
      "Iteration 28, loss = 0.06422547\n",
      "Validation score: 0.962684\n",
      "Iteration 29, loss = 0.06327473\n",
      "Validation score: 0.959916\n",
      "Iteration 30, loss = 0.06218473\n",
      "Validation score: 0.961577\n",
      "Iteration 31, loss = 0.06126662\n",
      "Validation score: 0.961023\n",
      "Iteration 32, loss = 0.06052864\n",
      "Validation score: 0.961909\n",
      "Iteration 33, loss = 0.05947627\n",
      "Validation score: 0.961134\n",
      "Iteration 34, loss = 0.05836108\n",
      "Validation score: 0.960912\n",
      "Iteration 35, loss = 0.05747892\n",
      "Validation score: 0.962020\n",
      "Iteration 36, loss = 0.05663271\n",
      "Validation score: 0.962130\n",
      "Iteration 37, loss = 0.05583144\n",
      "Validation score: 0.962463\n",
      "Iteration 38, loss = 0.05496722\n",
      "Validation score: 0.961909\n",
      "Iteration 39, loss = 0.05400836\n",
      "Validation score: 0.962573\n",
      "Iteration 40, loss = 0.05332810\n",
      "Validation score: 0.961688\n",
      "Iteration 41, loss = 0.05247103\n",
      "Validation score: 0.962352\n",
      "Iteration 42, loss = 0.05161787\n",
      "Validation score: 0.963016\n",
      "Iteration 43, loss = 0.05105616\n",
      "Validation score: 0.961466\n",
      "Iteration 44, loss = 0.05023096\n",
      "Validation score: 0.962795\n",
      "Iteration 45, loss = 0.04934344\n",
      "Validation score: 0.962020\n",
      "Iteration 46, loss = 0.04873385\n",
      "Validation score: 0.963016\n",
      "Iteration 47, loss = 0.04786856\n",
      "Validation score: 0.961245\n",
      "Iteration 48, loss = 0.04741790\n",
      "Validation score: 0.962795\n",
      "Iteration 49, loss = 0.04667780\n",
      "Validation score: 0.962906\n",
      "Iteration 50, loss = 0.04586388\n",
      "Validation score: 0.962684\n",
      "Iteration 51, loss = 0.04515726\n",
      "Validation score: 0.962573\n",
      "Iteration 52, loss = 0.04466813\n",
      "Validation score: 0.963127\n",
      "Iteration 53, loss = 0.04391182\n",
      "Validation score: 0.962906\n",
      "Iteration 54, loss = 0.04329223\n",
      "Validation score: 0.962352\n",
      "Iteration 55, loss = 0.04282385\n",
      "Validation score: 0.962241\n",
      "Iteration 56, loss = 0.04203195\n",
      "Validation score: 0.962684\n",
      "Iteration 57, loss = 0.04133906\n",
      "Validation score: 0.964124\n",
      "Iteration 58, loss = 0.04076662\n",
      "Validation score: 0.963016\n",
      "Iteration 59, loss = 0.04024922\n",
      "Validation score: 0.962795\n",
      "Iteration 60, loss = 0.03944212\n",
      "Validation score: 0.962684\n",
      "Iteration 61, loss = 0.03904139\n",
      "Validation score: 0.962020\n",
      "Iteration 62, loss = 0.03848021\n",
      "Validation score: 0.961688\n",
      "Iteration 63, loss = 0.03801893\n",
      "Validation score: 0.963016\n",
      "Iteration 64, loss = 0.03720134\n",
      "Validation score: 0.963570\n",
      "Iteration 65, loss = 0.03680994\n",
      "Validation score: 0.962352\n",
      "Iteration 66, loss = 0.03624583\n",
      "Validation score: 0.963016\n",
      "Iteration 67, loss = 0.03570920\n",
      "Validation score: 0.962906\n",
      "Iteration 68, loss = 0.03532109\n",
      "Validation score: 0.963127\n",
      "Iteration 69, loss = 0.03463265\n",
      "Validation score: 0.962684\n",
      "Iteration 70, loss = 0.03418595\n",
      "Validation score: 0.963348\n",
      "Iteration 71, loss = 0.03391170\n",
      "Validation score: 0.963127\n",
      "Iteration 72, loss = 0.03316666\n",
      "Validation score: 0.963459\n",
      "Iteration 73, loss = 0.03257382\n",
      "Validation score: 0.962684\n",
      "Iteration 74, loss = 0.03222619\n",
      "Validation score: 0.963238\n",
      "Iteration 75, loss = 0.03170122\n",
      "Validation score: 0.962684\n",
      "Iteration 76, loss = 0.03132282\n",
      "Validation score: 0.962241\n",
      "Iteration 77, loss = 0.03078083\n",
      "Validation score: 0.962906\n",
      "Iteration 78, loss = 0.03032296\n",
      "Validation score: 0.963570\n",
      "Iteration 79, loss = 0.02991501\n",
      "Validation score: 0.962352\n",
      "Iteration 80, loss = 0.02948725\n",
      "Validation score: 0.963791\n",
      "Iteration 81, loss = 0.02904860\n",
      "Validation score: 0.963681\n",
      "Iteration 82, loss = 0.02868064\n",
      "Validation score: 0.963570\n",
      "Iteration 83, loss = 0.02812716\n",
      "Validation score: 0.962463\n",
      "Validation score did not improve more than tol=0.000100 for 25 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.27040326\n",
      "Validation score: 0.935223\n",
      "Iteration 2, loss = 0.14449847\n",
      "Validation score: 0.943749\n",
      "Iteration 3, loss = 0.12442414\n",
      "Validation score: 0.947736\n",
      "Iteration 4, loss = 0.11419316\n",
      "Validation score: 0.950836\n",
      "Iteration 5, loss = 0.10770429\n",
      "Validation score: 0.952165\n",
      "Iteration 6, loss = 0.10278680\n",
      "Validation score: 0.952829\n",
      "Iteration 7, loss = 0.09899358\n",
      "Validation score: 0.954269\n",
      "Iteration 8, loss = 0.09584014\n",
      "Validation score: 0.954712\n",
      "Iteration 9, loss = 0.09302416\n",
      "Validation score: 0.955708\n",
      "Iteration 10, loss = 0.09039472\n",
      "Validation score: 0.956594\n",
      "Iteration 11, loss = 0.08815749\n",
      "Validation score: 0.956372\n",
      "Iteration 12, loss = 0.08627958\n",
      "Validation score: 0.957591\n",
      "Iteration 13, loss = 0.08432941\n",
      "Validation score: 0.957369\n",
      "Iteration 14, loss = 0.08242429\n",
      "Validation score: 0.958033\n",
      "Iteration 15, loss = 0.08065670\n",
      "Validation score: 0.958698\n",
      "Iteration 16, loss = 0.07921363\n",
      "Validation score: 0.957812\n",
      "Iteration 17, loss = 0.07762352\n",
      "Validation score: 0.958476\n",
      "Iteration 18, loss = 0.07594545\n",
      "Validation score: 0.959251\n",
      "Iteration 19, loss = 0.07456909\n",
      "Validation score: 0.958144\n",
      "Iteration 20, loss = 0.07319823\n",
      "Validation score: 0.959251\n",
      "Iteration 21, loss = 0.07194851\n",
      "Validation score: 0.958144\n",
      "Iteration 22, loss = 0.07068111\n",
      "Validation score: 0.959584\n",
      "Iteration 23, loss = 0.06951855\n",
      "Validation score: 0.958476\n",
      "Iteration 24, loss = 0.06836527\n",
      "Validation score: 0.958033\n",
      "Iteration 25, loss = 0.06691049\n",
      "Validation score: 0.959473\n",
      "Iteration 26, loss = 0.06592803\n",
      "Validation score: 0.959916\n",
      "Iteration 27, loss = 0.06471761\n",
      "Validation score: 0.959251\n",
      "Iteration 28, loss = 0.06369485\n",
      "Validation score: 0.959473\n",
      "Iteration 29, loss = 0.06261484\n",
      "Validation score: 0.959141\n",
      "Iteration 30, loss = 0.06158371\n",
      "Validation score: 0.958476\n",
      "Iteration 31, loss = 0.06069232\n",
      "Validation score: 0.959362\n",
      "Iteration 32, loss = 0.05970604\n",
      "Validation score: 0.960027\n",
      "Iteration 33, loss = 0.05870237\n",
      "Validation score: 0.960137\n",
      "Iteration 34, loss = 0.05786532\n",
      "Validation score: 0.960469\n",
      "Iteration 35, loss = 0.05685730\n",
      "Validation score: 0.961134\n",
      "Iteration 36, loss = 0.05610119\n",
      "Validation score: 0.960027\n",
      "Iteration 37, loss = 0.05511813\n",
      "Validation score: 0.960691\n",
      "Iteration 38, loss = 0.05454258\n",
      "Validation score: 0.961355\n",
      "Iteration 39, loss = 0.05351577\n",
      "Validation score: 0.959584\n",
      "Iteration 40, loss = 0.05263303\n",
      "Validation score: 0.960137\n",
      "Iteration 41, loss = 0.05195285\n",
      "Validation score: 0.960912\n",
      "Iteration 42, loss = 0.05088208\n",
      "Validation score: 0.962241\n",
      "Iteration 43, loss = 0.05030440\n",
      "Validation score: 0.959916\n",
      "Iteration 44, loss = 0.04946549\n",
      "Validation score: 0.961134\n",
      "Iteration 45, loss = 0.04892896\n",
      "Validation score: 0.960802\n",
      "Iteration 46, loss = 0.04807322\n",
      "Validation score: 0.960580\n",
      "Iteration 47, loss = 0.04726737\n",
      "Validation score: 0.960691\n",
      "Iteration 48, loss = 0.04670435\n",
      "Validation score: 0.961577\n",
      "Iteration 49, loss = 0.04598293\n",
      "Validation score: 0.959473\n",
      "Iteration 50, loss = 0.04521853\n",
      "Validation score: 0.961798\n",
      "Iteration 51, loss = 0.04470698\n",
      "Validation score: 0.960580\n",
      "Iteration 52, loss = 0.04395703\n",
      "Validation score: 0.960359\n",
      "Iteration 53, loss = 0.04348485\n",
      "Validation score: 0.961245\n",
      "Iteration 54, loss = 0.04271663\n",
      "Validation score: 0.960469\n",
      "Iteration 55, loss = 0.04203465\n",
      "Validation score: 0.960248\n",
      "Iteration 56, loss = 0.04150625\n",
      "Validation score: 0.960027\n",
      "Iteration 57, loss = 0.04101183\n",
      "Validation score: 0.959805\n",
      "Iteration 58, loss = 0.04023543\n",
      "Validation score: 0.961023\n",
      "Iteration 59, loss = 0.03953962\n",
      "Validation score: 0.959805\n",
      "Iteration 60, loss = 0.03922251\n",
      "Validation score: 0.961245\n",
      "Iteration 61, loss = 0.03857379\n",
      "Validation score: 0.960359\n",
      "Iteration 62, loss = 0.03801231\n",
      "Validation score: 0.961245\n",
      "Iteration 63, loss = 0.03756378\n",
      "Validation score: 0.959030\n",
      "Iteration 64, loss = 0.03683263\n",
      "Validation score: 0.961134\n",
      "Iteration 65, loss = 0.03640960\n",
      "Validation score: 0.960359\n",
      "Iteration 66, loss = 0.03582510\n",
      "Validation score: 0.959916\n",
      "Iteration 67, loss = 0.03554289\n",
      "Validation score: 0.960691\n",
      "Iteration 68, loss = 0.03486711\n",
      "Validation score: 0.960248\n",
      "Validation score did not improve more than tol=0.000100 for 25 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.35079480\n",
      "Validation score: 0.928912\n",
      "Iteration 2, loss = 0.15911497\n",
      "Validation score: 0.941645\n",
      "Iteration 3, loss = 0.13315414\n",
      "Validation score: 0.947957\n",
      "Iteration 4, loss = 0.12055523\n",
      "Validation score: 0.951057\n",
      "Iteration 5, loss = 0.11290672\n",
      "Validation score: 0.953272\n",
      "Iteration 6, loss = 0.10771144\n",
      "Validation score: 0.954933\n",
      "Iteration 7, loss = 0.10348099\n",
      "Validation score: 0.955044\n",
      "Iteration 8, loss = 0.10013845\n",
      "Validation score: 0.957148\n",
      "Iteration 9, loss = 0.09736488\n",
      "Validation score: 0.957812\n",
      "Iteration 10, loss = 0.09486105\n",
      "Validation score: 0.958255\n",
      "Iteration 11, loss = 0.09248384\n",
      "Validation score: 0.959030\n",
      "Iteration 12, loss = 0.09050651\n",
      "Validation score: 0.961466\n",
      "Iteration 13, loss = 0.08859732\n",
      "Validation score: 0.961134\n",
      "Iteration 14, loss = 0.08681514\n",
      "Validation score: 0.960027\n",
      "Iteration 15, loss = 0.08518971\n",
      "Validation score: 0.961245\n",
      "Iteration 16, loss = 0.08353743\n",
      "Validation score: 0.960359\n",
      "Iteration 17, loss = 0.08183243\n",
      "Validation score: 0.961798\n",
      "Iteration 18, loss = 0.08042376\n",
      "Validation score: 0.961466\n",
      "Iteration 19, loss = 0.07891616\n",
      "Validation score: 0.961688\n",
      "Iteration 20, loss = 0.07758905\n",
      "Validation score: 0.961688\n",
      "Iteration 21, loss = 0.07620722\n",
      "Validation score: 0.961023\n",
      "Iteration 22, loss = 0.07490323\n",
      "Validation score: 0.962463\n",
      "Iteration 23, loss = 0.07383951\n",
      "Validation score: 0.962573\n",
      "Iteration 24, loss = 0.07257428\n",
      "Validation score: 0.962795\n",
      "Iteration 25, loss = 0.07125248\n",
      "Validation score: 0.962573\n",
      "Iteration 26, loss = 0.07029662\n",
      "Validation score: 0.963348\n",
      "Iteration 27, loss = 0.06903509\n",
      "Validation score: 0.963348\n",
      "Iteration 28, loss = 0.06811055\n",
      "Validation score: 0.962795\n",
      "Iteration 29, loss = 0.06699819\n",
      "Validation score: 0.963016\n",
      "Iteration 30, loss = 0.06599439\n",
      "Validation score: 0.963459\n",
      "Iteration 31, loss = 0.06492801\n",
      "Validation score: 0.964013\n",
      "Iteration 32, loss = 0.06386503\n",
      "Validation score: 0.962795\n",
      "Iteration 33, loss = 0.06286582\n",
      "Validation score: 0.963459\n",
      "Iteration 34, loss = 0.06214630\n",
      "Validation score: 0.964677\n",
      "Iteration 35, loss = 0.06110608\n",
      "Validation score: 0.963127\n",
      "Iteration 36, loss = 0.06001769\n",
      "Validation score: 0.964124\n",
      "Iteration 37, loss = 0.05929936\n",
      "Validation score: 0.966227\n",
      "Iteration 38, loss = 0.05839854\n",
      "Validation score: 0.965342\n",
      "Iteration 39, loss = 0.05751558\n",
      "Validation score: 0.964677\n",
      "Iteration 40, loss = 0.05675201\n",
      "Validation score: 0.966449\n",
      "Iteration 41, loss = 0.05584279\n",
      "Validation score: 0.966117\n",
      "Iteration 42, loss = 0.05510779\n",
      "Validation score: 0.964788\n",
      "Iteration 43, loss = 0.05425340\n",
      "Validation score: 0.966227\n",
      "Iteration 44, loss = 0.05348331\n",
      "Validation score: 0.965452\n",
      "Iteration 45, loss = 0.05257340\n",
      "Validation score: 0.964456\n",
      "Iteration 46, loss = 0.05210802\n",
      "Validation score: 0.965452\n",
      "Iteration 47, loss = 0.05120845\n",
      "Validation score: 0.966227\n",
      "Iteration 48, loss = 0.05054680\n",
      "Validation score: 0.966560\n",
      "Iteration 49, loss = 0.04977460\n",
      "Validation score: 0.966006\n",
      "Iteration 50, loss = 0.04938468\n",
      "Validation score: 0.965895\n",
      "Iteration 51, loss = 0.04843699\n",
      "Validation score: 0.964566\n",
      "Iteration 52, loss = 0.04776889\n",
      "Validation score: 0.964566\n",
      "Iteration 53, loss = 0.04717368\n",
      "Validation score: 0.964899\n",
      "Iteration 54, loss = 0.04656001\n",
      "Validation score: 0.965452\n",
      "Iteration 55, loss = 0.04587249\n",
      "Validation score: 0.966227\n",
      "Iteration 56, loss = 0.04522641\n",
      "Validation score: 0.965785\n",
      "Iteration 57, loss = 0.04455341\n",
      "Validation score: 0.966227\n",
      "Iteration 58, loss = 0.04378840\n",
      "Validation score: 0.966006\n",
      "Iteration 59, loss = 0.04335892\n",
      "Validation score: 0.966006\n",
      "Iteration 60, loss = 0.04255372\n",
      "Validation score: 0.965563\n",
      "Iteration 61, loss = 0.04206375\n",
      "Validation score: 0.965785\n",
      "Iteration 62, loss = 0.04154978\n",
      "Validation score: 0.966781\n",
      "Iteration 63, loss = 0.04103773\n",
      "Validation score: 0.965009\n",
      "Iteration 64, loss = 0.04039078\n",
      "Validation score: 0.965674\n",
      "Iteration 65, loss = 0.03988411\n",
      "Validation score: 0.965120\n",
      "Iteration 66, loss = 0.03943331\n",
      "Validation score: 0.965231\n",
      "Iteration 67, loss = 0.03871571\n",
      "Validation score: 0.966006\n",
      "Iteration 68, loss = 0.03809703\n",
      "Validation score: 0.965895\n",
      "Iteration 69, loss = 0.03780017\n",
      "Validation score: 0.966117\n",
      "Iteration 70, loss = 0.03720426\n",
      "Validation score: 0.965563\n",
      "Iteration 71, loss = 0.03664274\n",
      "Validation score: 0.966227\n",
      "Iteration 72, loss = 0.03613127\n",
      "Validation score: 0.966227\n",
      "Iteration 73, loss = 0.03569973\n",
      "Validation score: 0.965563\n",
      "Iteration 74, loss = 0.03513621\n",
      "Validation score: 0.966006\n",
      "Iteration 75, loss = 0.03459646\n",
      "Validation score: 0.966338\n",
      "Iteration 76, loss = 0.03422311\n",
      "Validation score: 0.965895\n",
      "Iteration 77, loss = 0.03368727\n",
      "Validation score: 0.965674\n",
      "Iteration 78, loss = 0.03325123\n",
      "Validation score: 0.966670\n",
      "Iteration 79, loss = 0.03279351\n",
      "Validation score: 0.966449\n",
      "Iteration 80, loss = 0.03229693\n",
      "Validation score: 0.966006\n",
      "Iteration 81, loss = 0.03181269\n",
      "Validation score: 0.967335\n",
      "Iteration 82, loss = 0.03152023\n",
      "Validation score: 0.965674\n",
      "Iteration 83, loss = 0.03092368\n",
      "Validation score: 0.966892\n",
      "Iteration 84, loss = 0.03059591\n",
      "Validation score: 0.966892\n",
      "Iteration 85, loss = 0.03004870\n",
      "Validation score: 0.966117\n",
      "Iteration 86, loss = 0.02968686\n",
      "Validation score: 0.966117\n",
      "Iteration 87, loss = 0.02931217\n",
      "Validation score: 0.965895\n",
      "Iteration 88, loss = 0.02889252\n",
      "Validation score: 0.967003\n",
      "Iteration 89, loss = 0.02860383\n",
      "Validation score: 0.966670\n",
      "Iteration 90, loss = 0.02805570\n",
      "Validation score: 0.966560\n",
      "Iteration 91, loss = 0.02768300\n",
      "Validation score: 0.966117\n",
      "Iteration 92, loss = 0.02730458\n",
      "Validation score: 0.965674\n",
      "Iteration 93, loss = 0.02691523\n",
      "Validation score: 0.966560\n",
      "Iteration 94, loss = 0.02659045\n",
      "Validation score: 0.965895\n",
      "Iteration 95, loss = 0.02608841\n",
      "Validation score: 0.966560\n",
      "Iteration 96, loss = 0.02583027\n",
      "Validation score: 0.966892\n",
      "Iteration 97, loss = 0.02549017\n",
      "Validation score: 0.966006\n",
      "Iteration 98, loss = 0.02505438\n",
      "Validation score: 0.966117\n",
      "Iteration 99, loss = 0.02476815\n",
      "Validation score: 0.967003\n",
      "Iteration 100, loss = 0.02443849\n",
      "Validation score: 0.966560\n",
      "Iteration 101, loss = 0.02401296\n",
      "Validation score: 0.966560\n",
      "Iteration 102, loss = 0.02367526\n",
      "Validation score: 0.966670\n",
      "Iteration 103, loss = 0.02337639\n",
      "Validation score: 0.966227\n",
      "Iteration 104, loss = 0.02314822\n",
      "Validation score: 0.966227\n",
      "Iteration 105, loss = 0.02264408\n",
      "Validation score: 0.965120\n",
      "Iteration 106, loss = 0.02230172\n",
      "Validation score: 0.966449\n",
      "Iteration 107, loss = 0.02203878\n",
      "Validation score: 0.965674\n",
      "Validation score did not improve more than tol=0.000100 for 25 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.27844601\n",
      "Validation score: 0.935777\n",
      "Iteration 2, loss = 0.14711295\n",
      "Validation score: 0.945189\n",
      "Iteration 3, loss = 0.12507834\n",
      "Validation score: 0.952054\n",
      "Iteration 4, loss = 0.11436227\n",
      "Validation score: 0.953936\n",
      "Iteration 5, loss = 0.10747929\n",
      "Validation score: 0.957258\n",
      "Iteration 6, loss = 0.10271129\n",
      "Validation score: 0.957369\n",
      "Iteration 7, loss = 0.09909476\n",
      "Validation score: 0.957812\n",
      "Iteration 8, loss = 0.09589160\n",
      "Validation score: 0.958698\n",
      "Iteration 9, loss = 0.09322234\n",
      "Validation score: 0.958698\n",
      "Iteration 10, loss = 0.09089503\n",
      "Validation score: 0.960359\n",
      "Iteration 11, loss = 0.08872514\n",
      "Validation score: 0.962020\n",
      "Iteration 12, loss = 0.08667445\n",
      "Validation score: 0.960802\n",
      "Iteration 13, loss = 0.08491685\n",
      "Validation score: 0.960027\n",
      "Iteration 14, loss = 0.08334510\n",
      "Validation score: 0.961134\n",
      "Iteration 15, loss = 0.08169494\n",
      "Validation score: 0.961245\n",
      "Iteration 16, loss = 0.08011041\n",
      "Validation score: 0.961245\n",
      "Iteration 17, loss = 0.07864070\n",
      "Validation score: 0.962352\n",
      "Iteration 18, loss = 0.07730428\n",
      "Validation score: 0.962130\n",
      "Iteration 19, loss = 0.07584994\n",
      "Validation score: 0.961798\n",
      "Iteration 20, loss = 0.07448736\n",
      "Validation score: 0.962352\n",
      "Iteration 21, loss = 0.07340392\n",
      "Validation score: 0.963016\n",
      "Iteration 22, loss = 0.07195809\n",
      "Validation score: 0.962130\n",
      "Iteration 23, loss = 0.07102594\n",
      "Validation score: 0.962795\n",
      "Iteration 24, loss = 0.06970271\n",
      "Validation score: 0.963348\n",
      "Iteration 25, loss = 0.06861713\n",
      "Validation score: 0.964013\n",
      "Iteration 26, loss = 0.06747497\n",
      "Validation score: 0.963902\n",
      "Iteration 27, loss = 0.06633351\n",
      "Validation score: 0.964456\n",
      "Iteration 28, loss = 0.06551901\n",
      "Validation score: 0.963570\n",
      "Iteration 29, loss = 0.06432245\n",
      "Validation score: 0.963681\n",
      "Iteration 30, loss = 0.06336424\n",
      "Validation score: 0.963791\n",
      "Iteration 31, loss = 0.06242121\n",
      "Validation score: 0.964124\n",
      "Iteration 32, loss = 0.06136116\n",
      "Validation score: 0.964788\n",
      "Iteration 33, loss = 0.06047750\n",
      "Validation score: 0.964677\n",
      "Iteration 34, loss = 0.05958178\n",
      "Validation score: 0.963127\n",
      "Iteration 35, loss = 0.05873434\n",
      "Validation score: 0.964234\n",
      "Iteration 36, loss = 0.05768861\n",
      "Validation score: 0.963570\n",
      "Iteration 37, loss = 0.05699430\n",
      "Validation score: 0.964788\n",
      "Iteration 38, loss = 0.05611694\n",
      "Validation score: 0.965342\n",
      "Iteration 39, loss = 0.05516871\n",
      "Validation score: 0.964345\n",
      "Iteration 40, loss = 0.05436176\n",
      "Validation score: 0.964345\n",
      "Iteration 41, loss = 0.05356147\n",
      "Validation score: 0.964124\n",
      "Iteration 42, loss = 0.05287316\n",
      "Validation score: 0.965120\n",
      "Iteration 43, loss = 0.05201574\n",
      "Validation score: 0.963127\n",
      "Iteration 44, loss = 0.05130625\n",
      "Validation score: 0.966338\n",
      "Iteration 45, loss = 0.05055063\n",
      "Validation score: 0.965120\n",
      "Iteration 46, loss = 0.04983236\n",
      "Validation score: 0.964788\n",
      "Iteration 47, loss = 0.04905835\n",
      "Validation score: 0.965785\n",
      "Iteration 48, loss = 0.04841331\n",
      "Validation score: 0.964456\n",
      "Iteration 49, loss = 0.04766868\n",
      "Validation score: 0.965674\n",
      "Iteration 50, loss = 0.04701711\n",
      "Validation score: 0.965452\n",
      "Iteration 51, loss = 0.04619477\n",
      "Validation score: 0.963902\n",
      "Iteration 52, loss = 0.04573020\n",
      "Validation score: 0.964899\n",
      "Iteration 53, loss = 0.04484263\n",
      "Validation score: 0.964456\n",
      "Iteration 54, loss = 0.04447584\n",
      "Validation score: 0.964234\n",
      "Iteration 55, loss = 0.04363067\n",
      "Validation score: 0.965342\n",
      "Iteration 56, loss = 0.04305654\n",
      "Validation score: 0.965120\n",
      "Iteration 57, loss = 0.04255863\n",
      "Validation score: 0.964456\n",
      "Iteration 58, loss = 0.04193380\n",
      "Validation score: 0.965009\n",
      "Iteration 59, loss = 0.04135478\n",
      "Validation score: 0.963791\n",
      "Iteration 60, loss = 0.04086521\n",
      "Validation score: 0.965009\n",
      "Iteration 61, loss = 0.04011604\n",
      "Validation score: 0.965120\n",
      "Iteration 62, loss = 0.03957856\n",
      "Validation score: 0.964899\n",
      "Iteration 63, loss = 0.03909858\n",
      "Validation score: 0.965785\n",
      "Iteration 64, loss = 0.03833371\n",
      "Validation score: 0.964566\n",
      "Iteration 65, loss = 0.03808807\n",
      "Validation score: 0.963791\n",
      "Iteration 66, loss = 0.03739371\n",
      "Validation score: 0.963681\n",
      "Iteration 67, loss = 0.03695548\n",
      "Validation score: 0.964234\n",
      "Iteration 68, loss = 0.03636971\n",
      "Validation score: 0.964124\n",
      "Iteration 69, loss = 0.03601954\n",
      "Validation score: 0.964899\n",
      "Iteration 70, loss = 0.03543212\n",
      "Validation score: 0.963570\n",
      "Validation score did not improve more than tol=0.000100 for 25 consecutive epochs. Stopping.\n",
      "All 5 repeats done! \n",
      "\n",
      "Neural Network: undersampling at rate 0.4 from the most popular class:\n",
      "Iteration 1, loss = 0.26315272\n",
      "Validation score: 0.942170\n",
      "Iteration 2, loss = 0.12836809\n",
      "Validation score: 0.949841\n",
      "Iteration 3, loss = 0.11052801\n",
      "Validation score: 0.954408\n",
      "Iteration 4, loss = 0.10198461\n",
      "Validation score: 0.957080\n",
      "Iteration 5, loss = 0.09640424\n",
      "Validation score: 0.958459\n",
      "Iteration 6, loss = 0.09236989\n",
      "Validation score: 0.959407\n",
      "Iteration 7, loss = 0.08914355\n",
      "Validation score: 0.959493\n",
      "Iteration 8, loss = 0.08640850\n",
      "Validation score: 0.960527\n",
      "Iteration 9, loss = 0.08389653\n",
      "Validation score: 0.960614\n",
      "Iteration 10, loss = 0.08186882\n",
      "Validation score: 0.961389\n",
      "Iteration 11, loss = 0.07987678\n",
      "Validation score: 0.961475\n",
      "Iteration 12, loss = 0.07814927\n",
      "Validation score: 0.961389\n",
      "Iteration 13, loss = 0.07646139\n",
      "Validation score: 0.961734\n",
      "Iteration 14, loss = 0.07490206\n",
      "Validation score: 0.961993\n",
      "Iteration 15, loss = 0.07335213\n",
      "Validation score: 0.963113\n",
      "Iteration 16, loss = 0.07191543\n",
      "Validation score: 0.962768\n",
      "Iteration 17, loss = 0.07069862\n",
      "Validation score: 0.964061\n",
      "Iteration 18, loss = 0.06925096\n",
      "Validation score: 0.963975\n",
      "Iteration 19, loss = 0.06809745\n",
      "Validation score: 0.963285\n",
      "Iteration 20, loss = 0.06685357\n",
      "Validation score: 0.963802\n",
      "Iteration 21, loss = 0.06549678\n",
      "Validation score: 0.963458\n",
      "Iteration 22, loss = 0.06447095\n",
      "Validation score: 0.963199\n",
      "Iteration 23, loss = 0.06356553\n",
      "Validation score: 0.964406\n",
      "Iteration 24, loss = 0.06216881\n",
      "Validation score: 0.963802\n",
      "Iteration 25, loss = 0.06116889\n",
      "Validation score: 0.965699\n",
      "Iteration 26, loss = 0.06007843\n",
      "Validation score: 0.964664\n",
      "Iteration 27, loss = 0.05929012\n",
      "Validation score: 0.964664\n",
      "Iteration 28, loss = 0.05830729\n",
      "Validation score: 0.964492\n",
      "Iteration 29, loss = 0.05726054\n",
      "Validation score: 0.964492\n",
      "Iteration 30, loss = 0.05637941\n",
      "Validation score: 0.964837\n",
      "Iteration 31, loss = 0.05530293\n",
      "Validation score: 0.964837\n",
      "Iteration 32, loss = 0.05466193\n",
      "Validation score: 0.964233\n",
      "Iteration 33, loss = 0.05372509\n",
      "Validation score: 0.964750\n",
      "Iteration 34, loss = 0.05298477\n",
      "Validation score: 0.966388\n",
      "Iteration 35, loss = 0.05197649\n",
      "Validation score: 0.965957\n",
      "Iteration 36, loss = 0.05138006\n",
      "Validation score: 0.964664\n",
      "Iteration 37, loss = 0.05061708\n",
      "Validation score: 0.965957\n",
      "Iteration 38, loss = 0.04969748\n",
      "Validation score: 0.965785\n",
      "Iteration 39, loss = 0.04895288\n",
      "Validation score: 0.964837\n",
      "Iteration 40, loss = 0.04823627\n",
      "Validation score: 0.965009\n",
      "Iteration 41, loss = 0.04761463\n",
      "Validation score: 0.966129\n",
      "Iteration 42, loss = 0.04691208\n",
      "Validation score: 0.967077\n",
      "Iteration 43, loss = 0.04619543\n",
      "Validation score: 0.965440\n",
      "Iteration 44, loss = 0.04538850\n",
      "Validation score: 0.967077\n",
      "Iteration 45, loss = 0.04475522\n",
      "Validation score: 0.966043\n",
      "Iteration 46, loss = 0.04402303\n",
      "Validation score: 0.966474\n",
      "Iteration 47, loss = 0.04344269\n",
      "Validation score: 0.965009\n",
      "Iteration 48, loss = 0.04277464\n",
      "Validation score: 0.966302\n",
      "Iteration 49, loss = 0.04233262\n",
      "Validation score: 0.965699\n",
      "Iteration 50, loss = 0.04179841\n",
      "Validation score: 0.966474\n",
      "Iteration 51, loss = 0.04102894\n",
      "Validation score: 0.966388\n",
      "Iteration 52, loss = 0.04045598\n",
      "Validation score: 0.965785\n",
      "Iteration 53, loss = 0.03990645\n",
      "Validation score: 0.966733\n",
      "Iteration 54, loss = 0.03923750\n",
      "Validation score: 0.966647\n",
      "Iteration 55, loss = 0.03885406\n",
      "Validation score: 0.966043\n",
      "Iteration 56, loss = 0.03833065\n",
      "Validation score: 0.966905\n",
      "Iteration 57, loss = 0.03771849\n",
      "Validation score: 0.964837\n",
      "Iteration 58, loss = 0.03727380\n",
      "Validation score: 0.966474\n",
      "Iteration 59, loss = 0.03663244\n",
      "Validation score: 0.965785\n",
      "Iteration 60, loss = 0.03616386\n",
      "Validation score: 0.966388\n",
      "Iteration 61, loss = 0.03547563\n",
      "Validation score: 0.965785\n",
      "Iteration 62, loss = 0.03509900\n",
      "Validation score: 0.966819\n",
      "Iteration 63, loss = 0.03456096\n",
      "Validation score: 0.966991\n",
      "Iteration 64, loss = 0.03408122\n",
      "Validation score: 0.965526\n",
      "Iteration 65, loss = 0.03380201\n",
      "Validation score: 0.966043\n",
      "Iteration 66, loss = 0.03307283\n",
      "Validation score: 0.965871\n",
      "Iteration 67, loss = 0.03273879\n",
      "Validation score: 0.966560\n",
      "Iteration 68, loss = 0.03215347\n",
      "Validation score: 0.965009\n",
      "Validation score did not improve more than tol=0.000100 for 25 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.27043678\n",
      "Validation score: 0.941567\n",
      "Iteration 2, loss = 0.12896046\n",
      "Validation score: 0.951823\n",
      "Iteration 3, loss = 0.11024986\n",
      "Validation score: 0.954581\n",
      "Iteration 4, loss = 0.10112777\n",
      "Validation score: 0.957080\n",
      "Iteration 5, loss = 0.09528257\n",
      "Validation score: 0.958890\n",
      "Iteration 6, loss = 0.09115914\n",
      "Validation score: 0.960355\n",
      "Iteration 7, loss = 0.08793714\n",
      "Validation score: 0.961906\n",
      "Iteration 8, loss = 0.08526538\n",
      "Validation score: 0.961734\n",
      "Iteration 9, loss = 0.08265812\n",
      "Validation score: 0.963113\n",
      "Iteration 10, loss = 0.08053678\n",
      "Validation score: 0.964061\n",
      "Iteration 11, loss = 0.07858911\n",
      "Validation score: 0.963802\n",
      "Iteration 12, loss = 0.07671034\n",
      "Validation score: 0.964147\n",
      "Iteration 13, loss = 0.07508258\n",
      "Validation score: 0.965354\n",
      "Iteration 14, loss = 0.07359853\n",
      "Validation score: 0.965699\n",
      "Iteration 15, loss = 0.07192510\n",
      "Validation score: 0.965612\n",
      "Iteration 16, loss = 0.07057329\n",
      "Validation score: 0.966474\n",
      "Iteration 17, loss = 0.06914651\n",
      "Validation score: 0.966043\n",
      "Iteration 18, loss = 0.06782068\n",
      "Validation score: 0.966388\n",
      "Iteration 19, loss = 0.06671374\n",
      "Validation score: 0.966991\n",
      "Iteration 20, loss = 0.06534532\n",
      "Validation score: 0.965871\n",
      "Iteration 21, loss = 0.06430093\n",
      "Validation score: 0.967077\n",
      "Iteration 22, loss = 0.06311188\n",
      "Validation score: 0.966043\n",
      "Iteration 23, loss = 0.06195887\n",
      "Validation score: 0.967336\n",
      "Iteration 24, loss = 0.06102652\n",
      "Validation score: 0.967164\n",
      "Iteration 25, loss = 0.06005542\n",
      "Validation score: 0.967422\n",
      "Iteration 26, loss = 0.05894251\n",
      "Validation score: 0.967508\n",
      "Iteration 27, loss = 0.05808168\n",
      "Validation score: 0.968026\n",
      "Iteration 28, loss = 0.05706101\n",
      "Validation score: 0.968198\n",
      "Iteration 29, loss = 0.05608467\n",
      "Validation score: 0.968198\n",
      "Iteration 30, loss = 0.05533700\n",
      "Validation score: 0.967767\n",
      "Iteration 31, loss = 0.05434694\n",
      "Validation score: 0.968284\n",
      "Iteration 32, loss = 0.05345644\n",
      "Validation score: 0.967767\n",
      "Iteration 33, loss = 0.05275381\n",
      "Validation score: 0.968887\n",
      "Iteration 34, loss = 0.05190010\n",
      "Validation score: 0.968284\n",
      "Iteration 35, loss = 0.05105158\n",
      "Validation score: 0.968456\n",
      "Iteration 36, loss = 0.05033993\n",
      "Validation score: 0.968026\n",
      "Iteration 37, loss = 0.04955174\n",
      "Validation score: 0.968715\n",
      "Iteration 38, loss = 0.04883710\n",
      "Validation score: 0.968543\n",
      "Iteration 39, loss = 0.04805509\n",
      "Validation score: 0.968715\n",
      "Iteration 40, loss = 0.04730485\n",
      "Validation score: 0.968284\n",
      "Iteration 41, loss = 0.04651129\n",
      "Validation score: 0.967939\n",
      "Iteration 42, loss = 0.04592594\n",
      "Validation score: 0.969404\n",
      "Iteration 43, loss = 0.04529634\n",
      "Validation score: 0.968370\n",
      "Iteration 44, loss = 0.04455309\n",
      "Validation score: 0.969318\n",
      "Iteration 45, loss = 0.04391894\n",
      "Validation score: 0.969146\n",
      "Iteration 46, loss = 0.04330364\n",
      "Validation score: 0.967853\n",
      "Iteration 47, loss = 0.04266069\n",
      "Validation score: 0.968456\n",
      "Iteration 48, loss = 0.04213707\n",
      "Validation score: 0.967508\n",
      "Iteration 49, loss = 0.04148384\n",
      "Validation score: 0.968112\n",
      "Iteration 50, loss = 0.04077127\n",
      "Validation score: 0.967595\n",
      "Iteration 51, loss = 0.04032619\n",
      "Validation score: 0.968456\n",
      "Iteration 52, loss = 0.03970864\n",
      "Validation score: 0.968370\n",
      "Iteration 53, loss = 0.03911032\n",
      "Validation score: 0.969491\n",
      "Iteration 54, loss = 0.03851516\n",
      "Validation score: 0.969318\n",
      "Iteration 55, loss = 0.03800406\n",
      "Validation score: 0.968974\n",
      "Iteration 56, loss = 0.03762866\n",
      "Validation score: 0.969146\n",
      "Iteration 57, loss = 0.03695670\n",
      "Validation score: 0.968543\n",
      "Iteration 58, loss = 0.03652938\n",
      "Validation score: 0.968456\n",
      "Iteration 59, loss = 0.03589617\n",
      "Validation score: 0.968456\n",
      "Iteration 60, loss = 0.03533414\n",
      "Validation score: 0.969663\n",
      "Iteration 61, loss = 0.03498370\n",
      "Validation score: 0.968629\n",
      "Iteration 62, loss = 0.03434420\n",
      "Validation score: 0.968543\n",
      "Iteration 63, loss = 0.03392492\n",
      "Validation score: 0.968456\n",
      "Iteration 64, loss = 0.03347823\n",
      "Validation score: 0.968715\n",
      "Iteration 65, loss = 0.03312309\n",
      "Validation score: 0.968456\n",
      "Iteration 66, loss = 0.03261476\n",
      "Validation score: 0.968112\n",
      "Iteration 67, loss = 0.03205449\n",
      "Validation score: 0.968801\n",
      "Iteration 68, loss = 0.03154075\n",
      "Validation score: 0.969663\n",
      "Iteration 69, loss = 0.03128920\n",
      "Validation score: 0.968198\n",
      "Iteration 70, loss = 0.03085954\n",
      "Validation score: 0.968370\n",
      "Iteration 71, loss = 0.03038792\n",
      "Validation score: 0.968629\n",
      "Iteration 72, loss = 0.02993019\n",
      "Validation score: 0.968370\n",
      "Iteration 73, loss = 0.02944470\n",
      "Validation score: 0.968284\n",
      "Iteration 74, loss = 0.02904954\n",
      "Validation score: 0.968543\n",
      "Iteration 75, loss = 0.02872373\n",
      "Validation score: 0.969060\n",
      "Iteration 76, loss = 0.02817252\n",
      "Validation score: 0.968112\n",
      "Iteration 77, loss = 0.02776206\n",
      "Validation score: 0.969491\n",
      "Iteration 78, loss = 0.02746467\n",
      "Validation score: 0.968112\n",
      "Iteration 79, loss = 0.02707776\n",
      "Validation score: 0.967681\n",
      "Iteration 80, loss = 0.02668975\n",
      "Validation score: 0.967508\n",
      "Iteration 81, loss = 0.02639565\n",
      "Validation score: 0.968026\n",
      "Iteration 82, loss = 0.02602236\n",
      "Validation score: 0.968974\n",
      "Iteration 83, loss = 0.02560379\n",
      "Validation score: 0.968543\n",
      "Iteration 84, loss = 0.02519280\n",
      "Validation score: 0.968543\n",
      "Iteration 85, loss = 0.02491950\n",
      "Validation score: 0.968198\n",
      "Iteration 86, loss = 0.02466710\n",
      "Validation score: 0.967939\n",
      "Validation score did not improve more than tol=0.000100 for 25 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.26244439\n",
      "Validation score: 0.943377\n",
      "Iteration 2, loss = 0.12912121\n",
      "Validation score: 0.951737\n",
      "Iteration 3, loss = 0.10989739\n",
      "Validation score: 0.955012\n",
      "Iteration 4, loss = 0.10053192\n",
      "Validation score: 0.956908\n",
      "Iteration 5, loss = 0.09464493\n",
      "Validation score: 0.960097\n",
      "Iteration 6, loss = 0.09042127\n",
      "Validation score: 0.961303\n",
      "Iteration 7, loss = 0.08734443\n",
      "Validation score: 0.961820\n",
      "Iteration 8, loss = 0.08438905\n",
      "Validation score: 0.962251\n",
      "Iteration 9, loss = 0.08223957\n",
      "Validation score: 0.961906\n",
      "Iteration 10, loss = 0.08015838\n",
      "Validation score: 0.962510\n",
      "Iteration 11, loss = 0.07809696\n",
      "Validation score: 0.963975\n",
      "Iteration 12, loss = 0.07642662\n",
      "Validation score: 0.964320\n",
      "Iteration 13, loss = 0.07474935\n",
      "Validation score: 0.964664\n",
      "Iteration 14, loss = 0.07333515\n",
      "Validation score: 0.965181\n",
      "Iteration 15, loss = 0.07184820\n",
      "Validation score: 0.964406\n",
      "Iteration 16, loss = 0.07057612\n",
      "Validation score: 0.965181\n",
      "Iteration 17, loss = 0.06933481\n",
      "Validation score: 0.965526\n",
      "Iteration 18, loss = 0.06799694\n",
      "Validation score: 0.965181\n",
      "Iteration 19, loss = 0.06679581\n",
      "Validation score: 0.965957\n",
      "Iteration 20, loss = 0.06550140\n",
      "Validation score: 0.965957\n",
      "Iteration 21, loss = 0.06454079\n",
      "Validation score: 0.966388\n",
      "Iteration 22, loss = 0.06344369\n",
      "Validation score: 0.965354\n",
      "Iteration 23, loss = 0.06240778\n",
      "Validation score: 0.966043\n",
      "Iteration 24, loss = 0.06140660\n",
      "Validation score: 0.966043\n",
      "Iteration 25, loss = 0.06028189\n",
      "Validation score: 0.966819\n",
      "Iteration 26, loss = 0.05949060\n",
      "Validation score: 0.966388\n",
      "Iteration 27, loss = 0.05849794\n",
      "Validation score: 0.966302\n",
      "Iteration 28, loss = 0.05741935\n",
      "Validation score: 0.966302\n",
      "Iteration 29, loss = 0.05660194\n",
      "Validation score: 0.966819\n",
      "Iteration 30, loss = 0.05588308\n",
      "Validation score: 0.967077\n",
      "Iteration 31, loss = 0.05508977\n",
      "Validation score: 0.967336\n",
      "Iteration 32, loss = 0.05433109\n",
      "Validation score: 0.966388\n",
      "Iteration 33, loss = 0.05331345\n",
      "Validation score: 0.966991\n",
      "Iteration 34, loss = 0.05265790\n",
      "Validation score: 0.967595\n",
      "Iteration 35, loss = 0.05176131\n",
      "Validation score: 0.966905\n",
      "Iteration 36, loss = 0.05109859\n",
      "Validation score: 0.968974\n",
      "Iteration 37, loss = 0.05022693\n",
      "Validation score: 0.966733\n",
      "Iteration 38, loss = 0.04973186\n",
      "Validation score: 0.967681\n",
      "Iteration 39, loss = 0.04887388\n",
      "Validation score: 0.967336\n",
      "Iteration 40, loss = 0.04825394\n",
      "Validation score: 0.967250\n",
      "Iteration 41, loss = 0.04762782\n",
      "Validation score: 0.968629\n",
      "Iteration 42, loss = 0.04681737\n",
      "Validation score: 0.968456\n",
      "Iteration 43, loss = 0.04633719\n",
      "Validation score: 0.967164\n",
      "Iteration 44, loss = 0.04558068\n",
      "Validation score: 0.967422\n",
      "Iteration 45, loss = 0.04472981\n",
      "Validation score: 0.967422\n",
      "Iteration 46, loss = 0.04425715\n",
      "Validation score: 0.968456\n",
      "Iteration 47, loss = 0.04361577\n",
      "Validation score: 0.967164\n",
      "Iteration 48, loss = 0.04298695\n",
      "Validation score: 0.967164\n",
      "Iteration 49, loss = 0.04241438\n",
      "Validation score: 0.968629\n",
      "Iteration 50, loss = 0.04176328\n",
      "Validation score: 0.968370\n",
      "Iteration 51, loss = 0.04136761\n",
      "Validation score: 0.967853\n",
      "Iteration 52, loss = 0.04079596\n",
      "Validation score: 0.967939\n",
      "Iteration 53, loss = 0.04010901\n",
      "Validation score: 0.968284\n",
      "Iteration 54, loss = 0.03963534\n",
      "Validation score: 0.968112\n",
      "Iteration 55, loss = 0.03900311\n",
      "Validation score: 0.968887\n",
      "Iteration 56, loss = 0.03849560\n",
      "Validation score: 0.967422\n",
      "Iteration 57, loss = 0.03803357\n",
      "Validation score: 0.968629\n",
      "Iteration 58, loss = 0.03746358\n",
      "Validation score: 0.967939\n",
      "Iteration 59, loss = 0.03697106\n",
      "Validation score: 0.968370\n",
      "Iteration 60, loss = 0.03651637\n",
      "Validation score: 0.967939\n",
      "Iteration 61, loss = 0.03586988\n",
      "Validation score: 0.967767\n",
      "Iteration 62, loss = 0.03547108\n",
      "Validation score: 0.967767\n",
      "Validation score did not improve more than tol=0.000100 for 25 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.20226970\n",
      "Validation score: 0.944583\n",
      "Iteration 2, loss = 0.11960018\n",
      "Validation score: 0.952426\n",
      "Iteration 3, loss = 0.10438913\n",
      "Validation score: 0.956563\n",
      "Iteration 4, loss = 0.09631520\n",
      "Validation score: 0.958890\n",
      "Iteration 5, loss = 0.09111803\n",
      "Validation score: 0.959493\n",
      "Iteration 6, loss = 0.08742812\n",
      "Validation score: 0.960872\n",
      "Iteration 7, loss = 0.08432532\n",
      "Validation score: 0.961303\n",
      "Iteration 8, loss = 0.08175996\n",
      "Validation score: 0.963027\n",
      "Iteration 9, loss = 0.07956107\n",
      "Validation score: 0.963285\n",
      "Iteration 10, loss = 0.07730770\n",
      "Validation score: 0.963544\n",
      "Iteration 11, loss = 0.07560228\n",
      "Validation score: 0.963975\n",
      "Iteration 12, loss = 0.07384735\n",
      "Validation score: 0.964147\n",
      "Iteration 13, loss = 0.07228067\n",
      "Validation score: 0.964492\n",
      "Iteration 14, loss = 0.07081443\n",
      "Validation score: 0.965095\n",
      "Iteration 15, loss = 0.06941062\n",
      "Validation score: 0.964837\n",
      "Iteration 16, loss = 0.06797955\n",
      "Validation score: 0.964750\n",
      "Iteration 17, loss = 0.06669001\n",
      "Validation score: 0.965526\n",
      "Iteration 18, loss = 0.06544432\n",
      "Validation score: 0.965526\n",
      "Iteration 19, loss = 0.06422113\n",
      "Validation score: 0.965526\n",
      "Iteration 20, loss = 0.06310887\n",
      "Validation score: 0.967336\n",
      "Iteration 21, loss = 0.06209128\n",
      "Validation score: 0.966560\n",
      "Iteration 22, loss = 0.06098116\n",
      "Validation score: 0.966991\n",
      "Iteration 23, loss = 0.05996967\n",
      "Validation score: 0.968112\n",
      "Iteration 24, loss = 0.05896044\n",
      "Validation score: 0.966560\n",
      "Iteration 25, loss = 0.05791623\n",
      "Validation score: 0.966388\n",
      "Iteration 26, loss = 0.05691831\n",
      "Validation score: 0.966560\n",
      "Iteration 27, loss = 0.05618014\n",
      "Validation score: 0.967767\n",
      "Iteration 28, loss = 0.05514400\n",
      "Validation score: 0.967077\n",
      "Iteration 29, loss = 0.05421054\n",
      "Validation score: 0.967681\n",
      "Iteration 30, loss = 0.05341551\n",
      "Validation score: 0.967939\n",
      "Iteration 31, loss = 0.05264423\n",
      "Validation score: 0.966647\n",
      "Iteration 32, loss = 0.05169544\n",
      "Validation score: 0.966733\n",
      "Iteration 33, loss = 0.05099279\n",
      "Validation score: 0.967422\n",
      "Iteration 34, loss = 0.05020865\n",
      "Validation score: 0.967422\n",
      "Iteration 35, loss = 0.04946297\n",
      "Validation score: 0.967336\n",
      "Iteration 36, loss = 0.04861818\n",
      "Validation score: 0.967595\n",
      "Iteration 37, loss = 0.04804782\n",
      "Validation score: 0.967681\n",
      "Iteration 38, loss = 0.04734364\n",
      "Validation score: 0.967681\n",
      "Iteration 39, loss = 0.04655153\n",
      "Validation score: 0.968801\n",
      "Iteration 40, loss = 0.04590078\n",
      "Validation score: 0.967422\n",
      "Iteration 41, loss = 0.04520028\n",
      "Validation score: 0.967939\n",
      "Iteration 42, loss = 0.04452788\n",
      "Validation score: 0.967422\n",
      "Iteration 43, loss = 0.04392271\n",
      "Validation score: 0.967250\n",
      "Iteration 44, loss = 0.04330113\n",
      "Validation score: 0.968026\n",
      "Iteration 45, loss = 0.04252587\n",
      "Validation score: 0.967595\n",
      "Iteration 46, loss = 0.04197590\n",
      "Validation score: 0.968112\n",
      "Iteration 47, loss = 0.04143877\n",
      "Validation score: 0.967336\n",
      "Iteration 48, loss = 0.04080178\n",
      "Validation score: 0.967422\n",
      "Iteration 49, loss = 0.04018931\n",
      "Validation score: 0.968026\n",
      "Iteration 50, loss = 0.03963557\n",
      "Validation score: 0.967422\n",
      "Iteration 51, loss = 0.03912194\n",
      "Validation score: 0.967422\n",
      "Iteration 52, loss = 0.03858895\n",
      "Validation score: 0.968370\n",
      "Iteration 53, loss = 0.03792597\n",
      "Validation score: 0.967595\n",
      "Iteration 54, loss = 0.03749304\n",
      "Validation score: 0.967853\n",
      "Iteration 55, loss = 0.03693343\n",
      "Validation score: 0.968629\n",
      "Iteration 56, loss = 0.03655490\n",
      "Validation score: 0.967939\n",
      "Iteration 57, loss = 0.03573418\n",
      "Validation score: 0.968715\n",
      "Iteration 58, loss = 0.03536453\n",
      "Validation score: 0.968284\n",
      "Iteration 59, loss = 0.03491647\n",
      "Validation score: 0.968887\n",
      "Iteration 60, loss = 0.03442927\n",
      "Validation score: 0.968456\n",
      "Iteration 61, loss = 0.03399671\n",
      "Validation score: 0.968198\n",
      "Iteration 62, loss = 0.03342376\n",
      "Validation score: 0.968629\n",
      "Iteration 63, loss = 0.03286983\n",
      "Validation score: 0.968112\n",
      "Iteration 64, loss = 0.03240898\n",
      "Validation score: 0.966647\n",
      "Iteration 65, loss = 0.03207811\n",
      "Validation score: 0.967853\n",
      "Validation score did not improve more than tol=0.000100 for 25 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.24094108\n",
      "Validation score: 0.945618\n",
      "Iteration 2, loss = 0.12681272\n",
      "Validation score: 0.953374\n",
      "Iteration 3, loss = 0.10903364\n",
      "Validation score: 0.957080\n",
      "Iteration 4, loss = 0.10005923\n",
      "Validation score: 0.959062\n",
      "Iteration 5, loss = 0.09442426\n",
      "Validation score: 0.960872\n",
      "Iteration 6, loss = 0.09014729\n",
      "Validation score: 0.961820\n",
      "Iteration 7, loss = 0.08691790\n",
      "Validation score: 0.963630\n",
      "Iteration 8, loss = 0.08422755\n",
      "Validation score: 0.964750\n",
      "Iteration 9, loss = 0.08186254\n",
      "Validation score: 0.964578\n",
      "Iteration 10, loss = 0.07964356\n",
      "Validation score: 0.963889\n",
      "Iteration 11, loss = 0.07768939\n",
      "Validation score: 0.965009\n",
      "Iteration 12, loss = 0.07604116\n",
      "Validation score: 0.965699\n",
      "Iteration 13, loss = 0.07420912\n",
      "Validation score: 0.965268\n",
      "Iteration 14, loss = 0.07270641\n",
      "Validation score: 0.966216\n",
      "Iteration 15, loss = 0.07127738\n",
      "Validation score: 0.965871\n",
      "Iteration 16, loss = 0.06978061\n",
      "Validation score: 0.966216\n",
      "Iteration 17, loss = 0.06841227\n",
      "Validation score: 0.966647\n",
      "Iteration 18, loss = 0.06721185\n",
      "Validation score: 0.966302\n",
      "Iteration 19, loss = 0.06595872\n",
      "Validation score: 0.966905\n",
      "Iteration 20, loss = 0.06474813\n",
      "Validation score: 0.967250\n",
      "Iteration 21, loss = 0.06353672\n",
      "Validation score: 0.966991\n",
      "Iteration 22, loss = 0.06256017\n",
      "Validation score: 0.966905\n",
      "Iteration 23, loss = 0.06141304\n",
      "Validation score: 0.966388\n",
      "Iteration 24, loss = 0.06051626\n",
      "Validation score: 0.967767\n",
      "Iteration 25, loss = 0.05943015\n",
      "Validation score: 0.967508\n",
      "Iteration 26, loss = 0.05839896\n",
      "Validation score: 0.967336\n",
      "Iteration 27, loss = 0.05757967\n",
      "Validation score: 0.967164\n",
      "Iteration 28, loss = 0.05671695\n",
      "Validation score: 0.967250\n",
      "Iteration 29, loss = 0.05573259\n",
      "Validation score: 0.967595\n",
      "Iteration 30, loss = 0.05481817\n",
      "Validation score: 0.968370\n",
      "Iteration 31, loss = 0.05395070\n",
      "Validation score: 0.969146\n",
      "Iteration 32, loss = 0.05324607\n",
      "Validation score: 0.968284\n",
      "Iteration 33, loss = 0.05238038\n",
      "Validation score: 0.968801\n",
      "Iteration 34, loss = 0.05157408\n",
      "Validation score: 0.969060\n",
      "Iteration 35, loss = 0.05070212\n",
      "Validation score: 0.968974\n",
      "Iteration 36, loss = 0.04995348\n",
      "Validation score: 0.969146\n",
      "Iteration 37, loss = 0.04909675\n",
      "Validation score: 0.968026\n",
      "Iteration 38, loss = 0.04854658\n",
      "Validation score: 0.968198\n",
      "Iteration 39, loss = 0.04759597\n",
      "Validation score: 0.967853\n",
      "Iteration 40, loss = 0.04715272\n",
      "Validation score: 0.968456\n",
      "Iteration 41, loss = 0.04627535\n",
      "Validation score: 0.968887\n",
      "Iteration 42, loss = 0.04569338\n",
      "Validation score: 0.968715\n",
      "Iteration 43, loss = 0.04500924\n",
      "Validation score: 0.969060\n",
      "Iteration 44, loss = 0.04426275\n",
      "Validation score: 0.968370\n",
      "Iteration 45, loss = 0.04377649\n",
      "Validation score: 0.969232\n",
      "Iteration 46, loss = 0.04310118\n",
      "Validation score: 0.969404\n",
      "Iteration 47, loss = 0.04252393\n",
      "Validation score: 0.970180\n",
      "Iteration 48, loss = 0.04201960\n",
      "Validation score: 0.969060\n",
      "Iteration 49, loss = 0.04126362\n",
      "Validation score: 0.968801\n",
      "Iteration 50, loss = 0.04070910\n",
      "Validation score: 0.968284\n",
      "Iteration 51, loss = 0.04010444\n",
      "Validation score: 0.968026\n",
      "Iteration 52, loss = 0.03956009\n",
      "Validation score: 0.967939\n",
      "Iteration 53, loss = 0.03892083\n",
      "Validation score: 0.968629\n",
      "Iteration 54, loss = 0.03848337\n",
      "Validation score: 0.968543\n",
      "Iteration 55, loss = 0.03774611\n",
      "Validation score: 0.969404\n",
      "Iteration 56, loss = 0.03732219\n",
      "Validation score: 0.969146\n",
      "Iteration 57, loss = 0.03674598\n",
      "Validation score: 0.969232\n",
      "Iteration 58, loss = 0.03636010\n",
      "Validation score: 0.968801\n",
      "Iteration 59, loss = 0.03586252\n",
      "Validation score: 0.968026\n",
      "Iteration 60, loss = 0.03547233\n",
      "Validation score: 0.967336\n",
      "Iteration 61, loss = 0.03486969\n",
      "Validation score: 0.967508\n",
      "Iteration 62, loss = 0.03431877\n",
      "Validation score: 0.967508\n",
      "Iteration 63, loss = 0.03393851\n",
      "Validation score: 0.967595\n",
      "Iteration 64, loss = 0.03344321\n",
      "Validation score: 0.968112\n",
      "Iteration 65, loss = 0.03295307\n",
      "Validation score: 0.967939\n",
      "Iteration 66, loss = 0.03257459\n",
      "Validation score: 0.968801\n",
      "Iteration 67, loss = 0.03211105\n",
      "Validation score: 0.968026\n",
      "Iteration 68, loss = 0.03151381\n",
      "Validation score: 0.968715\n",
      "Iteration 69, loss = 0.03122235\n",
      "Validation score: 0.967767\n",
      "Iteration 70, loss = 0.03075051\n",
      "Validation score: 0.966819\n",
      "Iteration 71, loss = 0.03038786\n",
      "Validation score: 0.968198\n",
      "Iteration 72, loss = 0.03005077\n",
      "Validation score: 0.967939\n",
      "Iteration 73, loss = 0.02954107\n",
      "Validation score: 0.968112\n",
      "Validation score did not improve more than tol=0.000100 for 25 consecutive epochs. Stopping.\n",
      "All 5 repeats done! \n",
      "\n",
      "Neural Network: undersampling at rate 0.5 from the most popular class:\n",
      "Iteration 1, loss = 0.19454726\n",
      "Validation score: 0.948783\n",
      "Iteration 2, loss = 0.10783106\n",
      "Validation score: 0.958377\n",
      "Iteration 3, loss = 0.09380996\n",
      "Validation score: 0.961340\n",
      "Iteration 4, loss = 0.08674576\n",
      "Validation score: 0.965009\n",
      "Iteration 5, loss = 0.08202060\n",
      "Validation score: 0.964515\n",
      "Iteration 6, loss = 0.07844633\n",
      "Validation score: 0.966349\n",
      "Iteration 7, loss = 0.07567688\n",
      "Validation score: 0.965714\n",
      "Iteration 8, loss = 0.07323333\n",
      "Validation score: 0.965926\n",
      "Iteration 9, loss = 0.07115547\n",
      "Validation score: 0.967055\n",
      "Iteration 10, loss = 0.06930105\n",
      "Validation score: 0.967337\n",
      "Iteration 11, loss = 0.06750865\n",
      "Validation score: 0.966420\n",
      "Iteration 12, loss = 0.06598241\n",
      "Validation score: 0.966984\n",
      "Iteration 13, loss = 0.06432885\n",
      "Validation score: 0.967125\n",
      "Iteration 14, loss = 0.06312506\n",
      "Validation score: 0.967549\n",
      "Iteration 15, loss = 0.06172196\n",
      "Validation score: 0.968325\n",
      "Iteration 16, loss = 0.06052097\n",
      "Validation score: 0.967760\n",
      "Iteration 17, loss = 0.05937830\n",
      "Validation score: 0.968183\n",
      "Iteration 18, loss = 0.05820088\n",
      "Validation score: 0.968889\n",
      "Iteration 19, loss = 0.05706702\n",
      "Validation score: 0.968183\n",
      "Iteration 20, loss = 0.05602993\n",
      "Validation score: 0.968818\n",
      "Iteration 21, loss = 0.05514566\n",
      "Validation score: 0.968466\n",
      "Iteration 22, loss = 0.05418498\n",
      "Validation score: 0.969312\n",
      "Iteration 23, loss = 0.05319850\n",
      "Validation score: 0.968889\n",
      "Iteration 24, loss = 0.05237454\n",
      "Validation score: 0.969030\n",
      "Iteration 25, loss = 0.05146498\n",
      "Validation score: 0.969242\n",
      "Iteration 26, loss = 0.05069575\n",
      "Validation score: 0.969453\n",
      "Iteration 27, loss = 0.04988541\n",
      "Validation score: 0.968748\n",
      "Iteration 28, loss = 0.04891956\n",
      "Validation score: 0.969383\n",
      "Iteration 29, loss = 0.04820189\n",
      "Validation score: 0.970018\n",
      "Iteration 30, loss = 0.04757503\n",
      "Validation score: 0.969594\n",
      "Iteration 31, loss = 0.04673702\n",
      "Validation score: 0.968959\n",
      "Iteration 32, loss = 0.04619268\n",
      "Validation score: 0.969735\n",
      "Iteration 33, loss = 0.04534729\n",
      "Validation score: 0.969947\n",
      "Iteration 34, loss = 0.04459306\n",
      "Validation score: 0.970159\n",
      "Iteration 35, loss = 0.04398920\n",
      "Validation score: 0.969312\n",
      "Iteration 36, loss = 0.04327119\n",
      "Validation score: 0.969665\n",
      "Iteration 37, loss = 0.04276624\n",
      "Validation score: 0.971076\n",
      "Iteration 38, loss = 0.04202343\n",
      "Validation score: 0.970229\n",
      "Iteration 39, loss = 0.04134334\n",
      "Validation score: 0.970370\n",
      "Iteration 40, loss = 0.04073400\n",
      "Validation score: 0.969735\n",
      "Iteration 41, loss = 0.04022940\n",
      "Validation score: 0.970653\n",
      "Iteration 42, loss = 0.03959888\n",
      "Validation score: 0.969947\n",
      "Iteration 43, loss = 0.03893162\n",
      "Validation score: 0.969030\n",
      "Iteration 44, loss = 0.03854892\n",
      "Validation score: 0.970582\n",
      "Iteration 45, loss = 0.03793040\n",
      "Validation score: 0.970441\n",
      "Iteration 46, loss = 0.03731380\n",
      "Validation score: 0.970864\n",
      "Iteration 47, loss = 0.03686365\n",
      "Validation score: 0.970159\n",
      "Iteration 48, loss = 0.03626722\n",
      "Validation score: 0.971287\n",
      "Iteration 49, loss = 0.03583364\n",
      "Validation score: 0.971076\n",
      "Iteration 50, loss = 0.03519794\n",
      "Validation score: 0.970229\n",
      "Iteration 51, loss = 0.03476280\n",
      "Validation score: 0.970794\n",
      "Iteration 52, loss = 0.03434011\n",
      "Validation score: 0.970864\n",
      "Iteration 53, loss = 0.03377708\n",
      "Validation score: 0.971146\n",
      "Iteration 54, loss = 0.03333775\n",
      "Validation score: 0.970582\n",
      "Iteration 55, loss = 0.03282778\n",
      "Validation score: 0.970794\n",
      "Iteration 56, loss = 0.03235251\n",
      "Validation score: 0.970723\n",
      "Iteration 57, loss = 0.03189912\n",
      "Validation score: 0.970370\n",
      "Iteration 58, loss = 0.03142425\n",
      "Validation score: 0.971781\n",
      "Iteration 59, loss = 0.03104408\n",
      "Validation score: 0.971358\n",
      "Iteration 60, loss = 0.03054565\n",
      "Validation score: 0.970864\n",
      "Iteration 61, loss = 0.03005012\n",
      "Validation score: 0.970300\n",
      "Iteration 62, loss = 0.02960309\n",
      "Validation score: 0.971429\n",
      "Iteration 63, loss = 0.02927529\n",
      "Validation score: 0.971076\n",
      "Iteration 64, loss = 0.02886489\n",
      "Validation score: 0.971358\n",
      "Iteration 65, loss = 0.02839192\n",
      "Validation score: 0.971005\n",
      "Iteration 66, loss = 0.02812844\n",
      "Validation score: 0.971076\n",
      "Iteration 67, loss = 0.02749760\n",
      "Validation score: 0.970723\n",
      "Iteration 68, loss = 0.02719375\n",
      "Validation score: 0.971217\n",
      "Iteration 69, loss = 0.02682542\n",
      "Validation score: 0.971429\n",
      "Iteration 70, loss = 0.02642933\n",
      "Validation score: 0.971005\n",
      "Iteration 71, loss = 0.02603316\n",
      "Validation score: 0.970723\n",
      "Iteration 72, loss = 0.02573442\n",
      "Validation score: 0.969806\n",
      "Iteration 73, loss = 0.02533236\n",
      "Validation score: 0.969101\n",
      "Iteration 74, loss = 0.02493089\n",
      "Validation score: 0.970794\n",
      "Iteration 75, loss = 0.02461576\n",
      "Validation score: 0.971076\n",
      "Iteration 76, loss = 0.02417582\n",
      "Validation score: 0.971076\n",
      "Iteration 77, loss = 0.02382489\n",
      "Validation score: 0.969383\n",
      "Iteration 78, loss = 0.02361702\n",
      "Validation score: 0.971146\n",
      "Iteration 79, loss = 0.02320570\n",
      "Validation score: 0.970441\n",
      "Iteration 80, loss = 0.02290760\n",
      "Validation score: 0.970229\n",
      "Iteration 81, loss = 0.02260278\n",
      "Validation score: 0.970864\n",
      "Iteration 82, loss = 0.02218798\n",
      "Validation score: 0.970300\n",
      "Iteration 83, loss = 0.02186899\n",
      "Validation score: 0.970511\n",
      "Iteration 84, loss = 0.02161587\n",
      "Validation score: 0.970935\n",
      "Validation score did not improve more than tol=0.000100 for 25 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.21715201\n",
      "Validation score: 0.955556\n",
      "Iteration 2, loss = 0.10951280\n",
      "Validation score: 0.961481\n",
      "Iteration 3, loss = 0.09483288\n",
      "Validation score: 0.964021\n",
      "Iteration 4, loss = 0.08750649\n",
      "Validation score: 0.966138\n",
      "Iteration 5, loss = 0.08267035\n",
      "Validation score: 0.967337\n",
      "Iteration 6, loss = 0.07929541\n",
      "Validation score: 0.968113\n",
      "Iteration 7, loss = 0.07646021\n",
      "Validation score: 0.969242\n",
      "Iteration 8, loss = 0.07404339\n",
      "Validation score: 0.967901\n",
      "Iteration 9, loss = 0.07201173\n",
      "Validation score: 0.969101\n",
      "Iteration 10, loss = 0.06999091\n",
      "Validation score: 0.970582\n",
      "Iteration 11, loss = 0.06838818\n",
      "Validation score: 0.970511\n",
      "Iteration 12, loss = 0.06668415\n",
      "Validation score: 0.970229\n",
      "Iteration 13, loss = 0.06523596\n",
      "Validation score: 0.970159\n",
      "Iteration 14, loss = 0.06394771\n",
      "Validation score: 0.970370\n",
      "Iteration 15, loss = 0.06264125\n",
      "Validation score: 0.971287\n",
      "Iteration 16, loss = 0.06144492\n",
      "Validation score: 0.972346\n",
      "Iteration 17, loss = 0.06024163\n",
      "Validation score: 0.971076\n",
      "Iteration 18, loss = 0.05900679\n",
      "Validation score: 0.971711\n",
      "Iteration 19, loss = 0.05799922\n",
      "Validation score: 0.971358\n",
      "Iteration 20, loss = 0.05686355\n",
      "Validation score: 0.971922\n",
      "Iteration 21, loss = 0.05588305\n",
      "Validation score: 0.971499\n",
      "Iteration 22, loss = 0.05487645\n",
      "Validation score: 0.973122\n",
      "Iteration 23, loss = 0.05393718\n",
      "Validation score: 0.972910\n",
      "Iteration 24, loss = 0.05296646\n",
      "Validation score: 0.971781\n",
      "Iteration 25, loss = 0.05206112\n",
      "Validation score: 0.973122\n",
      "Iteration 26, loss = 0.05122014\n",
      "Validation score: 0.973333\n",
      "Iteration 27, loss = 0.05042838\n",
      "Validation score: 0.973545\n",
      "Iteration 28, loss = 0.04951413\n",
      "Validation score: 0.972840\n",
      "Iteration 29, loss = 0.04874266\n",
      "Validation score: 0.972910\n",
      "Iteration 30, loss = 0.04801754\n",
      "Validation score: 0.972698\n",
      "Iteration 31, loss = 0.04722006\n",
      "Validation score: 0.972628\n",
      "Iteration 32, loss = 0.04643848\n",
      "Validation score: 0.973333\n",
      "Iteration 33, loss = 0.04584260\n",
      "Validation score: 0.972840\n",
      "Iteration 34, loss = 0.04506255\n",
      "Validation score: 0.972628\n",
      "Iteration 35, loss = 0.04455044\n",
      "Validation score: 0.972557\n",
      "Iteration 36, loss = 0.04371511\n",
      "Validation score: 0.973122\n",
      "Iteration 37, loss = 0.04310570\n",
      "Validation score: 0.973404\n",
      "Iteration 38, loss = 0.04251699\n",
      "Validation score: 0.973122\n",
      "Iteration 39, loss = 0.04170996\n",
      "Validation score: 0.971922\n",
      "Iteration 40, loss = 0.04114730\n",
      "Validation score: 0.973122\n",
      "Iteration 41, loss = 0.04053567\n",
      "Validation score: 0.972769\n",
      "Iteration 42, loss = 0.03996155\n",
      "Validation score: 0.972769\n",
      "Iteration 43, loss = 0.03957386\n",
      "Validation score: 0.973545\n",
      "Iteration 44, loss = 0.03874977\n",
      "Validation score: 0.972981\n",
      "Iteration 45, loss = 0.03829693\n",
      "Validation score: 0.972416\n",
      "Iteration 46, loss = 0.03768187\n",
      "Validation score: 0.973757\n",
      "Iteration 47, loss = 0.03721990\n",
      "Validation score: 0.972557\n",
      "Iteration 48, loss = 0.03657603\n",
      "Validation score: 0.973616\n",
      "Iteration 49, loss = 0.03616399\n",
      "Validation score: 0.973333\n",
      "Iteration 50, loss = 0.03564998\n",
      "Validation score: 0.973122\n",
      "Iteration 51, loss = 0.03511570\n",
      "Validation score: 0.973263\n",
      "Iteration 52, loss = 0.03452969\n",
      "Validation score: 0.972698\n",
      "Iteration 53, loss = 0.03418260\n",
      "Validation score: 0.974039\n",
      "Iteration 54, loss = 0.03353496\n",
      "Validation score: 0.972981\n",
      "Iteration 55, loss = 0.03315057\n",
      "Validation score: 0.972981\n",
      "Iteration 56, loss = 0.03277645\n",
      "Validation score: 0.973404\n",
      "Iteration 57, loss = 0.03216008\n",
      "Validation score: 0.972769\n",
      "Iteration 58, loss = 0.03167041\n",
      "Validation score: 0.973827\n",
      "Iteration 59, loss = 0.03126374\n",
      "Validation score: 0.972910\n",
      "Iteration 60, loss = 0.03087198\n",
      "Validation score: 0.973757\n",
      "Iteration 61, loss = 0.03052927\n",
      "Validation score: 0.972840\n",
      "Iteration 62, loss = 0.02999823\n",
      "Validation score: 0.973122\n",
      "Iteration 63, loss = 0.02957449\n",
      "Validation score: 0.972628\n",
      "Iteration 64, loss = 0.02913952\n",
      "Validation score: 0.973616\n",
      "Iteration 65, loss = 0.02885968\n",
      "Validation score: 0.972487\n",
      "Iteration 66, loss = 0.02830515\n",
      "Validation score: 0.973192\n",
      "Iteration 67, loss = 0.02796933\n",
      "Validation score: 0.972346\n",
      "Iteration 68, loss = 0.02753015\n",
      "Validation score: 0.973051\n",
      "Iteration 69, loss = 0.02699639\n",
      "Validation score: 0.973474\n",
      "Iteration 70, loss = 0.02680313\n",
      "Validation score: 0.973474\n",
      "Iteration 71, loss = 0.02654469\n",
      "Validation score: 0.972205\n",
      "Iteration 72, loss = 0.02597085\n",
      "Validation score: 0.973616\n",
      "Iteration 73, loss = 0.02549443\n",
      "Validation score: 0.973968\n",
      "Iteration 74, loss = 0.02537160\n",
      "Validation score: 0.974180\n",
      "Iteration 75, loss = 0.02488618\n",
      "Validation score: 0.973686\n",
      "Iteration 76, loss = 0.02447058\n",
      "Validation score: 0.974180\n",
      "Iteration 77, loss = 0.02416890\n",
      "Validation score: 0.973263\n",
      "Iteration 78, loss = 0.02390349\n",
      "Validation score: 0.973968\n",
      "Iteration 79, loss = 0.02352564\n",
      "Validation score: 0.972698\n",
      "Iteration 80, loss = 0.02333501\n",
      "Validation score: 0.974603\n",
      "Iteration 81, loss = 0.02299562\n",
      "Validation score: 0.973968\n",
      "Iteration 82, loss = 0.02253147\n",
      "Validation score: 0.974250\n",
      "Iteration 83, loss = 0.02223625\n",
      "Validation score: 0.973898\n",
      "Iteration 84, loss = 0.02191771\n",
      "Validation score: 0.972769\n",
      "Iteration 85, loss = 0.02162391\n",
      "Validation score: 0.973333\n",
      "Iteration 86, loss = 0.02132995\n",
      "Validation score: 0.973263\n",
      "Iteration 87, loss = 0.02089324\n",
      "Validation score: 0.973192\n",
      "Iteration 88, loss = 0.02074744\n",
      "Validation score: 0.972698\n",
      "Iteration 89, loss = 0.02045221\n",
      "Validation score: 0.973686\n",
      "Iteration 90, loss = 0.02002620\n",
      "Validation score: 0.973404\n",
      "Iteration 91, loss = 0.01981687\n",
      "Validation score: 0.973051\n",
      "Iteration 92, loss = 0.01948618\n",
      "Validation score: 0.972981\n",
      "Iteration 93, loss = 0.01925127\n",
      "Validation score: 0.973263\n",
      "Iteration 94, loss = 0.01893344\n",
      "Validation score: 0.972557\n",
      "Iteration 95, loss = 0.01873570\n",
      "Validation score: 0.973404\n",
      "Iteration 96, loss = 0.01842118\n",
      "Validation score: 0.973474\n",
      "Iteration 97, loss = 0.01806863\n",
      "Validation score: 0.972346\n",
      "Iteration 98, loss = 0.01795550\n",
      "Validation score: 0.973827\n",
      "Iteration 99, loss = 0.01776600\n",
      "Validation score: 0.972981\n",
      "Iteration 100, loss = 0.01736280\n",
      "Validation score: 0.972840\n",
      "Iteration 101, loss = 0.01706308\n",
      "Validation score: 0.973333\n",
      "Iteration 102, loss = 0.01687608\n",
      "Validation score: 0.973192\n",
      "Iteration 103, loss = 0.01665214\n",
      "Validation score: 0.973122\n",
      "Iteration 104, loss = 0.01650904\n",
      "Validation score: 0.973616\n",
      "Iteration 105, loss = 0.01605260\n",
      "Validation score: 0.973333\n",
      "Iteration 106, loss = 0.01588569\n",
      "Validation score: 0.973122\n",
      "Validation score did not improve more than tol=0.000100 for 25 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.24947832\n",
      "Validation score: 0.949700\n",
      "Iteration 2, loss = 0.11353763\n",
      "Validation score: 0.957601\n",
      "Iteration 3, loss = 0.09772871\n",
      "Validation score: 0.960988\n",
      "Iteration 4, loss = 0.08998111\n",
      "Validation score: 0.962116\n",
      "Iteration 5, loss = 0.08513181\n",
      "Validation score: 0.963810\n",
      "Iteration 6, loss = 0.08154671\n",
      "Validation score: 0.965220\n",
      "Iteration 7, loss = 0.07864626\n",
      "Validation score: 0.965714\n",
      "Iteration 8, loss = 0.07634084\n",
      "Validation score: 0.965855\n",
      "Iteration 9, loss = 0.07420795\n",
      "Validation score: 0.966420\n",
      "Iteration 10, loss = 0.07228863\n",
      "Validation score: 0.967407\n",
      "Iteration 11, loss = 0.07041806\n",
      "Validation score: 0.967690\n",
      "Iteration 12, loss = 0.06894981\n",
      "Validation score: 0.968325\n",
      "Iteration 13, loss = 0.06742884\n",
      "Validation score: 0.968042\n",
      "Iteration 14, loss = 0.06603087\n",
      "Validation score: 0.968466\n",
      "Iteration 15, loss = 0.06462022\n",
      "Validation score: 0.969453\n",
      "Iteration 16, loss = 0.06338575\n",
      "Validation score: 0.969524\n",
      "Iteration 17, loss = 0.06205295\n",
      "Validation score: 0.969312\n",
      "Iteration 18, loss = 0.06080838\n",
      "Validation score: 0.969877\n",
      "Iteration 19, loss = 0.05962932\n",
      "Validation score: 0.970723\n",
      "Iteration 20, loss = 0.05860774\n",
      "Validation score: 0.971005\n",
      "Iteration 21, loss = 0.05768457\n",
      "Validation score: 0.970582\n",
      "Iteration 22, loss = 0.05662188\n",
      "Validation score: 0.970794\n",
      "Iteration 23, loss = 0.05565112\n",
      "Validation score: 0.971852\n",
      "Iteration 24, loss = 0.05467751\n",
      "Validation score: 0.970653\n",
      "Iteration 25, loss = 0.05369223\n",
      "Validation score: 0.971711\n",
      "Iteration 26, loss = 0.05298656\n",
      "Validation score: 0.971711\n",
      "Iteration 27, loss = 0.05202105\n",
      "Validation score: 0.971922\n",
      "Iteration 28, loss = 0.05122723\n",
      "Validation score: 0.971922\n",
      "Iteration 29, loss = 0.05033369\n",
      "Validation score: 0.972063\n",
      "Iteration 30, loss = 0.04959840\n",
      "Validation score: 0.971852\n",
      "Iteration 31, loss = 0.04873119\n",
      "Validation score: 0.972134\n",
      "Iteration 32, loss = 0.04800704\n",
      "Validation score: 0.972557\n",
      "Iteration 33, loss = 0.04729018\n",
      "Validation score: 0.971711\n",
      "Iteration 34, loss = 0.04656151\n",
      "Validation score: 0.971711\n",
      "Iteration 35, loss = 0.04584264\n",
      "Validation score: 0.972063\n",
      "Iteration 36, loss = 0.04496344\n",
      "Validation score: 0.972416\n",
      "Iteration 37, loss = 0.04439524\n",
      "Validation score: 0.972557\n",
      "Iteration 38, loss = 0.04374926\n",
      "Validation score: 0.973263\n",
      "Iteration 39, loss = 0.04303893\n",
      "Validation score: 0.971922\n",
      "Iteration 40, loss = 0.04261037\n",
      "Validation score: 0.973051\n",
      "Iteration 41, loss = 0.04173536\n",
      "Validation score: 0.973051\n",
      "Iteration 42, loss = 0.04117266\n",
      "Validation score: 0.971993\n",
      "Iteration 43, loss = 0.04065781\n",
      "Validation score: 0.972416\n",
      "Iteration 44, loss = 0.04004743\n",
      "Validation score: 0.973122\n",
      "Iteration 45, loss = 0.03938363\n",
      "Validation score: 0.972557\n",
      "Iteration 46, loss = 0.03874689\n",
      "Validation score: 0.972910\n",
      "Iteration 47, loss = 0.03828430\n",
      "Validation score: 0.972557\n",
      "Iteration 48, loss = 0.03764940\n",
      "Validation score: 0.972840\n",
      "Iteration 49, loss = 0.03716189\n",
      "Validation score: 0.972346\n",
      "Iteration 50, loss = 0.03659095\n",
      "Validation score: 0.972416\n",
      "Iteration 51, loss = 0.03603274\n",
      "Validation score: 0.972134\n",
      "Iteration 52, loss = 0.03555781\n",
      "Validation score: 0.972346\n",
      "Iteration 53, loss = 0.03499464\n",
      "Validation score: 0.972134\n",
      "Iteration 54, loss = 0.03460563\n",
      "Validation score: 0.972275\n",
      "Iteration 55, loss = 0.03409065\n",
      "Validation score: 0.972134\n",
      "Iteration 56, loss = 0.03347300\n",
      "Validation score: 0.972628\n",
      "Iteration 57, loss = 0.03305318\n",
      "Validation score: 0.972205\n",
      "Iteration 58, loss = 0.03272340\n",
      "Validation score: 0.972346\n",
      "Iteration 59, loss = 0.03235190\n",
      "Validation score: 0.971922\n",
      "Iteration 60, loss = 0.03161859\n",
      "Validation score: 0.972275\n",
      "Iteration 61, loss = 0.03129861\n",
      "Validation score: 0.972275\n",
      "Iteration 62, loss = 0.03084159\n",
      "Validation score: 0.971852\n",
      "Iteration 63, loss = 0.03044092\n",
      "Validation score: 0.972769\n",
      "Iteration 64, loss = 0.03001585\n",
      "Validation score: 0.972487\n",
      "Validation score did not improve more than tol=0.000100 for 25 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.18152688\n",
      "Validation score: 0.951675\n",
      "Iteration 2, loss = 0.10515199\n",
      "Validation score: 0.959506\n",
      "Iteration 3, loss = 0.09220404\n",
      "Validation score: 0.960776\n",
      "Iteration 4, loss = 0.08556803\n",
      "Validation score: 0.962540\n",
      "Iteration 5, loss = 0.08116139\n",
      "Validation score: 0.963880\n",
      "Iteration 6, loss = 0.07794343\n",
      "Validation score: 0.964727\n",
      "Iteration 7, loss = 0.07534707\n",
      "Validation score: 0.964444\n",
      "Iteration 8, loss = 0.07299743\n",
      "Validation score: 0.965009\n",
      "Iteration 9, loss = 0.07101108\n",
      "Validation score: 0.965926\n",
      "Iteration 10, loss = 0.06917104\n",
      "Validation score: 0.965714\n",
      "Iteration 11, loss = 0.06738741\n",
      "Validation score: 0.966420\n",
      "Iteration 12, loss = 0.06570956\n",
      "Validation score: 0.966349\n",
      "Iteration 13, loss = 0.06428292\n",
      "Validation score: 0.967690\n",
      "Iteration 14, loss = 0.06283756\n",
      "Validation score: 0.967760\n",
      "Iteration 15, loss = 0.06151722\n",
      "Validation score: 0.968677\n",
      "Iteration 16, loss = 0.06028248\n",
      "Validation score: 0.967760\n",
      "Iteration 17, loss = 0.05900840\n",
      "Validation score: 0.967549\n",
      "Iteration 18, loss = 0.05793980\n",
      "Validation score: 0.969171\n",
      "Iteration 19, loss = 0.05682177\n",
      "Validation score: 0.968325\n",
      "Iteration 20, loss = 0.05564882\n",
      "Validation score: 0.969383\n",
      "Iteration 21, loss = 0.05475725\n",
      "Validation score: 0.969524\n",
      "Iteration 22, loss = 0.05367998\n",
      "Validation score: 0.969453\n",
      "Iteration 23, loss = 0.05269599\n",
      "Validation score: 0.969030\n",
      "Iteration 24, loss = 0.05172602\n",
      "Validation score: 0.970159\n",
      "Iteration 25, loss = 0.05088270\n",
      "Validation score: 0.969735\n",
      "Iteration 26, loss = 0.05001924\n",
      "Validation score: 0.969806\n",
      "Iteration 27, loss = 0.04918362\n",
      "Validation score: 0.969594\n",
      "Iteration 28, loss = 0.04838623\n",
      "Validation score: 0.970159\n",
      "Iteration 29, loss = 0.04766039\n",
      "Validation score: 0.969947\n",
      "Iteration 30, loss = 0.04675810\n",
      "Validation score: 0.970300\n",
      "Iteration 31, loss = 0.04598557\n",
      "Validation score: 0.970723\n",
      "Iteration 32, loss = 0.04526806\n",
      "Validation score: 0.970582\n",
      "Iteration 33, loss = 0.04471889\n",
      "Validation score: 0.969453\n",
      "Iteration 34, loss = 0.04399586\n",
      "Validation score: 0.970723\n",
      "Iteration 35, loss = 0.04313455\n",
      "Validation score: 0.970441\n",
      "Iteration 36, loss = 0.04248993\n",
      "Validation score: 0.970864\n",
      "Iteration 37, loss = 0.04182375\n",
      "Validation score: 0.970300\n",
      "Iteration 38, loss = 0.04120416\n",
      "Validation score: 0.969947\n",
      "Iteration 39, loss = 0.04056300\n",
      "Validation score: 0.970229\n",
      "Iteration 40, loss = 0.03985524\n",
      "Validation score: 0.969524\n",
      "Iteration 41, loss = 0.03924862\n",
      "Validation score: 0.970511\n",
      "Iteration 42, loss = 0.03869553\n",
      "Validation score: 0.970018\n",
      "Iteration 43, loss = 0.03813058\n",
      "Validation score: 0.971499\n",
      "Iteration 44, loss = 0.03761771\n",
      "Validation score: 0.970159\n",
      "Iteration 45, loss = 0.03707584\n",
      "Validation score: 0.970935\n",
      "Iteration 46, loss = 0.03641562\n",
      "Validation score: 0.970935\n",
      "Iteration 47, loss = 0.03592076\n",
      "Validation score: 0.970018\n",
      "Iteration 48, loss = 0.03530993\n",
      "Validation score: 0.970935\n",
      "Iteration 49, loss = 0.03498935\n",
      "Validation score: 0.970511\n",
      "Iteration 50, loss = 0.03446261\n",
      "Validation score: 0.970229\n",
      "Iteration 51, loss = 0.03389047\n",
      "Validation score: 0.970370\n",
      "Iteration 52, loss = 0.03338202\n",
      "Validation score: 0.970300\n",
      "Iteration 53, loss = 0.03291569\n",
      "Validation score: 0.971146\n",
      "Iteration 54, loss = 0.03224899\n",
      "Validation score: 0.970864\n",
      "Iteration 55, loss = 0.03195054\n",
      "Validation score: 0.970370\n",
      "Iteration 56, loss = 0.03145241\n",
      "Validation score: 0.971005\n",
      "Iteration 57, loss = 0.03084410\n",
      "Validation score: 0.971146\n",
      "Iteration 58, loss = 0.03048714\n",
      "Validation score: 0.969806\n",
      "Iteration 59, loss = 0.03008148\n",
      "Validation score: 0.971005\n",
      "Iteration 60, loss = 0.02969929\n",
      "Validation score: 0.970159\n",
      "Iteration 61, loss = 0.02920927\n",
      "Validation score: 0.970723\n",
      "Iteration 62, loss = 0.02891977\n",
      "Validation score: 0.970653\n",
      "Iteration 63, loss = 0.02834053\n",
      "Validation score: 0.971076\n",
      "Iteration 64, loss = 0.02791001\n",
      "Validation score: 0.970582\n",
      "Iteration 65, loss = 0.02756661\n",
      "Validation score: 0.970794\n",
      "Iteration 66, loss = 0.02716874\n",
      "Validation score: 0.970511\n",
      "Iteration 67, loss = 0.02672964\n",
      "Validation score: 0.970582\n",
      "Iteration 68, loss = 0.02636886\n",
      "Validation score: 0.969665\n",
      "Iteration 69, loss = 0.02585876\n",
      "Validation score: 0.970370\n",
      "Validation score did not improve more than tol=0.000100 for 25 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.19572230\n",
      "Validation score: 0.951746\n",
      "Iteration 2, loss = 0.10925003\n",
      "Validation score: 0.959506\n",
      "Iteration 3, loss = 0.09512657\n",
      "Validation score: 0.963034\n",
      "Iteration 4, loss = 0.08796128\n",
      "Validation score: 0.963810\n",
      "Iteration 5, loss = 0.08317822\n",
      "Validation score: 0.964092\n",
      "Iteration 6, loss = 0.07954134\n",
      "Validation score: 0.965503\n",
      "Iteration 7, loss = 0.07680709\n",
      "Validation score: 0.966349\n",
      "Iteration 8, loss = 0.07429473\n",
      "Validation score: 0.966984\n",
      "Iteration 9, loss = 0.07230326\n",
      "Validation score: 0.966702\n",
      "Iteration 10, loss = 0.07026928\n",
      "Validation score: 0.967690\n",
      "Iteration 11, loss = 0.06859449\n",
      "Validation score: 0.968748\n",
      "Iteration 12, loss = 0.06690441\n",
      "Validation score: 0.969030\n",
      "Iteration 13, loss = 0.06549424\n",
      "Validation score: 0.969453\n",
      "Iteration 14, loss = 0.06404853\n",
      "Validation score: 0.969947\n",
      "Iteration 15, loss = 0.06271884\n",
      "Validation score: 0.970441\n",
      "Iteration 16, loss = 0.06156156\n",
      "Validation score: 0.970370\n",
      "Iteration 17, loss = 0.06025598\n",
      "Validation score: 0.971217\n",
      "Iteration 18, loss = 0.05907378\n",
      "Validation score: 0.971287\n",
      "Iteration 19, loss = 0.05795998\n",
      "Validation score: 0.971358\n",
      "Iteration 20, loss = 0.05696645\n",
      "Validation score: 0.971217\n",
      "Iteration 21, loss = 0.05585907\n",
      "Validation score: 0.971005\n",
      "Iteration 22, loss = 0.05497839\n",
      "Validation score: 0.971570\n",
      "Iteration 23, loss = 0.05394105\n",
      "Validation score: 0.971076\n",
      "Iteration 24, loss = 0.05299853\n",
      "Validation score: 0.972416\n",
      "Iteration 25, loss = 0.05215478\n",
      "Validation score: 0.971993\n",
      "Iteration 26, loss = 0.05116523\n",
      "Validation score: 0.971993\n",
      "Iteration 27, loss = 0.05043842\n",
      "Validation score: 0.971570\n",
      "Iteration 28, loss = 0.04941709\n",
      "Validation score: 0.972134\n",
      "Iteration 29, loss = 0.04878338\n",
      "Validation score: 0.972416\n",
      "Iteration 30, loss = 0.04798641\n",
      "Validation score: 0.971781\n",
      "Iteration 31, loss = 0.04705343\n",
      "Validation score: 0.972205\n",
      "Iteration 32, loss = 0.04631224\n",
      "Validation score: 0.971852\n",
      "Iteration 33, loss = 0.04556109\n",
      "Validation score: 0.971781\n",
      "Iteration 34, loss = 0.04507244\n",
      "Validation score: 0.972134\n",
      "Iteration 35, loss = 0.04430376\n",
      "Validation score: 0.972205\n",
      "Iteration 36, loss = 0.04347521\n",
      "Validation score: 0.972063\n",
      "Iteration 37, loss = 0.04287926\n",
      "Validation score: 0.972134\n",
      "Iteration 38, loss = 0.04225858\n",
      "Validation score: 0.972416\n",
      "Iteration 39, loss = 0.04161771\n",
      "Validation score: 0.972910\n",
      "Iteration 40, loss = 0.04089297\n",
      "Validation score: 0.972910\n",
      "Iteration 41, loss = 0.04031716\n",
      "Validation score: 0.972557\n",
      "Iteration 42, loss = 0.03974274\n",
      "Validation score: 0.972346\n",
      "Iteration 43, loss = 0.03909976\n",
      "Validation score: 0.972557\n",
      "Iteration 44, loss = 0.03853396\n",
      "Validation score: 0.972487\n",
      "Iteration 45, loss = 0.03802660\n",
      "Validation score: 0.972416\n",
      "Iteration 46, loss = 0.03747543\n",
      "Validation score: 0.971146\n",
      "Iteration 47, loss = 0.03690729\n",
      "Validation score: 0.972557\n",
      "Iteration 48, loss = 0.03623417\n",
      "Validation score: 0.972346\n",
      "Iteration 49, loss = 0.03577636\n",
      "Validation score: 0.972910\n",
      "Iteration 50, loss = 0.03520786\n",
      "Validation score: 0.972628\n",
      "Iteration 51, loss = 0.03483238\n",
      "Validation score: 0.973122\n",
      "Iteration 52, loss = 0.03429385\n",
      "Validation score: 0.972063\n",
      "Iteration 53, loss = 0.03376586\n",
      "Validation score: 0.971993\n",
      "Iteration 54, loss = 0.03329555\n",
      "Validation score: 0.972416\n",
      "Iteration 55, loss = 0.03283505\n",
      "Validation score: 0.973122\n",
      "Iteration 56, loss = 0.03242498\n",
      "Validation score: 0.973051\n",
      "Iteration 57, loss = 0.03190523\n",
      "Validation score: 0.973051\n",
      "Iteration 58, loss = 0.03159922\n",
      "Validation score: 0.973122\n",
      "Iteration 59, loss = 0.03104836\n",
      "Validation score: 0.973051\n",
      "Iteration 60, loss = 0.03045997\n",
      "Validation score: 0.973192\n",
      "Iteration 61, loss = 0.03014119\n",
      "Validation score: 0.972910\n",
      "Iteration 62, loss = 0.02972757\n",
      "Validation score: 0.973122\n",
      "Iteration 63, loss = 0.02939483\n",
      "Validation score: 0.972134\n",
      "Iteration 64, loss = 0.02882431\n",
      "Validation score: 0.972698\n",
      "Iteration 65, loss = 0.02851618\n",
      "Validation score: 0.972698\n",
      "Iteration 66, loss = 0.02809266\n",
      "Validation score: 0.972769\n",
      "Iteration 67, loss = 0.02781706\n",
      "Validation score: 0.972487\n",
      "Iteration 68, loss = 0.02727559\n",
      "Validation score: 0.973545\n",
      "Iteration 69, loss = 0.02692543\n",
      "Validation score: 0.973051\n",
      "Iteration 70, loss = 0.02654611\n",
      "Validation score: 0.971429\n",
      "Iteration 71, loss = 0.02616767\n",
      "Validation score: 0.972346\n",
      "Iteration 72, loss = 0.02578573\n",
      "Validation score: 0.972910\n",
      "Iteration 73, loss = 0.02536185\n",
      "Validation score: 0.972275\n",
      "Iteration 74, loss = 0.02523429\n",
      "Validation score: 0.972769\n",
      "Iteration 75, loss = 0.02470581\n",
      "Validation score: 0.972487\n",
      "Iteration 76, loss = 0.02437388\n",
      "Validation score: 0.971852\n",
      "Iteration 77, loss = 0.02404199\n",
      "Validation score: 0.972557\n",
      "Iteration 78, loss = 0.02377578\n",
      "Validation score: 0.971922\n",
      "Iteration 79, loss = 0.02337121\n",
      "Validation score: 0.971781\n",
      "Iteration 80, loss = 0.02308955\n",
      "Validation score: 0.971711\n",
      "Iteration 81, loss = 0.02265600\n",
      "Validation score: 0.972346\n",
      "Iteration 82, loss = 0.02246473\n",
      "Validation score: 0.971993\n",
      "Iteration 83, loss = 0.02213742\n",
      "Validation score: 0.972557\n",
      "Iteration 84, loss = 0.02174633\n",
      "Validation score: 0.972063\n",
      "Iteration 85, loss = 0.02157252\n",
      "Validation score: 0.972698\n",
      "Iteration 86, loss = 0.02118488\n",
      "Validation score: 0.972346\n",
      "Iteration 87, loss = 0.02087598\n",
      "Validation score: 0.972063\n",
      "Iteration 88, loss = 0.02071912\n",
      "Validation score: 0.971852\n",
      "Iteration 89, loss = 0.02028568\n",
      "Validation score: 0.972557\n",
      "Iteration 90, loss = 0.02003258\n",
      "Validation score: 0.972628\n",
      "Iteration 91, loss = 0.01965572\n",
      "Validation score: 0.971570\n",
      "Iteration 92, loss = 0.01937763\n",
      "Validation score: 0.972557\n",
      "Iteration 93, loss = 0.01929929\n",
      "Validation score: 0.972205\n",
      "Iteration 94, loss = 0.01887987\n",
      "Validation score: 0.972205\n",
      "Validation score did not improve more than tol=0.000100 for 25 consecutive epochs. Stopping.\n",
      "All 5 repeats done! \n",
      "\n",
      "Gradient Boosting 3000: \n",
      "Gradient Boosting 3000: undersampling at rate 0.1 from the most popular class:\n",
      "All 5 repeats done! \n",
      "\n",
      "Gradient Boosting 3000: undersampling at rate 0.2 from the most popular class:\n",
      "All 5 repeats done! \n",
      "\n",
      "Gradient Boosting 3000: undersampling at rate 0.3 from the most popular class:\n",
      "All 5 repeats done! \n",
      "\n",
      "Gradient Boosting 3000: undersampling at rate 0.4 from the most popular class:\n",
      "All 5 repeats done! \n",
      "\n",
      "Gradient Boosting 3000: undersampling at rate 0.5 from the most popular class:\n",
      "All 5 repeats done! \n",
      "\n",
      "Gradient Boosting 500: \n",
      "Gradient Boosting 500: undersampling at rate 0.1 from the most popular class:\n",
      "All 5 repeats done! \n",
      "\n",
      "Gradient Boosting 500: undersampling at rate 0.2 from the most popular class:\n",
      "All 5 repeats done! \n",
      "\n",
      "Gradient Boosting 500: undersampling at rate 0.3 from the most popular class:\n",
      "All 5 repeats done! \n",
      "\n",
      "Gradient Boosting 500: undersampling at rate 0.4 from the most popular class:\n",
      "All 5 repeats done! \n",
      "\n",
      "Gradient Boosting 500: undersampling at rate 0.5 from the most popular class:\n",
      "All 5 repeats done! \n",
      "\n",
      "Random Forest: \n",
      "Random Forest: undersampling at rate 0.1 from the most popular class:\n",
      "All 5 repeats done! \n",
      "\n",
      "Random Forest: undersampling at rate 0.2 from the most popular class:\n",
      "All 5 repeats done! \n",
      "\n",
      "Random Forest: undersampling at rate 0.3 from the most popular class:\n",
      "All 5 repeats done! \n",
      "\n",
      "Random Forest: undersampling at rate 0.4 from the most popular class:\n",
      "All 5 repeats done! \n",
      "\n",
      "Random Forest: undersampling at rate 0.5 from the most popular class:\n",
      "All 5 repeats done! \n",
      "\n",
      "results saved!\n"
     ]
    }
   ],
   "source": [
    "results = {}\n",
    "for name, model in angle_models.items():\n",
    "    print(f'{name}: ')\n",
    "    \n",
    "    model_results = {}\n",
    "    for rate in undersample_rates:\n",
    "        print(f'{name}: undersampling at rate {rate} from the most popular class:')\n",
    "        \n",
    "        model_results_rates = []\n",
    "        for i in range(repeats):\n",
    "            # undersampling from the most popular class\n",
    "            min_class_idx = np.argwhere(y_train).flatten()\n",
    "            maj_class_idx = np.argwhere(~y_train).flatten() # majority class is 'false': not feasible\n",
    "            chosen_maj_idx = np.random.choice(maj_class_idx, \n",
    "                                              size=int(len(maj_class_idx)*rate), \n",
    "                                              replace=False)\n",
    "            undersampler = np.concatenate((min_class_idx, chosen_maj_idx))\n",
    "            X_train_undersampled, y_train_undersampled = X_train[undersampler], y_train.iloc[undersampler]\n",
    "\n",
    "            model.fit(X_train_undersampled, y_train_undersampled)\n",
    "            eval_result = evaluate_classifier_and_get_results(model, X_test, y_test)\n",
    "            \n",
    "            model_results_rates.append(eval_result)\n",
    "            \n",
    "        model_results[rate] = model_results_rates\n",
    "        print(f'All {repeats} repeats done! \\n')\n",
    "        \n",
    "    results[name] = model_results\n",
    "    \n",
    "with open('./output/undersampling/results', 'wb') as f:\n",
    "    pickle.dump(results, f)\n",
    "    print('results saved!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "def calc_mean_std_from_repeat(repeat_data, keys=None, stats=['mean', 'std']):\n",
    "    '''\n",
    "    Takes in a list of dictionaries `repeat_data` (with the same keys) representing\n",
    "    the repeats, return a dictionary representing the mean and standard deviation of\n",
    "    each chosen `keys`.\n",
    "    Input:\n",
    "        repeat_data: list of dictionaries, with the same keys, representing repeat \n",
    "            data\n",
    "        keys: list of keys, of the dictionaries, to compute the mean and stds on.\n",
    "        stats: list of strings, index keys of statistics to get from \n",
    "            `DataFrame.describe()`\n",
    "    '''\n",
    "    if not keys:\n",
    "        keys = repeat_data[0].keys()\n",
    "    \n",
    "    filtered_repeat_data = [{key: data[key] for key in keys} \n",
    "                            for data in repeat_data]\n",
    "    mean_std = pd.DataFrame(filtered_repeat_data).describe().loc[stats]\n",
    "    return mean_std.values.T.flatten()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true,
     "source_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Decision Tree undersampling results:\n",
      "                     0.1       0.2       0.3       0.4       0.5\n",
      "accuracy_mean   0.929946  0.951519  0.960462  0.965889  0.968336\n",
      "accuracy_std    0.003576  0.001828  0.000641  0.000612  0.000971\n",
      "precision_mean  0.394773  0.493123  0.557915  0.612594  0.648785\n",
      "precision_std   0.013022  0.011313  0.005330  0.005268  0.011807\n",
      "recall_mean     0.895875  0.833375  0.795000  0.760563  0.723063\n",
      "recall_std      0.005175  0.007233  0.009257  0.010808  0.016018\n",
      "f1 score_mean   0.547944  0.619544  0.655653  0.678584  0.683775\n",
      "f1 score_std    0.012701  0.009244  0.004642  0.006421  0.008602\n",
      "roc_mean        0.913757  0.895383  0.881843  0.868328  0.851795\n",
      "roc_std         0.003158  0.003620  0.004429  0.005360  0.007688\n",
      "Logistic Regression undersampling results:\n",
      "                     0.1       0.2       0.3       0.4       0.5\n",
      "accuracy_mean   0.950590  0.964315  0.970053  0.973095  0.974592\n",
      "accuracy_std    0.000134  0.000223  0.000089  0.000136  0.000117\n",
      "precision_mean  0.488699  0.579965  0.636140  0.677082  0.705854\n",
      "precision_std   0.000713  0.001716  0.001091  0.002056  0.001232\n",
      "recall_mean     0.941812  0.893250  0.858625  0.825437  0.794438\n",
      "recall_std      0.002214  0.002355  0.002961  0.001909  0.001405\n",
      "f1 score_mean   0.643493  0.703295  0.730821  0.743932  0.747530\n",
      "f1 score_std    0.000733  0.001554  0.000878  0.000680  0.001135\n",
      "roc_mean        0.946419  0.930548  0.917108  0.902935  0.888992\n",
      "roc_std         0.001039  0.001158  0.001406  0.000852  0.000713\n",
      "Neural Network undersampling results:\n",
      "                     0.1       0.2       0.3       0.4       0.5\n",
      "accuracy_mean   0.954753  0.968780  0.972799  0.976971  0.978415\n",
      "accuracy_std    0.001751  0.000785  0.001374  0.000743  0.000352\n",
      "precision_mean  0.512350  0.616495  0.659245  0.715762  0.746586\n",
      "precision_std   0.010625  0.007312  0.014403  0.010933  0.007848\n",
      "recall_mean     0.936812  0.901812  0.882000  0.852438  0.824000\n",
      "recall_std      0.006073  0.005184  0.009361  0.007185  0.009957\n",
      "f1 score_mean   0.662333  0.732314  0.754416  0.778072  0.783313\n",
      "f1 score_std    0.007518  0.004739  0.009095  0.005076  0.002877\n",
      "roc_mean        0.946229  0.936960  0.929656  0.917799  0.905045\n",
      "roc_std         0.002130  0.002377  0.004371  0.003249  0.004677\n",
      "Gradient Boosting 3000 undersampling results:\n",
      "                     0.1       0.2       0.3       0.4       0.5\n",
      "accuracy_mean   0.956813  0.971307  0.976480  0.979111  0.980795\n",
      "accuracy_std    0.000655  0.000326  0.000445  0.000495  0.000262\n",
      "precision_mean  0.524160  0.636807  0.698761  0.743534  0.777364\n",
      "precision_std   0.003908  0.003131  0.004878  0.006869  0.003194\n",
      "recall_mean     0.953937  0.917062  0.884687  0.853187  0.832938\n",
      "recall_std      0.003222  0.000648  0.003859  0.003707  0.003161\n",
      "f1 score_mean   0.676561  0.751657  0.780800  0.794576  0.804188\n",
      "f1 score_std    0.003741  0.002033  0.003571  0.003941  0.002573\n",
      "roc_mean        0.955447  0.945533  0.932865  0.919278  0.910540\n",
      "roc_std         0.001735  0.000229  0.001931  0.001789  0.001595\n",
      "Gradient Boosting 500 undersampling results:\n",
      "                     0.1       0.2       0.3       0.4       0.5\n",
      "accuracy_mean   0.957029  0.970627  0.976554  0.979182  0.980469\n",
      "accuracy_std    0.000521  0.000821  0.000531  0.000118  0.000288\n",
      "precision_mean  0.525518  0.630870  0.699327  0.743156  0.773628\n",
      "precision_std   0.003240  0.007484  0.005453  0.001828  0.004226\n",
      "recall_mean     0.952500  0.915375  0.885625  0.856250  0.830562\n",
      "recall_std      0.001466  0.002683  0.003202  0.000937  0.004360\n",
      "f1 score_mean   0.677327  0.746929  0.781522  0.795703  0.801074\n",
      "f1 score_std    0.002451  0.005506  0.004358  0.000852  0.002732\n",
      "roc_mean        0.954877  0.944374  0.933349  0.920771  0.909241\n",
      "roc_std         0.000554  0.001465  0.001745  0.000420  0.002125\n",
      "Random Forest undersampling results:\n",
      "                     0.1       0.2       0.3       0.4       0.5\n",
      "accuracy_mean   0.938945  0.960154  0.967851  0.971624  0.973177\n",
      "accuracy_std    0.000782  0.000310  0.000273  0.000135  0.000145\n",
      "precision_mean  0.431855  0.551924  0.629157  0.692107  0.734934\n",
      "precision_std   0.003382  0.002265  0.002744  0.002360  0.002289\n",
      "recall_mean     0.917000  0.842125  0.781875  0.721813  0.678062\n",
      "recall_std      0.001050  0.001841  0.002539  0.003318  0.001994\n",
      "f1 score_mean   0.587175  0.666817  0.697248  0.706641  0.705351\n",
      "f1 score_std    0.003167  0.002120  0.002170  0.001300  0.001451\n",
      "roc_mean        0.928518  0.904073  0.879485  0.852926  0.832954\n",
      "roc_std         0.000691  0.001005  0.001273  0.001571  0.000976\n"
     ]
    }
   ],
   "source": [
    "keys = ['accuracy', 'precision', 'recall', 'f1 score', 'roc']\n",
    "stats = ['mean', 'std']\n",
    "col_names = reduce(lambda a, b: a+b, \n",
    "                   [[f'{col}_{idx}' for idx in stats] \n",
    "                    for col in keys])\n",
    "\n",
    "undersample_results = {}\n",
    "for name, undersample_result in results.items():    \n",
    "    undersample_mean_std_result = []\n",
    "    for _, repeat_result in undersample_result.items():\n",
    "        mean_std = calc_mean_std_from_repeat(repeat_result, \n",
    "                                             keys=keys, stats=stats)\n",
    "        undersample_mean_std_result.append(mean_std)\n",
    "    \n",
    "    undersample_mean_std_result = pd.DataFrame(\n",
    "        undersample_mean_std_result, \n",
    "        columns=col_names, index=undersample_result.keys())\n",
    "    undersample_results[name] = undersample_mean_std_result\n",
    "    \n",
    "    print(f'{name} undersampling results:')\n",
    "    print(undersample_mean_std_result.T.to_string())\n",
    "    \n",
    "#print('\\n'.join([df.to_string() for df in undersample_results.values()]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true,
     "source_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 3 folds for each of 10 candidates, totalling 30 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done   2 tasks      | elapsed:  1.3min\n",
      "[Parallel(n_jobs=-1)]: Done  22 out of  30 | elapsed: 12.2min remaining:  4.4min\n",
      "[Parallel(n_jobs=-1)]: Done  30 out of  30 | elapsed: 15.2min finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LRC grid search saved\n"
     ]
    }
   ],
   "source": [
    "lrc_search = model_selection.RandomizedSearchCV(\n",
    "    linear_model.LogisticRegression(), \n",
    "    param_distributions={\n",
    "        'C': [0.01, 0.10, 1.00, 10.00],\n",
    "        'class_weight': [{True: 2, False: 1}, {True: 3, False: 1}, 'balanced', None],\n",
    "        'max_iter': [1000]\n",
    "    }, \n",
    "    n_jobs=-1,\n",
    "    refit='f1',\n",
    "    scoring=['neg_log_loss', 'f1', 'roc_auc'],\n",
    "    cv=3,\n",
    "    verbose=5)\n",
    "lrc_search.fit(X_train, y_train)\n",
    "\n",
    "with open(f'./model_gridsearch/{loss_type}/lrc', 'wb') as f:\n",
    "    pickle.dump(lrc_search, f)\n",
    "    print('LRC grid search saved')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "with open(f'./model_gridsearch/{loss_type}/lrc', 'rb') as f:\n",
    "    lrc_search = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true,
     "source_hidden": true
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>params</th>\n",
       "      <th>overall_rank</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>12</td>\n",
       "      <td>{'C': 10.0, 'class_weight': {True: 2, False: 1}, 'max_iter': 1000}</td>\n",
       "      <td>5.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8</td>\n",
       "      <td>{'C': 1.0, 'class_weight': {True: 2, False: 1}, 'max_iter': 1000}</td>\n",
       "      <td>6.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>13</td>\n",
       "      <td>{'C': 10.0, 'class_weight': {True: 3, False: 1}, 'max_iter': 1000}</td>\n",
       "      <td>6.5</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                                params  \\\n",
       "12  {'C': 10.0, 'class_weight': {True: 2, False: 1}, 'max_iter': 1000}   \n",
       "8   {'C': 1.0, 'class_weight': {True: 2, False: 1}, 'max_iter': 1000}    \n",
       "13  {'C': 10.0, 'class_weight': {True: 3, False: 1}, 'max_iter': 1000}   \n",
       "\n",
       "    overall_rank  \n",
       "12  5.0           \n",
       "8   6.0           \n",
       "13  6.5           "
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_search_result(lrc_search.cv_results_).iloc[:3][['params', 'overall_rank']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true,
     "source_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "[Parallel(n_jobs=1)]: Done   1 out of   1 | elapsed:  2.1min finished\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "LogisticRegression(C=10.0, class_weight={False: 1, True: 2}, max_iter=1000,\n",
       "                   verbose=1)"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lrc = linear_model.LogisticRegression(C=10.0, class_weight={True: 2, False: 1}, max_iter=1000, verbose=1)\n",
    "lrc.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true,
     "source_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing results:\n",
      "accuracy = 0.9746101263575296\n",
      "adjusted balanced accuracy = 0.7716227790976131\n",
      "balanced accuracy = 0.8858113895488066\n",
      "average precision = 0.5753544643922275\n",
      "f1 score = 0.7508710801393728\n",
      "precision = 0.7175360710321864\n",
      "recall = 0.7874543239951279\n",
      "roc = 0.8858113895488066\n",
      "confusion matrix\n",
      " = [[31642   509]\n",
      " [  349  1293]]\n"
     ]
    }
   ],
   "source": [
    "evaluate_classifier(lrc, X_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Neural Networks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true,
     "source_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 3 folds for each of 10 candidates, totalling 30 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done   2 tasks      | elapsed:  9.4min\n",
      "[Parallel(n_jobs=-1)]: Done  22 out of  30 | elapsed: 41.2min remaining: 15.0min\n",
      "[Parallel(n_jobs=-1)]: Done  30 out of  30 | elapsed: 60.4min finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NN grid search saved\n"
     ]
    }
   ],
   "source": [
    "nn_search = model_selection.RandomizedSearchCV(\n",
    "    neural_network.MLPClassifier(), \n",
    "    param_distributions={\n",
    "        'alpha': [1e-5, 1e-4, 1e-3, 1e-2],\n",
    "        'hidden_layer_sizes': [(100,), (150,), (200,)],\n",
    "        'learning_rate_init': [1e-4, 1e-3, 1e-2, 1e-1],\n",
    "        'max_iter': [1000],\n",
    "        'early_stopping': [True],\n",
    "        'n_iter_no_change': [5, 25]\n",
    "    },\n",
    "    n_jobs=-1,\n",
    "    refit='f1',\n",
    "    scoring=['neg_log_loss', 'f1', 'roc_auc'],\n",
    "    cv=3,\n",
    "    verbose=5)\n",
    "nn_search.fit(X_train, y_train)\n",
    "\n",
    "with open(f'./model_gridsearch/{loss_type}/nn', 'wb') as f:\n",
    "    pickle.dump(nn_search, f)\n",
    "    print('NN grid search saved')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(f'./model_gridsearch/{loss_type}/nn', 'rb') as f:\n",
    "    nn_angle_search = pickle.load(f)\n",
    "nn_angle = nn_angle_search.best_estimator_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true,
     "source_hidden": true
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>mean_fit_time</th>\n",
       "      <th>mean_score_time</th>\n",
       "      <th>params</th>\n",
       "      <th>mean_test_neg_log_loss</th>\n",
       "      <th>rank_test_neg_log_loss</th>\n",
       "      <th>mean_test_f1</th>\n",
       "      <th>rank_test_f1</th>\n",
       "      <th>mean_test_roc_auc</th>\n",
       "      <th>rank_test_roc_auc</th>\n",
       "      <th>mean_test_balanced_accuracy</th>\n",
       "      <th>rank_test_balanced_accuracy</th>\n",
       "      <th>mean_test_accuracy</th>\n",
       "      <th>rank_test_accuracy</th>\n",
       "      <th>mean_test_recall</th>\n",
       "      <th>rank_test_recall</th>\n",
       "      <th>mean_test_precision</th>\n",
       "      <th>rank_test_precision</th>\n",
       "      <th>overall_rank</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>73</td>\n",
       "      <td>472.535543</td>\n",
       "      <td>4.300126</td>\n",
       "      <td>{'alpha': 0.01, 'early_stopping': True, 'hidden_layer_sizes': (100,), 'learning_rate_init': 0.0001, 'max_iter': 1000, 'n_iter_no_change': 25}</td>\n",
       "      <td>-0.049798</td>\n",
       "      <td>1</td>\n",
       "      <td>0.786166</td>\n",
       "      <td>6</td>\n",
       "      <td>0.990913</td>\n",
       "      <td>2</td>\n",
       "      <td>0.871463</td>\n",
       "      <td>14</td>\n",
       "      <td>0.980209</td>\n",
       "      <td>7</td>\n",
       "      <td>0.751052</td>\n",
       "      <td>14</td>\n",
       "      <td>0.824854</td>\n",
       "      <td>24</td>\n",
       "      <td>3.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>65</td>\n",
       "      <td>778.812411</td>\n",
       "      <td>8.860905</td>\n",
       "      <td>{'alpha': 0.001, 'early_stopping': True, 'hidden_layer_sizes': (200,), 'learning_rate_init': 0.0001, 'max_iter': 1000, 'n_iter_no_change': 25}</td>\n",
       "      <td>-0.050386</td>\n",
       "      <td>5</td>\n",
       "      <td>0.785522</td>\n",
       "      <td>9</td>\n",
       "      <td>0.990882</td>\n",
       "      <td>5</td>\n",
       "      <td>0.869208</td>\n",
       "      <td>24</td>\n",
       "      <td>0.980272</td>\n",
       "      <td>5</td>\n",
       "      <td>0.746232</td>\n",
       "      <td>25</td>\n",
       "      <td>0.829536</td>\n",
       "      <td>15</td>\n",
       "      <td>7.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>41</td>\n",
       "      <td>810.593611</td>\n",
       "      <td>8.836997</td>\n",
       "      <td>{'alpha': 0.0001, 'early_stopping': True, 'hidden_layer_sizes': (200,), 'learning_rate_init': 0.0001, 'max_iter': 1000, 'n_iter_no_change': 25}</td>\n",
       "      <td>-0.050518</td>\n",
       "      <td>10</td>\n",
       "      <td>0.786524</td>\n",
       "      <td>5</td>\n",
       "      <td>0.990899</td>\n",
       "      <td>4</td>\n",
       "      <td>0.869390</td>\n",
       "      <td>22</td>\n",
       "      <td>0.980374</td>\n",
       "      <td>2</td>\n",
       "      <td>0.746504</td>\n",
       "      <td>24</td>\n",
       "      <td>0.831110</td>\n",
       "      <td>8</td>\n",
       "      <td>7.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9</td>\n",
       "      <td>661.248960</td>\n",
       "      <td>7.237245</td>\n",
       "      <td>{'alpha': 1e-05, 'early_stopping': True, 'hidden_layer_sizes': (150,), 'learning_rate_init': 0.0001, 'max_iter': 1000, 'n_iter_no_change': 25}</td>\n",
       "      <td>-0.050664</td>\n",
       "      <td>14</td>\n",
       "      <td>0.788529</td>\n",
       "      <td>1</td>\n",
       "      <td>0.990866</td>\n",
       "      <td>6</td>\n",
       "      <td>0.876460</td>\n",
       "      <td>5</td>\n",
       "      <td>0.980216</td>\n",
       "      <td>6</td>\n",
       "      <td>0.761575</td>\n",
       "      <td>5</td>\n",
       "      <td>0.817495</td>\n",
       "      <td>39</td>\n",
       "      <td>7.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>33</td>\n",
       "      <td>603.114625</td>\n",
       "      <td>8.668832</td>\n",
       "      <td>{'alpha': 0.0001, 'early_stopping': True, 'hidden_layer_sizes': (150,), 'learning_rate_init': 0.0001, 'max_iter': 1000, 'n_iter_no_change': 25}</td>\n",
       "      <td>-0.050462</td>\n",
       "      <td>8</td>\n",
       "      <td>0.785602</td>\n",
       "      <td>8</td>\n",
       "      <td>0.990675</td>\n",
       "      <td>11</td>\n",
       "      <td>0.873743</td>\n",
       "      <td>9</td>\n",
       "      <td>0.980012</td>\n",
       "      <td>12</td>\n",
       "      <td>0.756076</td>\n",
       "      <td>9</td>\n",
       "      <td>0.818034</td>\n",
       "      <td>35</td>\n",
       "      <td>8.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>15</td>\n",
       "      <td>335.030072</td>\n",
       "      <td>6.458025</td>\n",
       "      <td>{'alpha': 1e-05, 'early_stopping': True, 'hidden_layer_sizes': (150,), 'learning_rate_init': 0.1, 'max_iter': 1000, 'n_iter_no_change': 25}</td>\n",
       "      <td>-0.160382</td>\n",
       "      <td>92</td>\n",
       "      <td>0.002158</td>\n",
       "      <td>88</td>\n",
       "      <td>0.914427</td>\n",
       "      <td>91</td>\n",
       "      <td>0.500393</td>\n",
       "      <td>87</td>\n",
       "      <td>0.951333</td>\n",
       "      <td>90</td>\n",
       "      <td>0.001086</td>\n",
       "      <td>88</td>\n",
       "      <td>0.193164</td>\n",
       "      <td>83</td>\n",
       "      <td>90.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>38</td>\n",
       "      <td>103.648472</td>\n",
       "      <td>5.495825</td>\n",
       "      <td>{'alpha': 0.0001, 'early_stopping': True, 'hidden_layer_sizes': (150,), 'learning_rate_init': 0.1, 'max_iter': 1000, 'n_iter_no_change': 5}</td>\n",
       "      <td>-0.169698</td>\n",
       "      <td>95</td>\n",
       "      <td>0.003221</td>\n",
       "      <td>86</td>\n",
       "      <td>0.937660</td>\n",
       "      <td>84</td>\n",
       "      <td>0.500540</td>\n",
       "      <td>84</td>\n",
       "      <td>0.951123</td>\n",
       "      <td>93</td>\n",
       "      <td>0.001629</td>\n",
       "      <td>86</td>\n",
       "      <td>0.159353</td>\n",
       "      <td>84</td>\n",
       "      <td>90.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>31</td>\n",
       "      <td>296.180444</td>\n",
       "      <td>7.025238</td>\n",
       "      <td>{'alpha': 0.0001, 'early_stopping': True, 'hidden_layer_sizes': (100,), 'learning_rate_init': 0.1, 'max_iter': 1000, 'n_iter_no_change': 25}</td>\n",
       "      <td>-0.147168</td>\n",
       "      <td>90</td>\n",
       "      <td>0.000407</td>\n",
       "      <td>93</td>\n",
       "      <td>0.907087</td>\n",
       "      <td>93</td>\n",
       "      <td>0.500024</td>\n",
       "      <td>92</td>\n",
       "      <td>0.951429</td>\n",
       "      <td>86</td>\n",
       "      <td>0.000204</td>\n",
       "      <td>93</td>\n",
       "      <td>0.108333</td>\n",
       "      <td>88</td>\n",
       "      <td>91.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7</td>\n",
       "      <td>259.400802</td>\n",
       "      <td>3.715485</td>\n",
       "      <td>{'alpha': 1e-05, 'early_stopping': True, 'hidden_layer_sizes': (100,), 'learning_rate_init': 0.1, 'max_iter': 1000, 'n_iter_no_change': 25}</td>\n",
       "      <td>-0.142647</td>\n",
       "      <td>87</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>96</td>\n",
       "      <td>0.917146</td>\n",
       "      <td>90</td>\n",
       "      <td>0.499952</td>\n",
       "      <td>96</td>\n",
       "      <td>0.951475</td>\n",
       "      <td>84</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>96</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>96</td>\n",
       "      <td>91.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>30</td>\n",
       "      <td>117.819146</td>\n",
       "      <td>4.502727</td>\n",
       "      <td>{'alpha': 0.0001, 'early_stopping': True, 'hidden_layer_sizes': (100,), 'learning_rate_init': 0.1, 'max_iter': 1000, 'n_iter_no_change': 5}</td>\n",
       "      <td>-0.167467</td>\n",
       "      <td>94</td>\n",
       "      <td>0.000270</td>\n",
       "      <td>95</td>\n",
       "      <td>0.913246</td>\n",
       "      <td>92</td>\n",
       "      <td>0.499990</td>\n",
       "      <td>94</td>\n",
       "      <td>0.951426</td>\n",
       "      <td>87</td>\n",
       "      <td>0.000136</td>\n",
       "      <td>94</td>\n",
       "      <td>0.033179</td>\n",
       "      <td>95</td>\n",
       "      <td>94.5</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>96 rows × 18 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "    mean_fit_time  mean_score_time  \\\n",
       "73  472.535543     4.300126          \n",
       "65  778.812411     8.860905          \n",
       "41  810.593611     8.836997          \n",
       "9   661.248960     7.237245          \n",
       "33  603.114625     8.668832          \n",
       "..         ...          ...          \n",
       "15  335.030072     6.458025          \n",
       "38  103.648472     5.495825          \n",
       "31  296.180444     7.025238          \n",
       "7   259.400802     3.715485          \n",
       "30  117.819146     4.502727          \n",
       "\n",
       "                                                                                                                                             params  \\\n",
       "73  {'alpha': 0.01, 'early_stopping': True, 'hidden_layer_sizes': (100,), 'learning_rate_init': 0.0001, 'max_iter': 1000, 'n_iter_no_change': 25}     \n",
       "65  {'alpha': 0.001, 'early_stopping': True, 'hidden_layer_sizes': (200,), 'learning_rate_init': 0.0001, 'max_iter': 1000, 'n_iter_no_change': 25}    \n",
       "41  {'alpha': 0.0001, 'early_stopping': True, 'hidden_layer_sizes': (200,), 'learning_rate_init': 0.0001, 'max_iter': 1000, 'n_iter_no_change': 25}   \n",
       "9   {'alpha': 1e-05, 'early_stopping': True, 'hidden_layer_sizes': (150,), 'learning_rate_init': 0.0001, 'max_iter': 1000, 'n_iter_no_change': 25}    \n",
       "33  {'alpha': 0.0001, 'early_stopping': True, 'hidden_layer_sizes': (150,), 'learning_rate_init': 0.0001, 'max_iter': 1000, 'n_iter_no_change': 25}   \n",
       "..                                                                                                                                              ...   \n",
       "15  {'alpha': 1e-05, 'early_stopping': True, 'hidden_layer_sizes': (150,), 'learning_rate_init': 0.1, 'max_iter': 1000, 'n_iter_no_change': 25}       \n",
       "38  {'alpha': 0.0001, 'early_stopping': True, 'hidden_layer_sizes': (150,), 'learning_rate_init': 0.1, 'max_iter': 1000, 'n_iter_no_change': 5}       \n",
       "31  {'alpha': 0.0001, 'early_stopping': True, 'hidden_layer_sizes': (100,), 'learning_rate_init': 0.1, 'max_iter': 1000, 'n_iter_no_change': 25}      \n",
       "7   {'alpha': 1e-05, 'early_stopping': True, 'hidden_layer_sizes': (100,), 'learning_rate_init': 0.1, 'max_iter': 1000, 'n_iter_no_change': 25}       \n",
       "30  {'alpha': 0.0001, 'early_stopping': True, 'hidden_layer_sizes': (100,), 'learning_rate_init': 0.1, 'max_iter': 1000, 'n_iter_no_change': 5}       \n",
       "\n",
       "    mean_test_neg_log_loss  rank_test_neg_log_loss  mean_test_f1  \\\n",
       "73 -0.049798                1                       0.786166       \n",
       "65 -0.050386                5                       0.785522       \n",
       "41 -0.050518                10                      0.786524       \n",
       "9  -0.050664                14                      0.788529       \n",
       "33 -0.050462                8                       0.785602       \n",
       "..       ...               ..                            ...       \n",
       "15 -0.160382                92                      0.002158       \n",
       "38 -0.169698                95                      0.003221       \n",
       "31 -0.147168                90                      0.000407       \n",
       "7  -0.142647                87                      0.000000       \n",
       "30 -0.167467                94                      0.000270       \n",
       "\n",
       "    rank_test_f1  mean_test_roc_auc  rank_test_roc_auc  \\\n",
       "73  6             0.990913           2                   \n",
       "65  9             0.990882           5                   \n",
       "41  5             0.990899           4                   \n",
       "9   1             0.990866           6                   \n",
       "33  8             0.990675           11                  \n",
       ".. ..                  ...           ..                  \n",
       "15  88            0.914427           91                  \n",
       "38  86            0.937660           84                  \n",
       "31  93            0.907087           93                  \n",
       "7   96            0.917146           90                  \n",
       "30  95            0.913246           92                  \n",
       "\n",
       "    mean_test_balanced_accuracy  rank_test_balanced_accuracy  \\\n",
       "73  0.871463                     14                            \n",
       "65  0.869208                     24                            \n",
       "41  0.869390                     22                            \n",
       "9   0.876460                     5                             \n",
       "33  0.873743                     9                             \n",
       "..       ...                    ..                             \n",
       "15  0.500393                     87                            \n",
       "38  0.500540                     84                            \n",
       "31  0.500024                     92                            \n",
       "7   0.499952                     96                            \n",
       "30  0.499990                     94                            \n",
       "\n",
       "    mean_test_accuracy  rank_test_accuracy  mean_test_recall  \\\n",
       "73  0.980209            7                   0.751052           \n",
       "65  0.980272            5                   0.746232           \n",
       "41  0.980374            2                   0.746504           \n",
       "9   0.980216            6                   0.761575           \n",
       "33  0.980012            12                  0.756076           \n",
       "..       ...            ..                       ...           \n",
       "15  0.951333            90                  0.001086           \n",
       "38  0.951123            93                  0.001629           \n",
       "31  0.951429            86                  0.000204           \n",
       "7   0.951475            84                  0.000000           \n",
       "30  0.951426            87                  0.000136           \n",
       "\n",
       "    rank_test_recall  mean_test_precision  rank_test_precision  overall_rank  \n",
       "73  14                0.824854             24                   3.5           \n",
       "65  25                0.829536             15                   7.0           \n",
       "41  24                0.831110             8                    7.5           \n",
       "9   5                 0.817495             39                   7.5           \n",
       "33  9                 0.818034             35                   8.0           \n",
       ".. ..                      ...             ..                   ...           \n",
       "15  88                0.193164             83                   90.0          \n",
       "38  86                0.159353             84                   90.5          \n",
       "31  93                0.108333             88                   91.5          \n",
       "7   96                0.000000             96                   91.5          \n",
       "30  94                0.033179             95                   94.5          \n",
       "\n",
       "[96 rows x 18 columns]"
      ]
     },
     "execution_count": 156,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_search_result(nn_angle_search.cv_results_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true,
     "source_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 1, loss = 0.12833527\n",
      "Validation score: 0.970211\n",
      "Iteration 2, loss = 0.06765638\n",
      "Validation score: 0.973992\n",
      "Iteration 3, loss = 0.06060239\n",
      "Validation score: 0.976294\n",
      "Iteration 4, loss = 0.05675297\n",
      "Validation score: 0.976557\n",
      "Iteration 5, loss = 0.05422615\n",
      "Validation score: 0.978102\n",
      "Iteration 6, loss = 0.05206493\n",
      "Validation score: 0.978431\n",
      "Iteration 7, loss = 0.05022001\n",
      "Validation score: 0.978299\n",
      "Iteration 8, loss = 0.04873073\n",
      "Validation score: 0.978168\n",
      "Iteration 9, loss = 0.04729036\n",
      "Validation score: 0.979056\n",
      "Iteration 10, loss = 0.04618162\n",
      "Validation score: 0.979056\n",
      "Iteration 11, loss = 0.04498943\n",
      "Validation score: 0.978858\n",
      "Iteration 12, loss = 0.04394029\n",
      "Validation score: 0.979648\n",
      "Iteration 13, loss = 0.04300674\n",
      "Validation score: 0.979384\n",
      "Iteration 14, loss = 0.04209225\n",
      "Validation score: 0.979220\n",
      "Iteration 15, loss = 0.04121736\n",
      "Validation score: 0.979615\n",
      "Iteration 16, loss = 0.04035396\n",
      "Validation score: 0.979779\n",
      "Iteration 17, loss = 0.03951800\n",
      "Validation score: 0.979845\n",
      "Iteration 18, loss = 0.03884174\n",
      "Validation score: 0.980305\n",
      "Iteration 19, loss = 0.03811195\n",
      "Validation score: 0.979746\n",
      "Iteration 20, loss = 0.03738226\n",
      "Validation score: 0.980009\n",
      "Iteration 21, loss = 0.03674155\n",
      "Validation score: 0.980075\n",
      "Iteration 22, loss = 0.03614225\n",
      "Validation score: 0.980338\n",
      "Iteration 23, loss = 0.03547713\n",
      "Validation score: 0.979911\n",
      "Iteration 24, loss = 0.03486870\n",
      "Validation score: 0.980634\n",
      "Iteration 25, loss = 0.03436330\n",
      "Validation score: 0.980338\n",
      "Iteration 26, loss = 0.03381186\n",
      "Validation score: 0.980338\n",
      "Iteration 27, loss = 0.03328750\n",
      "Validation score: 0.980568\n",
      "Iteration 28, loss = 0.03265600\n",
      "Validation score: 0.980272\n",
      "Iteration 29, loss = 0.03224652\n",
      "Validation score: 0.980404\n",
      "Iteration 30, loss = 0.03172944\n",
      "Validation score: 0.979615\n",
      "Iteration 31, loss = 0.03120754\n",
      "Validation score: 0.980174\n",
      "Iteration 32, loss = 0.03077642\n",
      "Validation score: 0.980470\n",
      "Iteration 33, loss = 0.03033250\n",
      "Validation score: 0.980535\n",
      "Iteration 34, loss = 0.02990510\n",
      "Validation score: 0.980930\n",
      "Iteration 35, loss = 0.02942023\n",
      "Validation score: 0.980634\n",
      "Iteration 36, loss = 0.02902655\n",
      "Validation score: 0.980864\n",
      "Iteration 37, loss = 0.02864326\n",
      "Validation score: 0.980535\n",
      "Iteration 38, loss = 0.02823962\n",
      "Validation score: 0.980798\n",
      "Iteration 39, loss = 0.02778577\n",
      "Validation score: 0.980371\n",
      "Iteration 40, loss = 0.02737608\n",
      "Validation score: 0.980371\n",
      "Iteration 41, loss = 0.02704298\n",
      "Validation score: 0.980765\n",
      "Iteration 42, loss = 0.02668622\n",
      "Validation score: 0.980864\n",
      "Iteration 43, loss = 0.02628385\n",
      "Validation score: 0.980798\n",
      "Iteration 44, loss = 0.02586336\n",
      "Validation score: 0.980437\n",
      "Iteration 45, loss = 0.02554909\n",
      "Validation score: 0.980765\n",
      "Iteration 46, loss = 0.02519539\n",
      "Validation score: 0.980864\n",
      "Iteration 47, loss = 0.02488643\n",
      "Validation score: 0.980470\n",
      "Iteration 48, loss = 0.02447661\n",
      "Validation score: 0.980765\n",
      "Iteration 49, loss = 0.02412565\n",
      "Validation score: 0.981028\n",
      "Iteration 50, loss = 0.02392325\n",
      "Validation score: 0.981028\n",
      "Iteration 51, loss = 0.02360102\n",
      "Validation score: 0.980305\n",
      "Iteration 52, loss = 0.02318322\n",
      "Validation score: 0.980700\n",
      "Iteration 53, loss = 0.02296864\n",
      "Validation score: 0.980535\n",
      "Iteration 54, loss = 0.02258090\n",
      "Validation score: 0.981259\n",
      "Iteration 55, loss = 0.02228740\n",
      "Validation score: 0.980700\n",
      "Iteration 56, loss = 0.02205905\n",
      "Validation score: 0.980897\n",
      "Iteration 57, loss = 0.02179337\n",
      "Validation score: 0.980831\n",
      "Iteration 58, loss = 0.02135439\n",
      "Validation score: 0.980470\n",
      "Iteration 59, loss = 0.02113982\n",
      "Validation score: 0.980272\n",
      "Iteration 60, loss = 0.02099401\n",
      "Validation score: 0.980700\n",
      "Iteration 61, loss = 0.02056121\n",
      "Validation score: 0.981061\n",
      "Iteration 62, loss = 0.02031738\n",
      "Validation score: 0.980568\n",
      "Iteration 63, loss = 0.02016532\n",
      "Validation score: 0.980535\n",
      "Iteration 64, loss = 0.01991287\n",
      "Validation score: 0.980568\n",
      "Iteration 65, loss = 0.01956181\n",
      "Validation score: 0.980831\n",
      "Iteration 66, loss = 0.01933668\n",
      "Validation score: 0.980239\n",
      "Iteration 67, loss = 0.01909801\n",
      "Validation score: 0.980700\n",
      "Iteration 68, loss = 0.01882686\n",
      "Validation score: 0.980765\n",
      "Iteration 69, loss = 0.01861293\n",
      "Validation score: 0.980305\n",
      "Iteration 70, loss = 0.01835826\n",
      "Validation score: 0.981357\n",
      "Iteration 71, loss = 0.01814535\n",
      "Validation score: 0.980568\n",
      "Iteration 72, loss = 0.01797456\n",
      "Validation score: 0.980634\n",
      "Iteration 73, loss = 0.01769783\n",
      "Validation score: 0.979911\n",
      "Iteration 74, loss = 0.01748801\n",
      "Validation score: 0.980502\n",
      "Iteration 75, loss = 0.01726971\n",
      "Validation score: 0.980798\n",
      "Iteration 76, loss = 0.01702418\n",
      "Validation score: 0.980437\n",
      "Iteration 77, loss = 0.01673364\n",
      "Validation score: 0.980897\n",
      "Iteration 78, loss = 0.01662532\n",
      "Validation score: 0.980634\n",
      "Iteration 79, loss = 0.01634488\n",
      "Validation score: 0.980568\n",
      "Iteration 80, loss = 0.01614793\n",
      "Validation score: 0.980798\n",
      "Validation score did not improve more than tol=0.000100 for 25 consecutive epochs. Stopping.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "MLPClassifier(alpha=0.001, early_stopping=True, hidden_layer_sizes=(150,),\n",
       "              learning_rate_init=0.0001, max_iter=1000, n_iter_no_change=25,\n",
       "              verbose=True)"
      ]
     },
     "execution_count": 152,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# alpha and hidden layer sizes chosen from parameter search\n",
    "nn = neural_network.MLPClassifier(alpha=1e-3,\n",
    "                                  hidden_layer_sizes=(150,),\n",
    "                                  max_iter=1000,\n",
    "                                  learning_rate_init=1e-4,\n",
    "                                  early_stopping=True,\n",
    "                                  n_iter_no_change=25,\n",
    "                                  verbose=True)\n",
    "nn.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true,
     "source_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing results:\n",
      "accuracy = 0.980528511821975\n",
      "adjusted balanced accuracy = 0.7856958448805158\n",
      "balanced accuracy = 0.8928479224402579\n",
      "average precision = 0.6522519358642096\n",
      "f1 score = 0.8013285024154589\n",
      "precision = 0.8071776155717761\n",
      "recall = 0.7955635491606715\n",
      "roc = 0.892847922440258\n",
      "confusion matrix\n",
      " = [[31808   317]\n",
      " [  341  1327]]\n"
     ]
    }
   ],
   "source": [
    "evaluate_classifier(nn, X_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Decision Trees"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true,
     "source_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 3 folds for each of 10 candidates, totalling 30 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done   2 tasks      | elapsed:  1.1min\n",
      "[Parallel(n_jobs=-1)]: Done  22 out of  30 | elapsed:  3.4min remaining:  1.2min\n",
      "[Parallel(n_jobs=-1)]: Done  30 out of  30 | elapsed:  4.2min finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DT grid search saved\n"
     ]
    }
   ],
   "source": [
    "tree_search = model_selection.RandomizedSearchCV(\n",
    "    tree.DecisionTreeClassifier(), \n",
    "    param_distributions={\n",
    "        'criterion': ['gini', 'entropy'],\n",
    "        'splitter': ['random', 'best'],\n",
    "        'class_weight': [{True: 2, False: 1}, {True: 3, False: 1}, 'balanced', None],\n",
    "        'max_depth': [None],\n",
    "        'min_samples_leaf': [20, 50],\n",
    "        'min_samples_split': [20, 50]\n",
    "    },\n",
    "    n_jobs=-1,\n",
    "    cv=3,\n",
    "    refit='f1',\n",
    "    scoring=['neg_log_loss', 'f1', 'roc_auc'],\n",
    "    verbose=5)\n",
    "tree_search.fit(X_train, y_train)\n",
    "\n",
    "with open(f'./model_gridsearch/{loss_type}/dt', 'wb') as f:\n",
    "    pickle.dump(tree_search, f)\n",
    "    print('DT grid search saved')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "with open(f'./model_gridsearch/{loss_type}/dt', 'rb') as f:\n",
    "    tree_search = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true,
     "source_hidden": true
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>params</th>\n",
       "      <th>overall_rank</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>30</td>\n",
       "      <td>{'class_weight': {True: 3, False: 1}, 'criterion': 'entropy', 'max_depth': None, 'min_samples_leaf': 50, 'min_samples_split': 50, 'splitter': 'random'}</td>\n",
       "      <td>13.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>18</td>\n",
       "      <td>{'class_weight': {True: 3, False: 1}, 'criterion': 'gini', 'max_depth': None, 'min_samples_leaf': 20, 'min_samples_split': 50, 'splitter': 'random'}</td>\n",
       "      <td>14.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>26</td>\n",
       "      <td>{'class_weight': {True: 3, False: 1}, 'criterion': 'entropy', 'max_depth': None, 'min_samples_leaf': 20, 'min_samples_split': 50, 'splitter': 'random'}</td>\n",
       "      <td>16.5</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                                                                                                                     params  \\\n",
       "30  {'class_weight': {True: 3, False: 1}, 'criterion': 'entropy', 'max_depth': None, 'min_samples_leaf': 50, 'min_samples_split': 50, 'splitter': 'random'}   \n",
       "18  {'class_weight': {True: 3, False: 1}, 'criterion': 'gini', 'max_depth': None, 'min_samples_leaf': 20, 'min_samples_split': 50, 'splitter': 'random'}      \n",
       "26  {'class_weight': {True: 3, False: 1}, 'criterion': 'entropy', 'max_depth': None, 'min_samples_leaf': 20, 'min_samples_split': 50, 'splitter': 'random'}   \n",
       "\n",
       "    overall_rank  \n",
       "30  13.5          \n",
       "18  14.5          \n",
       "26  16.5          "
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_search_result(tree_search.cv_results_).iloc[:3][['params', 'overall_rank']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true,
     "source_hidden": true
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DecisionTreeClassifier(class_weight={False: 1, True: 2}, criterion='entropy',\n",
       "                       min_samples_leaf=50, min_samples_split=50,\n",
       "                       splitter='random')"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tree_clf = tree.DecisionTreeClassifier(class_weight={True: 2, False: 1}, \n",
    "                                       max_depth=None, min_samples_leaf=50, min_samples_split=50,\n",
    "                                       splitter='random', criterion='entropy')\n",
    "tree_clf.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true,
     "source_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing results:\n",
      "accuracy = 0.9676264315094842\n",
      "adjusted balanced accuracy = 0.7059134896003636\n",
      "balanced accuracy = 0.8529567448001818\n",
      "average precision = 0.4846263962463577\n",
      "f1 score = 0.6854514088556641\n",
      "precision = 0.6492374727668845\n",
      "recall = 0.7259439707673568\n",
      "roc = 0.8529567448001817\n",
      "confusion matrix\n",
      " = [[31507   644]\n",
      " [  450  1192]]\n"
     ]
    }
   ],
   "source": [
    "evaluate_classifier(tree_clf, X_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Histogram-Based Boosting (Similar to LightGBM)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true,
     "source_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 3 folds for each of 10 candidates, totalling 30 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done   2 tasks      | elapsed:  9.6min\n",
      "[Parallel(n_jobs=-1)]: Done  22 out of  30 | elapsed: 42.2min remaining: 15.3min\n",
      "[Parallel(n_jobs=-1)]: Done  30 out of  30 | elapsed: 55.3min finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "HGB grid search saved\n"
     ]
    }
   ],
   "source": [
    "hgb_search = model_selection.RandomizedSearchCV(\n",
    "    ensemble.HistGradientBoostingClassifier(), \n",
    "    param_distributions={\n",
    "        'max_iter': [500, 1000, 1500, 3000],\n",
    "        'learning_rate': [0.05, 0.1],\n",
    "        'min_samples_leaf': [20, 50],\n",
    "        'warm_start': [False],\n",
    "        'n_iter_no_change': [100]\n",
    "    },\n",
    "    n_jobs=-1,\n",
    "    cv=3,\n",
    "    refit='f1',\n",
    "    scoring=['neg_log_loss', 'f1', 'roc_auc'],\n",
    "    verbose=5)\n",
    "hgb_search.fit(X_train, y_train)\n",
    "\n",
    "with open(f'./model_gridsearch/{loss_type}/hgb', 'wb') as f:\n",
    "    pickle.dump(hgb_search, f)\n",
    "    print('HGB grid search saved')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(f'./model_gridsearch/{loss_type}/hgb', 'rb') as f:\n",
    "    hgb_angle_search = pickle.load(f)\n",
    "hgb_angle = hgb_angle_search.best_estimator_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true,
     "source_hidden": true
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>params</th>\n",
       "      <th>overall_rank</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>7</td>\n",
       "      <td>{'early_stopping': False, 'learning_rate': 0.05, 'max_iter': 3000, 'min_samples_leaf': 50, 'warm_start': False}</td>\n",
       "      <td>2.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>{'early_stopping': False, 'learning_rate': 0.05, 'max_iter': 3000, 'min_samples_leaf': 20, 'warm_start': False}</td>\n",
       "      <td>3.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>12</td>\n",
       "      <td>{'early_stopping': False, 'learning_rate': 0.1, 'max_iter': 1500, 'min_samples_leaf': 20, 'warm_start': False}</td>\n",
       "      <td>4.5</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                                                                             params  \\\n",
       "7   {'early_stopping': False, 'learning_rate': 0.05, 'max_iter': 3000, 'min_samples_leaf': 50, 'warm_start': False}   \n",
       "6   {'early_stopping': False, 'learning_rate': 0.05, 'max_iter': 3000, 'min_samples_leaf': 20, 'warm_start': False}   \n",
       "12  {'early_stopping': False, 'learning_rate': 0.1, 'max_iter': 1500, 'min_samples_leaf': 20, 'warm_start': False}    \n",
       "\n",
       "    overall_rank  \n",
       "7   2.0           \n",
       "6   3.0           \n",
       "12  4.5           "
      ]
     },
     "execution_count": 153,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_search_result(hgb_search.cv_results_).iloc[:3][['params', 'overall_rank']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true,
     "source_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Binning 0.854 GB of training data: 6.197 s\n",
      "Binning 0.095 GB of validation data: 0.089 s\n",
      "Fitting gradient boosted rounds:\n",
      "[1/3000] 1 tree, 31 leaves, max depth = 8, train loss: 0.15229, val loss: 0.15383, in 0.224s\n",
      "[2/3000] 1 tree, 31 leaves, max depth = 7, train loss: 0.13819, val loss: 0.14036, in 0.217s\n",
      "[3/3000] 1 tree, 31 leaves, max depth = 8, train loss: 0.12788, val loss: 0.13030, in 0.221s\n",
      "[4/3000] 1 tree, 31 leaves, max depth = 7, train loss: 0.11960, val loss: 0.12243, in 0.230s\n",
      "[5/3000] 1 tree, 31 leaves, max depth = 7, train loss: 0.11284, val loss: 0.11586, in 0.248s\n",
      "[6/3000] 1 tree, 31 leaves, max depth = 9, train loss: 0.10707, val loss: 0.10985, in 0.237s\n",
      "[7/3000] 1 tree, 31 leaves, max depth = 8, train loss: 0.10211, val loss: 0.10486, in 0.254s\n",
      "[8/3000] 1 tree, 31 leaves, max depth = 9, train loss: 0.09795, val loss: 0.10069, in 0.242s\n",
      "[9/3000] 1 tree, 31 leaves, max depth = 9, train loss: 0.09417, val loss: 0.09702, in 0.267s\n",
      "[10/3000] 1 tree, 31 leaves, max depth = 10, train loss: 0.09073, val loss: 0.09373, in 0.243s\n",
      "[11/3000] 1 tree, 31 leaves, max depth = 10, train loss: 0.08778, val loss: 0.09082, in 0.245s\n",
      "[12/3000] 1 tree, 31 leaves, max depth = 9, train loss: 0.08516, val loss: 0.08813, in 0.271s\n",
      "[13/3000] 1 tree, 31 leaves, max depth = 9, train loss: 0.08292, val loss: 0.08604, in 0.252s\n",
      "[14/3000] 1 tree, 31 leaves, max depth = 11, train loss: 0.08073, val loss: 0.08396, in 0.253s\n",
      "[15/3000] 1 tree, 31 leaves, max depth = 10, train loss: 0.07865, val loss: 0.08201, in 0.283s\n",
      "[16/3000] 1 tree, 31 leaves, max depth = 9, train loss: 0.07674, val loss: 0.08013, in 0.252s\n",
      "[17/3000] 1 tree, 31 leaves, max depth = 9, train loss: 0.07511, val loss: 0.07847, in 0.316s\n",
      "[18/3000] 1 tree, 31 leaves, max depth = 10, train loss: 0.07342, val loss: 0.07675, in 0.436s\n",
      "[19/3000] 1 tree, 31 leaves, max depth = 12, train loss: 0.07212, val loss: 0.07558, in 0.336s\n",
      "[20/3000] 1 tree, 31 leaves, max depth = 12, train loss: 0.07083, val loss: 0.07438, in 0.319s\n",
      "[21/3000] 1 tree, 31 leaves, max depth = 9, train loss: 0.06963, val loss: 0.07325, in 0.308s\n",
      "[22/3000] 1 tree, 31 leaves, max depth = 12, train loss: 0.06848, val loss: 0.07212, in 0.330s\n",
      "[23/3000] 1 tree, 31 leaves, max depth = 11, train loss: 0.06737, val loss: 0.07109, in 0.270s\n",
      "[24/3000] 1 tree, 31 leaves, max depth = 11, train loss: 0.06633, val loss: 0.07004, in 0.278s\n",
      "[25/3000] 1 tree, 31 leaves, max depth = 9, train loss: 0.06548, val loss: 0.06940, in 0.266s\n",
      "[26/3000] 1 tree, 31 leaves, max depth = 9, train loss: 0.06454, val loss: 0.06849, in 0.262s\n",
      "[27/3000] 1 tree, 31 leaves, max depth = 10, train loss: 0.06366, val loss: 0.06769, in 0.294s\n",
      "[28/3000] 1 tree, 31 leaves, max depth = 11, train loss: 0.06281, val loss: 0.06682, in 0.287s\n",
      "[29/3000] 1 tree, 31 leaves, max depth = 12, train loss: 0.06210, val loss: 0.06616, in 0.302s\n",
      "[30/3000] 1 tree, 31 leaves, max depth = 9, train loss: 0.06143, val loss: 0.06560, in 0.261s\n",
      "[31/3000] 1 tree, 31 leaves, max depth = 11, train loss: 0.06068, val loss: 0.06489, in 0.286s\n",
      "[32/3000] 1 tree, 31 leaves, max depth = 10, train loss: 0.06009, val loss: 0.06440, in 0.266s\n",
      "[33/3000] 1 tree, 31 leaves, max depth = 15, train loss: 0.05945, val loss: 0.06378, in 0.282s\n",
      "[34/3000] 1 tree, 31 leaves, max depth = 11, train loss: 0.05886, val loss: 0.06345, in 0.265s\n",
      "[35/3000] 1 tree, 31 leaves, max depth = 13, train loss: 0.05836, val loss: 0.06304, in 0.300s\n",
      "[36/3000] 1 tree, 31 leaves, max depth = 14, train loss: 0.05789, val loss: 0.06260, in 0.265s\n",
      "[37/3000] 1 tree, 31 leaves, max depth = 13, train loss: 0.05739, val loss: 0.06221, in 0.264s\n",
      "[38/3000] 1 tree, 31 leaves, max depth = 12, train loss: 0.05686, val loss: 0.06179, in 0.403s\n",
      "[39/3000] 1 tree, 31 leaves, max depth = 12, train loss: 0.05642, val loss: 0.06142, in 0.334s\n",
      "[40/3000] 1 tree, 31 leaves, max depth = 12, train loss: 0.05594, val loss: 0.06103, in 0.304s\n",
      "[41/3000] 1 tree, 31 leaves, max depth = 13, train loss: 0.05556, val loss: 0.06073, in 0.306s\n",
      "[42/3000] 1 tree, 31 leaves, max depth = 9, train loss: 0.05517, val loss: 0.06039, in 0.265s\n",
      "[43/3000] 1 tree, 31 leaves, max depth = 10, train loss: 0.05474, val loss: 0.06002, in 0.286s\n",
      "[44/3000] 1 tree, 31 leaves, max depth = 9, train loss: 0.05437, val loss: 0.05974, in 0.248s\n",
      "[45/3000] 1 tree, 31 leaves, max depth = 10, train loss: 0.05396, val loss: 0.05946, in 0.293s\n",
      "[46/3000] 1 tree, 31 leaves, max depth = 11, train loss: 0.05361, val loss: 0.05923, in 0.266s\n",
      "[47/3000] 1 tree, 31 leaves, max depth = 8, train loss: 0.05328, val loss: 0.05898, in 0.264s\n",
      "[48/3000] 1 tree, 31 leaves, max depth = 13, train loss: 0.05293, val loss: 0.05876, in 0.274s\n",
      "[49/3000] 1 tree, 31 leaves, max depth = 14, train loss: 0.05255, val loss: 0.05839, in 0.290s\n",
      "[50/3000] 1 tree, 31 leaves, max depth = 9, train loss: 0.05222, val loss: 0.05820, in 0.282s\n",
      "[51/3000] 1 tree, 31 leaves, max depth = 11, train loss: 0.05192, val loss: 0.05795, in 0.274s\n",
      "[52/3000] 1 tree, 31 leaves, max depth = 11, train loss: 0.05165, val loss: 0.05776, in 0.262s\n",
      "[53/3000] 1 tree, 31 leaves, max depth = 10, train loss: 0.05138, val loss: 0.05756, in 0.294s\n",
      "[54/3000] 1 tree, 31 leaves, max depth = 12, train loss: 0.05107, val loss: 0.05741, in 0.263s\n",
      "[55/3000] 1 tree, 31 leaves, max depth = 12, train loss: 0.05080, val loss: 0.05720, in 0.288s\n",
      "[56/3000] 1 tree, 31 leaves, max depth = 10, train loss: 0.05044, val loss: 0.05687, in 0.273s\n",
      "[57/3000] 1 tree, 31 leaves, max depth = 13, train loss: 0.05018, val loss: 0.05671, in 0.254s\n",
      "[58/3000] 1 tree, 31 leaves, max depth = 13, train loss: 0.04995, val loss: 0.05658, in 0.424s\n",
      "[59/3000] 1 tree, 31 leaves, max depth = 8, train loss: 0.04969, val loss: 0.05637, in 0.334s\n",
      "[60/3000] 1 tree, 31 leaves, max depth = 10, train loss: 0.04939, val loss: 0.05616, in 0.335s\n",
      "[61/3000] 1 tree, 31 leaves, max depth = 10, train loss: 0.04913, val loss: 0.05591, in 0.276s\n",
      "[62/3000] 1 tree, 31 leaves, max depth = 10, train loss: 0.04883, val loss: 0.05572, in 0.325s\n",
      "[63/3000] 1 tree, 31 leaves, max depth = 9, train loss: 0.04876, val loss: 0.05556, in 0.303s\n",
      "[64/3000] 1 tree, 31 leaves, max depth = 9, train loss: 0.04847, val loss: 0.05540, in 0.298s\n",
      "[65/3000] 1 tree, 31 leaves, max depth = 18, train loss: 0.04822, val loss: 0.05525, in 0.288s\n",
      "[66/3000] 1 tree, 31 leaves, max depth = 15, train loss: 0.04801, val loss: 0.05510, in 0.329s\n",
      "[67/3000] 1 tree, 31 leaves, max depth = 12, train loss: 0.04780, val loss: 0.05500, in 0.333s\n",
      "[68/3000] 1 tree, 31 leaves, max depth = 15, train loss: 0.04755, val loss: 0.05482, in 0.274s\n",
      "[69/3000] 1 tree, 31 leaves, max depth = 12, train loss: 0.04730, val loss: 0.05464, in 0.297s\n",
      "[70/3000] 1 tree, 31 leaves, max depth = 9, train loss: 0.04710, val loss: 0.05451, in 0.323s\n",
      "[71/3000] 1 tree, 31 leaves, max depth = 9, train loss: 0.04693, val loss: 0.05441, in 0.349s\n",
      "[72/3000] 1 tree, 31 leaves, max depth = 12, train loss: 0.04668, val loss: 0.05418, in 0.323s\n",
      "[73/3000] 1 tree, 31 leaves, max depth = 10, train loss: 0.04645, val loss: 0.05405, in 0.322s\n",
      "[74/3000] 1 tree, 31 leaves, max depth = 15, train loss: 0.04626, val loss: 0.05397, in 0.288s\n",
      "[75/3000] 1 tree, 31 leaves, max depth = 11, train loss: 0.04605, val loss: 0.05388, in 0.316s\n",
      "[76/3000] 1 tree, 31 leaves, max depth = 11, train loss: 0.04588, val loss: 0.05378, in 0.263s\n",
      "[77/3000] 1 tree, 31 leaves, max depth = 14, train loss: 0.04569, val loss: 0.05366, in 0.295s\n",
      "[78/3000] 1 tree, 31 leaves, max depth = 8, train loss: 0.04547, val loss: 0.05360, in 0.425s\n",
      "[79/3000] 1 tree, 31 leaves, max depth = 11, train loss: 0.04531, val loss: 0.05355, in 0.280s\n",
      "[80/3000] 1 tree, 31 leaves, max depth = 11, train loss: 0.04510, val loss: 0.05336, in 0.301s\n",
      "[81/3000] 1 tree, 31 leaves, max depth = 11, train loss: 0.04493, val loss: 0.05326, in 0.261s\n",
      "[82/3000] 1 tree, 31 leaves, max depth = 12, train loss: 0.04473, val loss: 0.05315, in 0.292s\n",
      "[83/3000] 1 tree, 31 leaves, max depth = 9, train loss: 0.04455, val loss: 0.05304, in 0.342s\n",
      "[84/3000] 1 tree, 31 leaves, max depth = 13, train loss: 0.04439, val loss: 0.05291, in 0.264s\n",
      "[85/3000] 1 tree, 31 leaves, max depth = 14, train loss: 0.04424, val loss: 0.05289, in 0.237s\n",
      "[86/3000] 1 tree, 31 leaves, max depth = 12, train loss: 0.04405, val loss: 0.05279, in 0.274s\n",
      "[87/3000] 1 tree, 31 leaves, max depth = 10, train loss: 0.04389, val loss: 0.05271, in 0.301s\n",
      "[88/3000] 1 tree, 31 leaves, max depth = 10, train loss: 0.04369, val loss: 0.05259, in 0.312s\n",
      "[89/3000] 1 tree, 31 leaves, max depth = 12, train loss: 0.04351, val loss: 0.05245, in 0.260s\n",
      "[90/3000] 1 tree, 31 leaves, max depth = 9, train loss: 0.04336, val loss: 0.05236, in 0.325s\n",
      "[91/3000] 1 tree, 31 leaves, max depth = 11, train loss: 0.04317, val loss: 0.05228, in 0.287s\n",
      "[92/3000] 1 tree, 31 leaves, max depth = 14, train loss: 0.04302, val loss: 0.05221, in 0.253s\n",
      "[93/3000] 1 tree, 31 leaves, max depth = 14, train loss: 0.04286, val loss: 0.05206, in 0.287s\n",
      "[94/3000] 1 tree, 31 leaves, max depth = 11, train loss: 0.04270, val loss: 0.05200, in 0.313s\n",
      "[95/3000] 1 tree, 31 leaves, max depth = 11, train loss: 0.04251, val loss: 0.05185, in 0.261s\n",
      "[96/3000] 1 tree, 31 leaves, max depth = 14, train loss: 0.04236, val loss: 0.05179, in 0.292s\n",
      "[97/3000] 1 tree, 31 leaves, max depth = 14, train loss: 0.04219, val loss: 0.05168, in 0.301s\n",
      "[98/3000] 1 tree, 31 leaves, max depth = 13, train loss: 0.04204, val loss: 0.05162, in 0.423s\n",
      "[99/3000] 1 tree, 31 leaves, max depth = 11, train loss: 0.04190, val loss: 0.05152, in 0.342s\n",
      "[100/3000] 1 tree, 31 leaves, max depth = 11, train loss: 0.04177, val loss: 0.05147, in 0.240s\n",
      "[101/3000] 1 tree, 31 leaves, max depth = 10, train loss: 0.04161, val loss: 0.05138, in 0.318s\n",
      "[102/3000] 1 tree, 31 leaves, max depth = 13, train loss: 0.04142, val loss: 0.05130, in 0.261s\n",
      "[103/3000] 1 tree, 31 leaves, max depth = 13, train loss: 0.04128, val loss: 0.05119, in 0.239s\n",
      "[104/3000] 1 tree, 31 leaves, max depth = 11, train loss: 0.04113, val loss: 0.05113, in 0.279s\n",
      "[105/3000] 1 tree, 31 leaves, max depth = 12, train loss: 0.04099, val loss: 0.05102, in 0.254s\n",
      "[106/3000] 1 tree, 31 leaves, max depth = 10, train loss: 0.04085, val loss: 0.05095, in 0.272s\n",
      "[107/3000] 1 tree, 31 leaves, max depth = 10, train loss: 0.04073, val loss: 0.05090, in 0.224s\n",
      "[108/3000] 1 tree, 31 leaves, max depth = 9, train loss: 0.04059, val loss: 0.05088, in 0.318s\n",
      "[109/3000] 1 tree, 31 leaves, max depth = 9, train loss: 0.04047, val loss: 0.05085, in 0.272s\n",
      "[110/3000] 1 tree, 31 leaves, max depth = 10, train loss: 0.04034, val loss: 0.05066, in 0.260s\n",
      "[111/3000] 1 tree, 31 leaves, max depth = 14, train loss: 0.04083, val loss: 0.05141, in 0.243s\n",
      "[112/3000] 1 tree, 31 leaves, max depth = 7, train loss: 0.04019, val loss: 0.05088, in 0.199s\n",
      "[113/3000] 1 tree, 31 leaves, max depth = 9, train loss: 0.04003, val loss: 0.05073, in 0.258s\n",
      "[114/3000] 1 tree, 31 leaves, max depth = 9, train loss: 0.03987, val loss: 0.05063, in 0.286s\n",
      "[115/3000] 1 tree, 31 leaves, max depth = 13, train loss: 0.03975, val loss: 0.05055, in 0.198s\n",
      "[116/3000] 1 tree, 31 leaves, max depth = 12, train loss: 0.03962, val loss: 0.05048, in 0.276s\n",
      "[117/3000] 1 tree, 31 leaves, max depth = 12, train loss: 0.03950, val loss: 0.05047, in 0.250s\n",
      "[118/3000] 1 tree, 31 leaves, max depth = 11, train loss: 0.03938, val loss: 0.05040, in 0.222s\n",
      "[119/3000] 1 tree, 31 leaves, max depth = 10, train loss: 0.03948, val loss: 0.05072, in 0.335s\n",
      "[120/3000] 1 tree, 31 leaves, max depth = 9, train loss: 0.03922, val loss: 0.05048, in 0.253s\n",
      "[121/3000] 1 tree, 31 leaves, max depth = 11, train loss: 0.03908, val loss: 0.05040, in 0.259s\n",
      "[122/3000] 1 tree, 31 leaves, max depth = 12, train loss: 0.03894, val loss: 0.05033, in 0.273s\n",
      "[123/3000] 1 tree, 31 leaves, max depth = 15, train loss: 0.03880, val loss: 0.05027, in 0.290s\n",
      "[124/3000] 1 tree, 31 leaves, max depth = 10, train loss: 0.03870, val loss: 0.05025, in 0.225s\n",
      "[125/3000] 1 tree, 31 leaves, max depth = 12, train loss: 0.03857, val loss: 0.05019, in 0.298s\n",
      "[126/3000] 1 tree, 31 leaves, max depth = 14, train loss: 0.03846, val loss: 0.05011, in 0.241s\n",
      "[127/3000] 1 tree, 31 leaves, max depth = 11, train loss: 0.03833, val loss: 0.05002, in 0.372s\n",
      "[128/3000] 1 tree, 31 leaves, max depth = 13, train loss: 0.03815, val loss: 0.04992, in 0.340s\n",
      "[129/3000] 1 tree, 31 leaves, max depth = 13, train loss: 0.03804, val loss: 0.04987, in 0.323s\n",
      "[130/3000] 1 tree, 31 leaves, max depth = 10, train loss: 0.03792, val loss: 0.04985, in 0.301s\n",
      "[131/3000] 1 tree, 31 leaves, max depth = 11, train loss: 0.03783, val loss: 0.04983, in 0.283s\n",
      "[132/3000] 1 tree, 31 leaves, max depth = 13, train loss: 0.03769, val loss: 0.04977, in 0.242s\n",
      "[133/3000] 1 tree, 31 leaves, max depth = 12, train loss: 0.03759, val loss: 0.04976, in 0.244s\n",
      "[134/3000] 1 tree, 31 leaves, max depth = 11, train loss: 0.03749, val loss: 0.04971, in 0.188s\n",
      "[135/3000] 1 tree, 31 leaves, max depth = 10, train loss: 0.03740, val loss: 0.04969, in 0.197s\n",
      "[136/3000] 1 tree, 31 leaves, max depth = 10, train loss: 0.03729, val loss: 0.04965, in 0.242s\n",
      "[137/3000] 1 tree, 31 leaves, max depth = 11, train loss: 0.03719, val loss: 0.04963, in 0.244s\n",
      "[138/3000] 1 tree, 31 leaves, max depth = 13, train loss: 0.03708, val loss: 0.04957, in 0.410s\n",
      "[139/3000] 1 tree, 31 leaves, max depth = 10, train loss: 0.03699, val loss: 0.04954, in 0.260s\n",
      "[140/3000] 1 tree, 31 leaves, max depth = 11, train loss: 0.03689, val loss: 0.04947, in 0.316s\n",
      "[141/3000] 1 tree, 31 leaves, max depth = 9, train loss: 0.03679, val loss: 0.04942, in 0.248s\n",
      "[142/3000] 1 tree, 31 leaves, max depth = 13, train loss: 0.03670, val loss: 0.04935, in 0.248s\n",
      "[143/3000] 1 tree, 31 leaves, max depth = 12, train loss: 0.03659, val loss: 0.04931, in 0.255s\n",
      "[144/3000] 1 tree, 31 leaves, max depth = 11, train loss: 0.03647, val loss: 0.04926, in 0.229s\n",
      "[145/3000] 1 tree, 31 leaves, max depth = 13, train loss: 0.03637, val loss: 0.04925, in 0.267s\n",
      "[146/3000] 1 tree, 31 leaves, max depth = 10, train loss: 0.03626, val loss: 0.04918, in 0.292s\n",
      "[147/3000] 1 tree, 31 leaves, max depth = 12, train loss: 0.03615, val loss: 0.04912, in 0.192s\n",
      "[148/3000] 1 tree, 31 leaves, max depth = 12, train loss: 0.03607, val loss: 0.04912, in 0.213s\n",
      "[149/3000] 1 tree, 31 leaves, max depth = 12, train loss: 0.03599, val loss: 0.04908, in 0.216s\n",
      "[150/3000] 1 tree, 31 leaves, max depth = 11, train loss: 0.03589, val loss: 0.04908, in 0.200s\n",
      "[151/3000] 1 tree, 31 leaves, max depth = 10, train loss: 0.03581, val loss: 0.04904, in 0.230s\n",
      "[152/3000] 1 tree, 31 leaves, max depth = 11, train loss: 0.03572, val loss: 0.04904, in 0.168s\n",
      "[153/3000] 1 tree, 31 leaves, max depth = 11, train loss: 0.03563, val loss: 0.04902, in 0.239s\n",
      "[154/3000] 1 tree, 31 leaves, max depth = 10, train loss: 0.03555, val loss: 0.04900, in 0.195s\n",
      "[155/3000] 1 tree, 31 leaves, max depth = 10, train loss: 0.03548, val loss: 0.04898, in 0.195s\n",
      "[156/3000] 1 tree, 31 leaves, max depth = 9, train loss: 0.03538, val loss: 0.04892, in 0.308s\n",
      "[157/3000] 1 tree, 31 leaves, max depth = 9, train loss: 0.03528, val loss: 0.04889, in 0.236s\n",
      "[158/3000] 1 tree, 31 leaves, max depth = 14, train loss: 0.03518, val loss: 0.04891, in 0.416s\n",
      "[159/3000] 1 tree, 31 leaves, max depth = 11, train loss: 0.03506, val loss: 0.04888, in 0.295s\n",
      "[160/3000] 1 tree, 31 leaves, max depth = 14, train loss: 0.03498, val loss: 0.04885, in 0.249s\n",
      "[161/3000] 1 tree, 31 leaves, max depth = 10, train loss: 0.03488, val loss: 0.04881, in 0.216s\n",
      "[162/3000] 1 tree, 31 leaves, max depth = 13, train loss: 0.03481, val loss: 0.04880, in 0.197s\n",
      "[163/3000] 1 tree, 31 leaves, max depth = 10, train loss: 0.03473, val loss: 0.04875, in 0.208s\n",
      "[164/3000] 1 tree, 31 leaves, max depth = 11, train loss: 0.03466, val loss: 0.04872, in 0.200s\n",
      "[165/3000] 1 tree, 31 leaves, max depth = 12, train loss: 0.03457, val loss: 0.04870, in 0.226s\n",
      "[166/3000] 1 tree, 31 leaves, max depth = 12, train loss: 0.03448, val loss: 0.04863, in 0.200s\n",
      "[167/3000] 1 tree, 31 leaves, max depth = 11, train loss: 0.03437, val loss: 0.04856, in 0.290s\n",
      "[168/3000] 1 tree, 31 leaves, max depth = 16, train loss: 0.03429, val loss: 0.04854, in 0.208s\n",
      "[169/3000] 1 tree, 31 leaves, max depth = 10, train loss: 0.03420, val loss: 0.04851, in 0.199s\n",
      "[170/3000] 1 tree, 31 leaves, max depth = 10, train loss: 0.03412, val loss: 0.04842, in 0.230s\n",
      "[171/3000] 1 tree, 31 leaves, max depth = 11, train loss: 0.03406, val loss: 0.04840, in 0.221s\n",
      "[172/3000] 1 tree, 31 leaves, max depth = 11, train loss: 0.03397, val loss: 0.04838, in 0.179s\n",
      "[173/3000] 1 tree, 31 leaves, max depth = 9, train loss: 0.03389, val loss: 0.04838, in 0.237s\n",
      "[174/3000] 1 tree, 31 leaves, max depth = 11, train loss: 0.03381, val loss: 0.04835, in 0.164s\n",
      "[175/3000] 1 tree, 31 leaves, max depth = 9, train loss: 0.03370, val loss: 0.04829, in 0.237s\n",
      "[176/3000] 1 tree, 31 leaves, max depth = 11, train loss: 0.03362, val loss: 0.04820, in 0.225s\n",
      "[177/3000] 1 tree, 31 leaves, max depth = 9, train loss: 0.03353, val loss: 0.04818, in 0.216s\n",
      "[178/3000] 1 tree, 31 leaves, max depth = 11, train loss: 0.03344, val loss: 0.04815, in 0.329s\n",
      "[179/3000] 1 tree, 31 leaves, max depth = 9, train loss: 0.03336, val loss: 0.04816, in 0.249s\n",
      "[180/3000] 1 tree, 31 leaves, max depth = 13, train loss: 0.03328, val loss: 0.04812, in 0.269s\n",
      "[181/3000] 1 tree, 31 leaves, max depth = 9, train loss: 0.03320, val loss: 0.04810, in 0.240s\n",
      "[182/3000] 1 tree, 31 leaves, max depth = 12, train loss: 0.03311, val loss: 0.04808, in 0.209s\n",
      "[183/3000] 1 tree, 31 leaves, max depth = 8, train loss: 0.03303, val loss: 0.04807, in 0.201s\n",
      "[184/3000] 1 tree, 31 leaves, max depth = 13, train loss: 0.03294, val loss: 0.04806, in 0.268s\n",
      "[185/3000] 1 tree, 31 leaves, max depth = 14, train loss: 0.03286, val loss: 0.04805, in 0.201s\n",
      "[186/3000] 1 tree, 31 leaves, max depth = 12, train loss: 0.03278, val loss: 0.04805, in 0.179s\n",
      "[187/3000] 1 tree, 31 leaves, max depth = 9, train loss: 0.03269, val loss: 0.04799, in 0.194s\n",
      "[188/3000] 1 tree, 31 leaves, max depth = 13, train loss: 0.03263, val loss: 0.04796, in 0.211s\n",
      "[189/3000] 1 tree, 31 leaves, max depth = 10, train loss: 0.03255, val loss: 0.04796, in 0.224s\n",
      "[190/3000] 1 tree, 31 leaves, max depth = 9, train loss: 0.03247, val loss: 0.04791, in 0.258s\n",
      "[191/3000] 1 tree, 31 leaves, max depth = 10, train loss: 0.03241, val loss: 0.04791, in 0.219s\n",
      "[192/3000] 1 tree, 31 leaves, max depth = 9, train loss: 0.03232, val loss: 0.04789, in 0.227s\n",
      "[193/3000] 1 tree, 31 leaves, max depth = 11, train loss: 0.03226, val loss: 0.04787, in 0.284s\n",
      "[194/3000] 1 tree, 31 leaves, max depth = 9, train loss: 0.03219, val loss: 0.04787, in 0.238s\n",
      "[195/3000] 1 tree, 31 leaves, max depth = 11, train loss: 0.03213, val loss: 0.04786, in 0.197s\n",
      "[196/3000] 1 tree, 31 leaves, max depth = 17, train loss: 0.03207, val loss: 0.04784, in 0.212s\n",
      "[197/3000] 1 tree, 31 leaves, max depth = 11, train loss: 0.03201, val loss: 0.04782, in 0.206s\n",
      "[198/3000] 1 tree, 31 leaves, max depth = 11, train loss: 0.03194, val loss: 0.04778, in 0.434s\n",
      "[199/3000] 1 tree, 31 leaves, max depth = 14, train loss: 0.03187, val loss: 0.04777, in 0.225s\n",
      "[200/3000] 1 tree, 31 leaves, max depth = 10, train loss: 0.03180, val loss: 0.04773, in 0.248s\n",
      "[201/3000] 1 tree, 31 leaves, max depth = 10, train loss: 0.03174, val loss: 0.04770, in 0.260s\n",
      "[202/3000] 1 tree, 31 leaves, max depth = 10, train loss: 0.03166, val loss: 0.04771, in 0.188s\n",
      "[203/3000] 1 tree, 31 leaves, max depth = 13, train loss: 0.03158, val loss: 0.04773, in 0.191s\n",
      "[204/3000] 1 tree, 31 leaves, max depth = 13, train loss: 0.03151, val loss: 0.04770, in 0.186s\n",
      "[205/3000] 1 tree, 31 leaves, max depth = 14, train loss: 0.03146, val loss: 0.04771, in 0.192s\n",
      "[206/3000] 1 tree, 31 leaves, max depth = 11, train loss: 0.03139, val loss: 0.04770, in 0.217s\n",
      "[207/3000] 1 tree, 31 leaves, max depth = 11, train loss: 0.03132, val loss: 0.04770, in 0.204s\n",
      "[208/3000] 1 tree, 31 leaves, max depth = 10, train loss: 0.03123, val loss: 0.04766, in 0.285s\n",
      "[209/3000] 1 tree, 31 leaves, max depth = 10, train loss: 0.03116, val loss: 0.04767, in 0.254s\n",
      "[210/3000] 1 tree, 31 leaves, max depth = 14, train loss: 0.03107, val loss: 0.04764, in 0.296s\n",
      "[211/3000] 1 tree, 31 leaves, max depth = 11, train loss: 0.03099, val loss: 0.04762, in 0.229s\n",
      "[212/3000] 1 tree, 31 leaves, max depth = 12, train loss: 0.03093, val loss: 0.04764, in 0.178s\n",
      "[213/3000] 1 tree, 31 leaves, max depth = 12, train loss: 0.03087, val loss: 0.04764, in 0.252s\n",
      "[214/3000] 1 tree, 31 leaves, max depth = 11, train loss: 0.03081, val loss: 0.04761, in 0.217s\n",
      "[215/3000] 1 tree, 31 leaves, max depth = 11, train loss: 0.03075, val loss: 0.04762, in 0.209s\n",
      "[216/3000] 1 tree, 31 leaves, max depth = 8, train loss: 0.03068, val loss: 0.04759, in 0.238s\n",
      "[217/3000] 1 tree, 31 leaves, max depth = 12, train loss: 0.03061, val loss: 0.04757, in 0.179s\n",
      "[218/3000] 1 tree, 31 leaves, max depth = 9, train loss: 0.03055, val loss: 0.04757, in 0.413s\n",
      "[219/3000] 1 tree, 31 leaves, max depth = 14, train loss: 0.03048, val loss: 0.04754, in 0.242s\n",
      "[220/3000] 1 tree, 31 leaves, max depth = 12, train loss: 0.03043, val loss: 0.04753, in 0.197s\n",
      "[221/3000] 1 tree, 31 leaves, max depth = 10, train loss: 0.03035, val loss: 0.04753, in 0.260s\n",
      "[222/3000] 1 tree, 31 leaves, max depth = 13, train loss: 0.03028, val loss: 0.04749, in 0.298s\n",
      "[223/3000] 1 tree, 31 leaves, max depth = 11, train loss: 0.03022, val loss: 0.04747, in 0.229s\n",
      "[224/3000] 1 tree, 31 leaves, max depth = 10, train loss: 0.03014, val loss: 0.04747, in 0.201s\n",
      "[225/3000] 1 tree, 31 leaves, max depth = 11, train loss: 0.03006, val loss: 0.04746, in 0.172s\n",
      "[226/3000] 1 tree, 31 leaves, max depth = 9, train loss: 0.02999, val loss: 0.04744, in 0.244s\n",
      "[227/3000] 1 tree, 31 leaves, max depth = 9, train loss: 0.02994, val loss: 0.04743, in 0.182s\n",
      "[228/3000] 1 tree, 31 leaves, max depth = 13, train loss: 0.02984, val loss: 0.04737, in 0.294s\n",
      "[229/3000] 1 tree, 31 leaves, max depth = 11, train loss: 0.02979, val loss: 0.04739, in 0.170s\n",
      "[230/3000] 1 tree, 31 leaves, max depth = 8, train loss: 0.02972, val loss: 0.04734, in 0.224s\n",
      "[231/3000] 1 tree, 31 leaves, max depth = 14, train loss: 0.02965, val loss: 0.04732, in 0.225s\n",
      "[232/3000] 1 tree, 31 leaves, max depth = 11, train loss: 0.02957, val loss: 0.04729, in 0.210s\n",
      "[233/3000] 1 tree, 31 leaves, max depth = 9, train loss: 0.02952, val loss: 0.04730, in 0.196s\n",
      "[234/3000] 1 tree, 31 leaves, max depth = 8, train loss: 0.02946, val loss: 0.04729, in 0.202s\n",
      "[235/3000] 1 tree, 31 leaves, max depth = 12, train loss: 0.02940, val loss: 0.04728, in 0.193s\n",
      "[236/3000] 1 tree, 31 leaves, max depth = 11, train loss: 0.02931, val loss: 0.04722, in 0.243s\n",
      "[237/3000] 1 tree, 31 leaves, max depth = 12, train loss: 0.02925, val loss: 0.04720, in 0.217s\n",
      "[238/3000] 1 tree, 31 leaves, max depth = 14, train loss: 0.02920, val loss: 0.04721, in 0.337s\n",
      "[239/3000] 1 tree, 31 leaves, max depth = 12, train loss: 0.02915, val loss: 0.04722, in 0.199s\n",
      "[240/3000] 1 tree, 31 leaves, max depth = 13, train loss: 0.02907, val loss: 0.04723, in 0.282s\n",
      "[241/3000] 1 tree, 31 leaves, max depth = 9, train loss: 0.02902, val loss: 0.04722, in 0.211s\n",
      "[242/3000] 1 tree, 31 leaves, max depth = 11, train loss: 0.02895, val loss: 0.04720, in 0.240s\n",
      "[243/3000] 1 tree, 31 leaves, max depth = 10, train loss: 0.02889, val loss: 0.04720, in 0.220s\n",
      "[244/3000] 1 tree, 31 leaves, max depth = 11, train loss: 0.02882, val loss: 0.04720, in 0.219s\n",
      "[245/3000] 1 tree, 31 leaves, max depth = 9, train loss: 0.02876, val loss: 0.04719, in 0.182s\n",
      "[246/3000] 1 tree, 31 leaves, max depth = 13, train loss: 0.02870, val loss: 0.04715, in 0.197s\n",
      "[247/3000] 1 tree, 31 leaves, max depth = 11, train loss: 0.02864, val loss: 0.04714, in 0.183s\n",
      "[248/3000] 1 tree, 31 leaves, max depth = 14, train loss: 0.02860, val loss: 0.04714, in 0.160s\n",
      "[249/3000] 1 tree, 31 leaves, max depth = 11, train loss: 0.02854, val loss: 0.04711, in 0.241s\n",
      "[250/3000] 1 tree, 31 leaves, max depth = 12, train loss: 0.02847, val loss: 0.04704, in 0.233s\n",
      "[251/3000] 1 tree, 31 leaves, max depth = 9, train loss: 0.02840, val loss: 0.04705, in 0.257s\n",
      "[252/3000] 1 tree, 31 leaves, max depth = 12, train loss: 0.02835, val loss: 0.04704, in 0.177s\n",
      "[253/3000] 1 tree, 31 leaves, max depth = 9, train loss: 0.02830, val loss: 0.04703, in 0.200s\n",
      "[254/3000] 1 tree, 31 leaves, max depth = 8, train loss: 0.02826, val loss: 0.04704, in 0.194s\n",
      "[255/3000] 1 tree, 31 leaves, max depth = 10, train loss: 0.02819, val loss: 0.04704, in 0.206s\n",
      "[256/3000] 1 tree, 31 leaves, max depth = 11, train loss: 0.02814, val loss: 0.04703, in 0.176s\n",
      "[257/3000] 1 tree, 31 leaves, max depth = 11, train loss: 0.02806, val loss: 0.04701, in 0.387s\n",
      "[258/3000] 1 tree, 31 leaves, max depth = 12, train loss: 0.02800, val loss: 0.04700, in 0.282s\n",
      "[259/3000] 1 tree, 31 leaves, max depth = 10, train loss: 0.02795, val loss: 0.04697, in 0.241s\n",
      "[260/3000] 1 tree, 31 leaves, max depth = 17, train loss: 0.02790, val loss: 0.04696, in 0.178s\n",
      "[261/3000] 1 tree, 31 leaves, max depth = 10, train loss: 0.02784, val loss: 0.04692, in 0.244s\n",
      "[262/3000] 1 tree, 31 leaves, max depth = 12, train loss: 0.02777, val loss: 0.04691, in 0.300s\n",
      "[263/3000] 1 tree, 31 leaves, max depth = 9, train loss: 0.02772, val loss: 0.04690, in 0.192s\n",
      "[264/3000] 1 tree, 31 leaves, max depth = 11, train loss: 0.02767, val loss: 0.04690, in 0.228s\n",
      "[265/3000] 1 tree, 31 leaves, max depth = 12, train loss: 0.02761, val loss: 0.04690, in 0.207s\n",
      "[266/3000] 1 tree, 31 leaves, max depth = 9, train loss: 0.02755, val loss: 0.04689, in 0.204s\n",
      "[267/3000] 1 tree, 31 leaves, max depth = 10, train loss: 0.02749, val loss: 0.04687, in 0.226s\n",
      "[268/3000] 1 tree, 31 leaves, max depth = 11, train loss: 0.02745, val loss: 0.04686, in 0.221s\n",
      "[269/3000] 1 tree, 31 leaves, max depth = 10, train loss: 0.02739, val loss: 0.04687, in 0.207s\n",
      "[270/3000] 1 tree, 31 leaves, max depth = 10, train loss: 0.02735, val loss: 0.04688, in 0.171s\n",
      "[271/3000] 1 tree, 31 leaves, max depth = 9, train loss: 0.02730, val loss: 0.04686, in 0.265s\n",
      "[272/3000] 1 tree, 31 leaves, max depth = 9, train loss: 0.02725, val loss: 0.04685, in 0.176s\n",
      "[273/3000] 1 tree, 31 leaves, max depth = 10, train loss: 0.02720, val loss: 0.04684, in 0.215s\n",
      "[274/3000] 1 tree, 31 leaves, max depth = 13, train loss: 0.02715, val loss: 0.04680, in 0.205s\n",
      "[275/3000] 1 tree, 31 leaves, max depth = 10, train loss: 0.02709, val loss: 0.04676, in 0.251s\n",
      "[276/3000] 1 tree, 31 leaves, max depth = 11, train loss: 0.02704, val loss: 0.04675, in 0.213s\n",
      "[277/3000] 1 tree, 31 leaves, max depth = 9, train loss: 0.02697, val loss: 0.04675, in 0.266s\n",
      "[278/3000] 1 tree, 31 leaves, max depth = 9, train loss: 0.02690, val loss: 0.04672, in 0.383s\n",
      "[279/3000] 1 tree, 31 leaves, max depth = 11, train loss: 0.02685, val loss: 0.04672, in 0.215s\n",
      "[280/3000] 1 tree, 31 leaves, max depth = 8, train loss: 0.02679, val loss: 0.04670, in 0.186s\n",
      "[281/3000] 1 tree, 31 leaves, max depth = 10, train loss: 0.02673, val loss: 0.04671, in 0.228s\n",
      "[282/3000] 1 tree, 31 leaves, max depth = 12, train loss: 0.02668, val loss: 0.04669, in 0.189s\n",
      "[283/3000] 1 tree, 31 leaves, max depth = 13, train loss: 0.02664, val loss: 0.04669, in 0.177s\n",
      "[284/3000] 1 tree, 31 leaves, max depth = 9, train loss: 0.02657, val loss: 0.04669, in 0.215s\n",
      "[285/3000] 1 tree, 31 leaves, max depth = 17, train loss: 0.02651, val loss: 0.04667, in 0.191s\n",
      "[286/3000] 1 tree, 31 leaves, max depth = 10, train loss: 0.02645, val loss: 0.04664, in 0.201s\n",
      "[287/3000] 1 tree, 31 leaves, max depth = 12, train loss: 0.02639, val loss: 0.04662, in 0.185s\n",
      "[288/3000] 1 tree, 31 leaves, max depth = 10, train loss: 0.02635, val loss: 0.04663, in 0.182s\n",
      "[289/3000] 1 tree, 31 leaves, max depth = 10, train loss: 0.02629, val loss: 0.04660, in 0.196s\n",
      "[290/3000] 1 tree, 31 leaves, max depth = 11, train loss: 0.02623, val loss: 0.04657, in 0.229s\n",
      "[291/3000] 1 tree, 31 leaves, max depth = 8, train loss: 0.02618, val loss: 0.04659, in 0.206s\n",
      "[292/3000] 1 tree, 31 leaves, max depth = 12, train loss: 0.02611, val loss: 0.04657, in 0.222s\n",
      "[293/3000] 1 tree, 31 leaves, max depth = 13, train loss: 0.02606, val loss: 0.04654, in 0.206s\n",
      "[294/3000] 1 tree, 31 leaves, max depth = 9, train loss: 0.02601, val loss: 0.04654, in 0.159s\n",
      "[295/3000] 1 tree, 31 leaves, max depth = 12, train loss: 0.02595, val loss: 0.04649, in 0.227s\n",
      "[296/3000] 1 tree, 31 leaves, max depth = 10, train loss: 0.02590, val loss: 0.04648, in 0.259s\n",
      "[297/3000] 1 tree, 31 leaves, max depth = 16, train loss: 0.02585, val loss: 0.04648, in 0.192s\n",
      "[298/3000] 1 tree, 31 leaves, max depth = 10, train loss: 0.02580, val loss: 0.04650, in 0.346s\n",
      "[299/3000] 1 tree, 31 leaves, max depth = 10, train loss: 0.02575, val loss: 0.04648, in 0.231s\n",
      "[300/3000] 1 tree, 31 leaves, max depth = 10, train loss: 0.02571, val loss: 0.04648, in 0.191s\n",
      "[301/3000] 1 tree, 31 leaves, max depth = 12, train loss: 0.02566, val loss: 0.04648, in 0.247s\n",
      "[302/3000] 1 tree, 31 leaves, max depth = 12, train loss: 0.02560, val loss: 0.04647, in 0.193s\n",
      "[303/3000] 1 tree, 31 leaves, max depth = 10, train loss: 0.02555, val loss: 0.04646, in 0.194s\n",
      "[304/3000] 1 tree, 31 leaves, max depth = 9, train loss: 0.02549, val loss: 0.04645, in 0.182s\n",
      "[305/3000] 1 tree, 31 leaves, max depth = 10, train loss: 0.02545, val loss: 0.04645, in 0.165s\n",
      "[306/3000] 1 tree, 31 leaves, max depth = 11, train loss: 0.02539, val loss: 0.04643, in 0.225s\n",
      "[307/3000] 1 tree, 31 leaves, max depth = 11, train loss: 0.02534, val loss: 0.04642, in 0.186s\n",
      "[308/3000] 1 tree, 31 leaves, max depth = 12, train loss: 0.02528, val loss: 0.04641, in 0.260s\n",
      "[309/3000] 1 tree, 31 leaves, max depth = 13, train loss: 0.02523, val loss: 0.04643, in 0.178s\n",
      "[310/3000] 1 tree, 31 leaves, max depth = 11, train loss: 0.02518, val loss: 0.04642, in 0.200s\n",
      "[311/3000] 1 tree, 31 leaves, max depth = 16, train loss: 0.02514, val loss: 0.04643, in 0.226s\n",
      "[312/3000] 1 tree, 31 leaves, max depth = 11, train loss: 0.02508, val loss: 0.04640, in 0.205s\n",
      "[313/3000] 1 tree, 31 leaves, max depth = 17, train loss: 0.02503, val loss: 0.04639, in 0.181s\n",
      "[314/3000] 1 tree, 31 leaves, max depth = 10, train loss: 0.02499, val loss: 0.04637, in 0.219s\n",
      "[315/3000] 1 tree, 31 leaves, max depth = 12, train loss: 0.02496, val loss: 0.04638, in 0.178s\n",
      "[316/3000] 1 tree, 31 leaves, max depth = 11, train loss: 0.02491, val loss: 0.04638, in 0.241s\n",
      "[317/3000] 1 tree, 31 leaves, max depth = 11, train loss: 0.02487, val loss: 0.04637, in 0.213s\n",
      "[318/3000] 1 tree, 31 leaves, max depth = 13, train loss: 0.02482, val loss: 0.04638, in 0.364s\n",
      "[319/3000] 1 tree, 31 leaves, max depth = 16, train loss: 0.02477, val loss: 0.04637, in 0.230s\n",
      "[320/3000] 1 tree, 31 leaves, max depth = 15, train loss: 0.02472, val loss: 0.04635, in 0.210s\n",
      "[321/3000] 1 tree, 31 leaves, max depth = 9, train loss: 0.02466, val loss: 0.04633, in 0.275s\n",
      "[322/3000] 1 tree, 31 leaves, max depth = 13, train loss: 0.02462, val loss: 0.04632, in 0.205s\n",
      "[323/3000] 1 tree, 31 leaves, max depth = 12, train loss: 0.02458, val loss: 0.04632, in 0.215s\n",
      "[324/3000] 1 tree, 31 leaves, max depth = 10, train loss: 0.02454, val loss: 0.04631, in 0.175s\n",
      "[325/3000] 1 tree, 31 leaves, max depth = 11, train loss: 0.02449, val loss: 0.04631, in 0.170s\n",
      "[326/3000] 1 tree, 31 leaves, max depth = 11, train loss: 0.02444, val loss: 0.04627, in 0.209s\n",
      "[327/3000] 1 tree, 31 leaves, max depth = 10, train loss: 0.02438, val loss: 0.04626, in 0.201s\n",
      "[328/3000] 1 tree, 31 leaves, max depth = 11, train loss: 0.02434, val loss: 0.04627, in 0.208s\n",
      "[329/3000] 1 tree, 31 leaves, max depth = 12, train loss: 0.02430, val loss: 0.04628, in 0.166s\n",
      "[330/3000] 1 tree, 31 leaves, max depth = 10, train loss: 0.02425, val loss: 0.04626, in 0.209s\n",
      "[331/3000] 1 tree, 31 leaves, max depth = 10, train loss: 0.02419, val loss: 0.04622, in 0.204s\n",
      "[332/3000] 1 tree, 31 leaves, max depth = 9, train loss: 0.02414, val loss: 0.04623, in 0.176s\n",
      "[333/3000] 1 tree, 31 leaves, max depth = 11, train loss: 0.02410, val loss: 0.04621, in 0.209s\n",
      "[334/3000] 1 tree, 31 leaves, max depth = 9, train loss: 0.02405, val loss: 0.04622, in 0.223s\n",
      "[335/3000] 1 tree, 31 leaves, max depth = 10, train loss: 0.02401, val loss: 0.04620, in 0.181s\n",
      "[336/3000] 1 tree, 31 leaves, max depth = 10, train loss: 0.02395, val loss: 0.04620, in 0.204s\n",
      "[337/3000] 1 tree, 31 leaves, max depth = 11, train loss: 0.02390, val loss: 0.04621, in 0.283s\n",
      "[338/3000] 1 tree, 31 leaves, max depth = 10, train loss: 0.02386, val loss: 0.04623, in 0.343s\n",
      "[339/3000] 1 tree, 31 leaves, max depth = 10, train loss: 0.02381, val loss: 0.04622, in 0.512s\n",
      "[340/3000] 1 tree, 31 leaves, max depth = 8, train loss: 0.02376, val loss: 0.04621, in 0.303s\n",
      "[341/3000] 1 tree, 31 leaves, max depth = 9, train loss: 0.02370, val loss: 0.04619, in 0.235s\n",
      "[342/3000] 1 tree, 31 leaves, max depth = 10, train loss: 0.02365, val loss: 0.04618, in 0.220s\n",
      "[343/3000] 1 tree, 31 leaves, max depth = 13, train loss: 0.02360, val loss: 0.04617, in 0.230s\n",
      "[344/3000] 1 tree, 31 leaves, max depth = 10, train loss: 0.02356, val loss: 0.04615, in 0.222s\n",
      "[345/3000] 1 tree, 31 leaves, max depth = 10, train loss: 0.02352, val loss: 0.04616, in 0.202s\n",
      "[346/3000] 1 tree, 31 leaves, max depth = 15, train loss: 0.02347, val loss: 0.04616, in 0.213s\n",
      "[347/3000] 1 tree, 31 leaves, max depth = 12, train loss: 0.02344, val loss: 0.04615, in 0.163s\n",
      "[348/3000] 1 tree, 31 leaves, max depth = 14, train loss: 0.02338, val loss: 0.04614, in 0.179s\n",
      "[349/3000] 1 tree, 31 leaves, max depth = 10, train loss: 0.02335, val loss: 0.04613, in 0.196s\n",
      "[350/3000] 1 tree, 31 leaves, max depth = 15, train loss: 0.02329, val loss: 0.04614, in 0.191s\n",
      "[351/3000] 1 tree, 31 leaves, max depth = 11, train loss: 0.02325, val loss: 0.04612, in 0.243s\n",
      "[352/3000] 1 tree, 31 leaves, max depth = 10, train loss: 0.02322, val loss: 0.04611, in 0.168s\n",
      "[353/3000] 1 tree, 31 leaves, max depth = 13, train loss: 0.02318, val loss: 0.04610, in 0.181s\n",
      "[354/3000] 1 tree, 31 leaves, max depth = 12, train loss: 0.02314, val loss: 0.04613, in 0.177s\n",
      "[355/3000] 1 tree, 31 leaves, max depth = 10, train loss: 0.02310, val loss: 0.04615, in 0.173s\n",
      "[356/3000] 1 tree, 31 leaves, max depth = 12, train loss: 0.02304, val loss: 0.04616, in 0.238s\n",
      "[357/3000] 1 tree, 31 leaves, max depth = 11, train loss: 0.02299, val loss: 0.04615, in 0.230s\n",
      "[358/3000] 1 tree, 31 leaves, max depth = 12, train loss: 0.02294, val loss: 0.04613, in 0.184s\n",
      "[359/3000] 1 tree, 31 leaves, max depth = 11, train loss: 0.02288, val loss: 0.04610, in 0.376s\n",
      "[360/3000] 1 tree, 31 leaves, max depth = 11, train loss: 0.02284, val loss: 0.04611, in 0.216s\n",
      "[361/3000] 1 tree, 31 leaves, max depth = 9, train loss: 0.02280, val loss: 0.04612, in 0.229s\n",
      "[362/3000] 1 tree, 31 leaves, max depth = 8, train loss: 0.02276, val loss: 0.04611, in 0.233s\n",
      "[363/3000] 1 tree, 31 leaves, max depth = 13, train loss: 0.02273, val loss: 0.04608, in 0.209s\n",
      "[364/3000] 1 tree, 31 leaves, max depth = 9, train loss: 0.02268, val loss: 0.04609, in 0.295s\n",
      "[365/3000] 1 tree, 31 leaves, max depth = 14, train loss: 0.02264, val loss: 0.04607, in 0.234s\n",
      "[366/3000] 1 tree, 31 leaves, max depth = 10, train loss: 0.02259, val loss: 0.04606, in 0.197s\n",
      "[367/3000] 1 tree, 31 leaves, max depth = 9, train loss: 0.02256, val loss: 0.04604, in 0.179s\n",
      "[368/3000] 1 tree, 31 leaves, max depth = 11, train loss: 0.02251, val loss: 0.04604, in 0.178s\n",
      "[369/3000] 1 tree, 31 leaves, max depth = 12, train loss: 0.02247, val loss: 0.04603, in 0.249s\n",
      "[370/3000] 1 tree, 31 leaves, max depth = 11, train loss: 0.02243, val loss: 0.04601, in 0.182s\n",
      "[371/3000] 1 tree, 31 leaves, max depth = 10, train loss: 0.02238, val loss: 0.04603, in 0.242s\n",
      "[372/3000] 1 tree, 31 leaves, max depth = 14, train loss: 0.02233, val loss: 0.04600, in 0.201s\n",
      "[373/3000] 1 tree, 31 leaves, max depth = 12, train loss: 0.02230, val loss: 0.04597, in 0.178s\n",
      "[374/3000] 1 tree, 31 leaves, max depth = 10, train loss: 0.02227, val loss: 0.04598, in 0.202s\n",
      "[375/3000] 1 tree, 31 leaves, max depth = 10, train loss: 0.02223, val loss: 0.04599, in 0.162s\n",
      "[376/3000] 1 tree, 31 leaves, max depth = 10, train loss: 0.02219, val loss: 0.04598, in 0.224s\n",
      "[377/3000] 1 tree, 31 leaves, max depth = 12, train loss: 0.02214, val loss: 0.04598, in 0.193s\n",
      "[378/3000] 1 tree, 31 leaves, max depth = 11, train loss: 0.02208, val loss: 0.04598, in 0.217s\n",
      "[379/3000] 1 tree, 31 leaves, max depth = 12, train loss: 0.02205, val loss: 0.04598, in 0.323s\n",
      "[380/3000] 1 tree, 31 leaves, max depth = 9, train loss: 0.02200, val loss: 0.04598, in 0.274s\n",
      "[381/3000] 1 tree, 31 leaves, max depth = 14, train loss: 0.02198, val loss: 0.04596, in 0.188s\n",
      "[382/3000] 1 tree, 31 leaves, max depth = 13, train loss: 0.02193, val loss: 0.04594, in 0.199s\n",
      "[383/3000] 1 tree, 31 leaves, max depth = 11, train loss: 0.02189, val loss: 0.04596, in 0.179s\n",
      "[384/3000] 1 tree, 31 leaves, max depth = 11, train loss: 0.02185, val loss: 0.04593, in 0.185s\n",
      "[385/3000] 1 tree, 31 leaves, max depth = 14, train loss: 0.02179, val loss: 0.04587, in 0.278s\n",
      "[386/3000] 1 tree, 31 leaves, max depth = 12, train loss: 0.02175, val loss: 0.04585, in 0.195s\n",
      "[387/3000] 1 tree, 31 leaves, max depth = 11, train loss: 0.02171, val loss: 0.04587, in 0.225s\n",
      "[388/3000] 1 tree, 31 leaves, max depth = 9, train loss: 0.02168, val loss: 0.04588, in 0.193s\n",
      "[389/3000] 1 tree, 31 leaves, max depth = 15, train loss: 0.02165, val loss: 0.04589, in 0.184s\n",
      "[390/3000] 1 tree, 31 leaves, max depth = 12, train loss: 0.02161, val loss: 0.04586, in 0.168s\n",
      "[391/3000] 1 tree, 31 leaves, max depth = 12, train loss: 0.02158, val loss: 0.04585, in 0.173s\n",
      "[392/3000] 1 tree, 31 leaves, max depth = 10, train loss: 0.02155, val loss: 0.04585, in 0.194s\n",
      "[393/3000] 1 tree, 31 leaves, max depth = 10, train loss: 0.02151, val loss: 0.04583, in 0.203s\n",
      "[394/3000] 1 tree, 31 leaves, max depth = 15, train loss: 0.02148, val loss: 0.04584, in 0.180s\n",
      "[395/3000] 1 tree, 31 leaves, max depth = 12, train loss: 0.02144, val loss: 0.04584, in 0.191s\n",
      "[396/3000] 1 tree, 31 leaves, max depth = 12, train loss: 0.02140, val loss: 0.04584, in 0.189s\n",
      "[397/3000] 1 tree, 31 leaves, max depth = 12, train loss: 0.02136, val loss: 0.04584, in 0.213s\n",
      "[398/3000] 1 tree, 31 leaves, max depth = 11, train loss: 0.02130, val loss: 0.04581, in 0.223s\n",
      "[399/3000] 1 tree, 31 leaves, max depth = 14, train loss: 0.02126, val loss: 0.04582, in 0.312s\n",
      "[400/3000] 1 tree, 31 leaves, max depth = 9, train loss: 0.02123, val loss: 0.04584, in 0.218s\n",
      "[401/3000] 1 tree, 31 leaves, max depth = 10, train loss: 0.02119, val loss: 0.04582, in 0.200s\n",
      "[402/3000] 1 tree, 31 leaves, max depth = 11, train loss: 0.02116, val loss: 0.04584, in 0.245s\n",
      "[403/3000] 1 tree, 31 leaves, max depth = 8, train loss: 0.02113, val loss: 0.04583, in 0.215s\n",
      "[404/3000] 1 tree, 31 leaves, max depth = 15, train loss: 0.02110, val loss: 0.04584, in 0.247s\n",
      "[405/3000] 1 tree, 31 leaves, max depth = 13, train loss: 0.02105, val loss: 0.04583, in 0.243s\n",
      "[406/3000] 1 tree, 31 leaves, max depth = 10, train loss: 0.02100, val loss: 0.04582, in 0.250s\n",
      "[407/3000] 1 tree, 31 leaves, max depth = 11, train loss: 0.02096, val loss: 0.04582, in 0.269s\n",
      "[408/3000] 1 tree, 31 leaves, max depth = 12, train loss: 0.02092, val loss: 0.04581, in 0.228s\n",
      "[409/3000] 1 tree, 31 leaves, max depth = 16, train loss: 0.02087, val loss: 0.04582, in 0.206s\n",
      "[410/3000] 1 tree, 31 leaves, max depth = 14, train loss: 0.02083, val loss: 0.04582, in 0.212s\n",
      "[411/3000] 1 tree, 31 leaves, max depth = 12, train loss: 0.02079, val loss: 0.04580, in 0.222s\n",
      "[412/3000] 1 tree, 31 leaves, max depth = 10, train loss: 0.02075, val loss: 0.04580, in 0.231s\n",
      "[413/3000] 1 tree, 31 leaves, max depth = 14, train loss: 0.02070, val loss: 0.04578, in 0.263s\n",
      "[414/3000] 1 tree, 31 leaves, max depth = 10, train loss: 0.02067, val loss: 0.04579, in 0.218s\n",
      "[415/3000] 1 tree, 31 leaves, max depth = 12, train loss: 0.02063, val loss: 0.04578, in 0.178s\n",
      "[416/3000] 1 tree, 31 leaves, max depth = 10, train loss: 0.02057, val loss: 0.04576, in 0.245s\n",
      "[417/3000] 1 tree, 31 leaves, max depth = 14, train loss: 0.02053, val loss: 0.04576, in 0.191s\n",
      "[418/3000] 1 tree, 31 leaves, max depth = 15, train loss: 0.02048, val loss: 0.04575, in 0.353s\n",
      "[419/3000] 1 tree, 31 leaves, max depth = 20, train loss: 0.02045, val loss: 0.04575, in 0.221s\n",
      "[420/3000] 1 tree, 31 leaves, max depth = 9, train loss: 0.02042, val loss: 0.04575, in 0.220s\n",
      "[421/3000] 1 tree, 31 leaves, max depth = 9, train loss: 0.02038, val loss: 0.04576, in 0.183s\n",
      "[422/3000] 1 tree, 31 leaves, max depth = 14, train loss: 0.02033, val loss: 0.04576, in 0.212s\n",
      "[423/3000] 1 tree, 31 leaves, max depth = 12, train loss: 0.02028, val loss: 0.04577, in 0.270s\n",
      "[424/3000] 1 tree, 31 leaves, max depth = 10, train loss: 0.02024, val loss: 0.04576, in 0.247s\n",
      "[425/3000] 1 tree, 31 leaves, max depth = 10, train loss: 0.02020, val loss: 0.04576, in 0.228s\n",
      "[426/3000] 1 tree, 31 leaves, max depth = 9, train loss: 0.02017, val loss: 0.04576, in 0.192s\n",
      "[427/3000] 1 tree, 31 leaves, max depth = 12, train loss: 0.02013, val loss: 0.04578, in 0.165s\n",
      "[428/3000] 1 tree, 31 leaves, max depth = 13, train loss: 0.02010, val loss: 0.04578, in 0.180s\n",
      "[429/3000] 1 tree, 31 leaves, max depth = 11, train loss: 0.02005, val loss: 0.04576, in 0.262s\n",
      "[430/3000] 1 tree, 31 leaves, max depth = 14, train loss: 0.02000, val loss: 0.04578, in 0.255s\n",
      "[431/3000] 1 tree, 31 leaves, max depth = 15, train loss: 0.01996, val loss: 0.04580, in 0.224s\n",
      "[432/3000] 1 tree, 31 leaves, max depth = 11, train loss: 0.01993, val loss: 0.04578, in 0.229s\n",
      "[433/3000] 1 tree, 31 leaves, max depth = 10, train loss: 0.01988, val loss: 0.04578, in 0.236s\n",
      "[434/3000] 1 tree, 31 leaves, max depth = 13, train loss: 0.01986, val loss: 0.04577, in 0.192s\n",
      "[435/3000] 1 tree, 31 leaves, max depth = 9, train loss: 0.01982, val loss: 0.04576, in 0.242s\n",
      "[436/3000] 1 tree, 31 leaves, max depth = 10, train loss: 0.01978, val loss: 0.04574, in 0.232s\n",
      "[437/3000] 1 tree, 31 leaves, max depth = 9, train loss: 0.01974, val loss: 0.04573, in 0.215s\n",
      "[438/3000] 1 tree, 31 leaves, max depth = 8, train loss: 0.01970, val loss: 0.04572, in 0.336s\n",
      "[439/3000] 1 tree, 31 leaves, max depth = 11, train loss: 0.01966, val loss: 0.04573, in 0.272s\n",
      "[440/3000] 1 tree, 31 leaves, max depth = 17, train loss: 0.01963, val loss: 0.04570, in 0.249s\n",
      "[441/3000] 1 tree, 31 leaves, max depth = 11, train loss: 0.01960, val loss: 0.04570, in 0.194s\n",
      "[442/3000] 1 tree, 31 leaves, max depth = 14, train loss: 0.01956, val loss: 0.04569, in 0.235s\n",
      "[443/3000] 1 tree, 31 leaves, max depth = 12, train loss: 0.01952, val loss: 0.04569, in 0.237s\n",
      "[444/3000] 1 tree, 31 leaves, max depth = 10, train loss: 0.01948, val loss: 0.04566, in 0.218s\n",
      "[445/3000] 1 tree, 31 leaves, max depth = 10, train loss: 0.01945, val loss: 0.04568, in 0.214s\n",
      "[446/3000] 1 tree, 31 leaves, max depth = 10, train loss: 0.01942, val loss: 0.04566, in 0.268s\n",
      "[447/3000] 1 tree, 31 leaves, max depth = 16, train loss: 0.01940, val loss: 0.04567, in 0.176s\n",
      "[448/3000] 1 tree, 31 leaves, max depth = 13, train loss: 0.01936, val loss: 0.04568, in 0.260s\n",
      "[449/3000] 1 tree, 31 leaves, max depth = 12, train loss: 0.01931, val loss: 0.04567, in 0.299s\n",
      "[450/3000] 1 tree, 31 leaves, max depth = 9, train loss: 0.01928, val loss: 0.04568, in 0.256s\n",
      "[451/3000] 1 tree, 31 leaves, max depth = 12, train loss: 0.01925, val loss: 0.04568, in 0.191s\n",
      "[452/3000] 1 tree, 31 leaves, max depth = 13, train loss: 0.01922, val loss: 0.04567, in 0.208s\n",
      "[453/3000] 1 tree, 31 leaves, max depth = 10, train loss: 0.01918, val loss: 0.04567, in 0.284s\n",
      "[454/3000] 1 tree, 31 leaves, max depth = 14, train loss: 0.01916, val loss: 0.04567, in 0.189s\n",
      "[455/3000] 1 tree, 31 leaves, max depth = 13, train loss: 0.01912, val loss: 0.04567, in 0.256s\n",
      "[456/3000] 1 tree, 31 leaves, max depth = 11, train loss: 0.01908, val loss: 0.04566, in 0.245s\n",
      "[457/3000] 1 tree, 31 leaves, max depth = 10, train loss: 0.01905, val loss: 0.04566, in 0.194s\n",
      "[458/3000] 1 tree, 31 leaves, max depth = 9, train loss: 0.01901, val loss: 0.04564, in 0.363s\n",
      "[459/3000] 1 tree, 31 leaves, max depth = 12, train loss: 0.01897, val loss: 0.04564, in 0.301s\n",
      "[460/3000] 1 tree, 31 leaves, max depth = 10, train loss: 0.01895, val loss: 0.04562, in 0.215s\n",
      "[461/3000] 1 tree, 31 leaves, max depth = 11, train loss: 0.01891, val loss: 0.04562, in 0.195s\n",
      "[462/3000] 1 tree, 31 leaves, max depth = 12, train loss: 0.01887, val loss: 0.04561, in 0.289s\n",
      "[463/3000] 1 tree, 31 leaves, max depth = 11, train loss: 0.01884, val loss: 0.04559, in 0.237s\n",
      "[464/3000] 1 tree, 31 leaves, max depth = 13, train loss: 0.01881, val loss: 0.04562, in 0.188s\n",
      "[465/3000] 1 tree, 31 leaves, max depth = 12, train loss: 0.01879, val loss: 0.04563, in 0.227s\n",
      "[466/3000] 1 tree, 31 leaves, max depth = 13, train loss: 0.01875, val loss: 0.04563, in 0.207s\n",
      "[467/3000] 1 tree, 31 leaves, max depth = 10, train loss: 0.01871, val loss: 0.04561, in 0.268s\n",
      "[468/3000] 1 tree, 31 leaves, max depth = 10, train loss: 0.01868, val loss: 0.04562, in 0.200s\n",
      "[469/3000] 1 tree, 31 leaves, max depth = 10, train loss: 0.01864, val loss: 0.04562, in 0.223s\n",
      "[470/3000] 1 tree, 31 leaves, max depth = 10, train loss: 0.01861, val loss: 0.04560, in 0.208s\n",
      "[471/3000] 1 tree, 31 leaves, max depth = 13, train loss: 0.01857, val loss: 0.04562, in 0.195s\n",
      "[472/3000] 1 tree, 31 leaves, max depth = 12, train loss: 0.01854, val loss: 0.04562, in 0.237s\n",
      "[473/3000] 1 tree, 31 leaves, max depth = 12, train loss: 0.01851, val loss: 0.04561, in 0.208s\n",
      "[474/3000] 1 tree, 31 leaves, max depth = 10, train loss: 0.01848, val loss: 0.04561, in 0.245s\n",
      "[475/3000] 1 tree, 31 leaves, max depth = 13, train loss: 0.01844, val loss: 0.04562, in 0.237s\n",
      "[476/3000] 1 tree, 31 leaves, max depth = 10, train loss: 0.01840, val loss: 0.04561, in 0.216s\n",
      "[477/3000] 1 tree, 31 leaves, max depth = 11, train loss: 0.01836, val loss: 0.04560, in 0.249s\n",
      "[478/3000] 1 tree, 31 leaves, max depth = 10, train loss: 0.01834, val loss: 0.04560, in 0.406s\n",
      "[479/3000] 1 tree, 31 leaves, max depth = 9, train loss: 0.01829, val loss: 0.04562, in 0.308s\n",
      "[480/3000] 1 tree, 31 leaves, max depth = 15, train loss: 0.01826, val loss: 0.04560, in 0.279s\n",
      "[481/3000] 1 tree, 31 leaves, max depth = 9, train loss: 0.01822, val loss: 0.04559, in 0.231s\n",
      "[482/3000] 1 tree, 31 leaves, max depth = 14, train loss: 0.01817, val loss: 0.04555, in 0.268s\n",
      "[483/3000] 1 tree, 31 leaves, max depth = 14, train loss: 0.01813, val loss: 0.04555, in 0.229s\n",
      "[484/3000] 1 tree, 31 leaves, max depth = 14, train loss: 0.01810, val loss: 0.04555, in 0.235s\n",
      "[485/3000] 1 tree, 31 leaves, max depth = 10, train loss: 0.01808, val loss: 0.04555, in 0.228s\n",
      "[486/3000] 1 tree, 31 leaves, max depth = 10, train loss: 0.01803, val loss: 0.04554, in 0.265s\n",
      "[487/3000] 1 tree, 31 leaves, max depth = 11, train loss: 0.01801, val loss: 0.04555, in 0.200s\n",
      "[488/3000] 1 tree, 31 leaves, max depth = 9, train loss: 0.01798, val loss: 0.04555, in 0.205s\n",
      "[489/3000] 1 tree, 31 leaves, max depth = 15, train loss: 0.01795, val loss: 0.04556, in 0.223s\n",
      "[490/3000] 1 tree, 31 leaves, max depth = 10, train loss: 0.01791, val loss: 0.04557, in 0.217s\n",
      "[491/3000] 1 tree, 31 leaves, max depth = 9, train loss: 0.01789, val loss: 0.04558, in 0.182s\n",
      "[492/3000] 1 tree, 31 leaves, max depth = 13, train loss: 0.01785, val loss: 0.04555, in 0.208s\n",
      "[493/3000] 1 tree, 31 leaves, max depth = 11, train loss: 0.01782, val loss: 0.04555, in 0.236s\n",
      "[494/3000] 1 tree, 31 leaves, max depth = 9, train loss: 0.01779, val loss: 0.04555, in 0.213s\n",
      "[495/3000] 1 tree, 31 leaves, max depth = 17, train loss: 0.01777, val loss: 0.04557, in 0.227s\n",
      "[496/3000] 1 tree, 31 leaves, max depth = 12, train loss: 0.01775, val loss: 0.04561, in 0.197s\n",
      "[497/3000] 1 tree, 31 leaves, max depth = 8, train loss: 0.01772, val loss: 0.04561, in 0.212s\n",
      "[498/3000] 1 tree, 31 leaves, max depth = 10, train loss: 0.01769, val loss: 0.04561, in 0.360s\n",
      "[499/3000] 1 tree, 31 leaves, max depth = 11, train loss: 0.01765, val loss: 0.04560, in 0.222s\n",
      "[500/3000] 1 tree, 31 leaves, max depth = 14, train loss: 0.01762, val loss: 0.04560, in 0.222s\n",
      "[501/3000] 1 tree, 31 leaves, max depth = 13, train loss: 0.01760, val loss: 0.04560, in 0.254s\n",
      "[502/3000] 1 tree, 31 leaves, max depth = 14, train loss: 0.01756, val loss: 0.04561, in 0.284s\n",
      "[503/3000] 1 tree, 31 leaves, max depth = 9, train loss: 0.01753, val loss: 0.04563, in 0.254s\n",
      "[504/3000] 1 tree, 31 leaves, max depth = 11, train loss: 0.01750, val loss: 0.04563, in 0.178s\n",
      "[505/3000] 1 tree, 31 leaves, max depth = 10, train loss: 0.01746, val loss: 0.04563, in 0.195s\n",
      "[506/3000] 1 tree, 31 leaves, max depth = 12, train loss: 0.01741, val loss: 0.04561, in 0.273s\n",
      "[507/3000] 1 tree, 31 leaves, max depth = 15, train loss: 0.01738, val loss: 0.04560, in 0.257s\n",
      "[508/3000] 1 tree, 31 leaves, max depth = 13, train loss: 0.01734, val loss: 0.04559, in 0.248s\n",
      "[509/3000] 1 tree, 31 leaves, max depth = 8, train loss: 0.01732, val loss: 0.04560, in 0.205s\n",
      "[510/3000] 1 tree, 31 leaves, max depth = 9, train loss: 0.01730, val loss: 0.04560, in 0.233s\n",
      "[511/3000] 1 tree, 31 leaves, max depth = 11, train loss: 0.01727, val loss: 0.04558, in 0.253s\n",
      "[512/3000] 1 tree, 31 leaves, max depth = 10, train loss: 0.01723, val loss: 0.04555, in 0.214s\n",
      "[513/3000] 1 tree, 31 leaves, max depth = 8, train loss: 0.01721, val loss: 0.04556, in 0.251s\n",
      "[514/3000] 1 tree, 31 leaves, max depth = 10, train loss: 0.01718, val loss: 0.04556, in 0.190s\n",
      "[515/3000] 1 tree, 31 leaves, max depth = 13, train loss: 0.01715, val loss: 0.04556, in 0.237s\n",
      "[516/3000] 1 tree, 31 leaves, max depth = 11, train loss: 0.01713, val loss: 0.04556, in 0.232s\n",
      "[517/3000] 1 tree, 31 leaves, max depth = 13, train loss: 0.01709, val loss: 0.04554, in 0.194s\n",
      "[518/3000] 1 tree, 31 leaves, max depth = 10, train loss: 0.01706, val loss: 0.04555, in 0.346s\n",
      "[519/3000] 1 tree, 31 leaves, max depth = 9, train loss: 0.01704, val loss: 0.04553, in 0.242s\n",
      "[520/3000] 1 tree, 31 leaves, max depth = 10, train loss: 0.01701, val loss: 0.04555, in 0.233s\n",
      "[521/3000] 1 tree, 31 leaves, max depth = 13, train loss: 0.01697, val loss: 0.04555, in 0.229s\n",
      "[522/3000] 1 tree, 31 leaves, max depth = 14, train loss: 0.01692, val loss: 0.04554, in 0.298s\n",
      "[523/3000] 1 tree, 31 leaves, max depth = 11, train loss: 0.01688, val loss: 0.04554, in 0.246s\n",
      "[524/3000] 1 tree, 31 leaves, max depth = 12, train loss: 0.01684, val loss: 0.04555, in 0.217s\n",
      "[525/3000] 1 tree, 31 leaves, max depth = 8, train loss: 0.01681, val loss: 0.04552, in 0.272s\n",
      "[526/3000] 1 tree, 31 leaves, max depth = 9, train loss: 0.01676, val loss: 0.04551, in 0.252s\n",
      "[527/3000] 1 tree, 31 leaves, max depth = 10, train loss: 0.01673, val loss: 0.04551, in 0.186s\n",
      "[528/3000] 1 tree, 31 leaves, max depth = 14, train loss: 0.01671, val loss: 0.04551, in 0.195s\n",
      "[529/3000] 1 tree, 31 leaves, max depth = 16, train loss: 0.01668, val loss: 0.04552, in 0.183s\n",
      "[530/3000] 1 tree, 31 leaves, max depth = 11, train loss: 0.01665, val loss: 0.04550, in 0.271s\n",
      "[531/3000] 1 tree, 31 leaves, max depth = 13, train loss: 0.01662, val loss: 0.04548, in 0.233s\n",
      "[532/3000] 1 tree, 31 leaves, max depth = 10, train loss: 0.01659, val loss: 0.04548, in 0.221s\n",
      "[533/3000] 1 tree, 31 leaves, max depth = 14, train loss: 0.01656, val loss: 0.04549, in 0.184s\n",
      "[534/3000] 1 tree, 31 leaves, max depth = 14, train loss: 0.01651, val loss: 0.04546, in 0.249s\n",
      "[535/3000] 1 tree, 31 leaves, max depth = 13, train loss: 0.01648, val loss: 0.04546, in 0.263s\n",
      "[536/3000] 1 tree, 31 leaves, max depth = 12, train loss: 0.01644, val loss: 0.04545, in 0.214s\n",
      "[537/3000] 1 tree, 31 leaves, max depth = 14, train loss: 0.01641, val loss: 0.04544, in 0.193s\n",
      "[538/3000] 1 tree, 31 leaves, max depth = 9, train loss: 0.01638, val loss: 0.04543, in 0.354s\n",
      "[539/3000] 1 tree, 31 leaves, max depth = 13, train loss: 0.01635, val loss: 0.04543, in 0.246s\n",
      "[540/3000] 1 tree, 31 leaves, max depth = 9, train loss: 0.01629, val loss: 0.04540, in 0.309s\n",
      "[541/3000] 1 tree, 31 leaves, max depth = 10, train loss: 0.01626, val loss: 0.04541, in 0.194s\n",
      "[542/3000] 1 tree, 31 leaves, max depth = 9, train loss: 0.01622, val loss: 0.04541, in 0.228s\n",
      "[543/3000] 1 tree, 31 leaves, max depth = 9, train loss: 0.01621, val loss: 0.04542, in 0.204s\n",
      "[544/3000] 1 tree, 31 leaves, max depth = 10, train loss: 0.01618, val loss: 0.04544, in 0.229s\n",
      "[545/3000] 1 tree, 31 leaves, max depth = 8, train loss: 0.01615, val loss: 0.04543, in 0.284s\n",
      "[546/3000] 1 tree, 31 leaves, max depth = 9, train loss: 0.01613, val loss: 0.04543, in 0.225s\n",
      "[547/3000] 1 tree, 31 leaves, max depth = 10, train loss: 0.01610, val loss: 0.04539, in 0.253s\n",
      "[548/3000] 1 tree, 31 leaves, max depth = 11, train loss: 0.01607, val loss: 0.04539, in 0.256s\n",
      "[549/3000] 1 tree, 31 leaves, max depth = 13, train loss: 0.01603, val loss: 0.04539, in 0.240s\n",
      "[550/3000] 1 tree, 31 leaves, max depth = 15, train loss: 0.01599, val loss: 0.04537, in 0.282s\n",
      "[551/3000] 1 tree, 31 leaves, max depth = 13, train loss: 0.01595, val loss: 0.04538, in 0.207s\n",
      "[552/3000] 1 tree, 31 leaves, max depth = 14, train loss: 0.01592, val loss: 0.04538, in 0.257s\n",
      "[553/3000] 1 tree, 31 leaves, max depth = 11, train loss: 0.01589, val loss: 0.04536, in 0.244s\n",
      "[554/3000] 1 tree, 31 leaves, max depth = 16, train loss: 0.01586, val loss: 0.04535, in 0.210s\n",
      "[555/3000] 1 tree, 31 leaves, max depth = 12, train loss: 0.01583, val loss: 0.04538, in 0.196s\n",
      "[556/3000] 1 tree, 31 leaves, max depth = 12, train loss: 0.01581, val loss: 0.04538, in 0.195s\n",
      "[557/3000] 1 tree, 31 leaves, max depth = 14, train loss: 0.01579, val loss: 0.04538, in 0.386s\n",
      "[558/3000] 1 tree, 31 leaves, max depth = 17, train loss: 0.01575, val loss: 0.04538, in 0.260s\n",
      "[559/3000] 1 tree, 31 leaves, max depth = 12, train loss: 0.01572, val loss: 0.04535, in 0.296s\n",
      "[560/3000] 1 tree, 31 leaves, max depth = 12, train loss: 0.01569, val loss: 0.04535, in 0.266s\n",
      "[561/3000] 1 tree, 31 leaves, max depth = 10, train loss: 0.01564, val loss: 0.04536, in 0.284s\n",
      "[562/3000] 1 tree, 31 leaves, max depth = 9, train loss: 0.01561, val loss: 0.04533, in 0.260s\n",
      "[563/3000] 1 tree, 31 leaves, max depth = 11, train loss: 0.01557, val loss: 0.04533, in 0.264s\n",
      "[564/3000] 1 tree, 31 leaves, max depth = 15, train loss: 0.01555, val loss: 0.04534, in 0.195s\n",
      "[565/3000] 1 tree, 31 leaves, max depth = 11, train loss: 0.01552, val loss: 0.04534, in 0.235s\n",
      "[566/3000] 1 tree, 31 leaves, max depth = 9, train loss: 0.01549, val loss: 0.04532, in 0.287s\n",
      "[567/3000] 1 tree, 31 leaves, max depth = 12, train loss: 0.01546, val loss: 0.04534, in 0.214s\n",
      "[568/3000] 1 tree, 31 leaves, max depth = 14, train loss: 0.01543, val loss: 0.04532, in 0.265s\n",
      "[569/3000] 1 tree, 31 leaves, max depth = 13, train loss: 0.01539, val loss: 0.04530, in 0.231s\n",
      "[570/3000] 1 tree, 31 leaves, max depth = 14, train loss: 0.01537, val loss: 0.04532, in 0.194s\n",
      "[571/3000] 1 tree, 31 leaves, max depth = 11, train loss: 0.01535, val loss: 0.04532, in 0.213s\n",
      "[572/3000] 1 tree, 31 leaves, max depth = 10, train loss: 0.01531, val loss: 0.04533, in 0.256s\n",
      "[573/3000] 1 tree, 31 leaves, max depth = 11, train loss: 0.01529, val loss: 0.04534, in 0.235s\n",
      "[574/3000] 1 tree, 31 leaves, max depth = 15, train loss: 0.01526, val loss: 0.04533, in 0.241s\n",
      "[575/3000] 1 tree, 31 leaves, max depth = 15, train loss: 0.01523, val loss: 0.04534, in 0.179s\n",
      "[576/3000] 1 tree, 31 leaves, max depth = 11, train loss: 0.01520, val loss: 0.04536, in 0.205s\n",
      "[577/3000] 1 tree, 31 leaves, max depth = 10, train loss: 0.01517, val loss: 0.04537, in 0.202s\n",
      "[578/3000] 1 tree, 31 leaves, max depth = 8, train loss: 0.01515, val loss: 0.04537, in 0.355s\n",
      "[579/3000] 1 tree, 31 leaves, max depth = 11, train loss: 0.01513, val loss: 0.04535, in 0.258s\n",
      "[580/3000] 1 tree, 31 leaves, max depth = 15, train loss: 0.01511, val loss: 0.04535, in 0.211s\n",
      "[581/3000] 1 tree, 31 leaves, max depth = 11, train loss: 0.01509, val loss: 0.04535, in 0.214s\n",
      "[582/3000] 1 tree, 31 leaves, max depth = 9, train loss: 0.01506, val loss: 0.04537, in 0.243s\n",
      "[583/3000] 1 tree, 31 leaves, max depth = 9, train loss: 0.01504, val loss: 0.04536, in 0.266s\n",
      "[584/3000] 1 tree, 31 leaves, max depth = 14, train loss: 0.01503, val loss: 0.04536, in 0.184s\n",
      "[585/3000] 1 tree, 31 leaves, max depth = 8, train loss: 0.01501, val loss: 0.04537, in 0.217s\n",
      "[586/3000] 1 tree, 31 leaves, max depth = 11, train loss: 0.01498, val loss: 0.04534, in 0.213s\n",
      "[587/3000] 1 tree, 31 leaves, max depth = 12, train loss: 0.01494, val loss: 0.04534, in 0.257s\n",
      "[588/3000] 1 tree, 31 leaves, max depth = 10, train loss: 0.01492, val loss: 0.04533, in 0.259s\n",
      "[589/3000] 1 tree, 31 leaves, max depth = 10, train loss: 0.01489, val loss: 0.04533, in 0.241s\n",
      "[590/3000] 1 tree, 31 leaves, max depth = 12, train loss: 0.01486, val loss: 0.04534, in 0.212s\n",
      "[591/3000] 1 tree, 31 leaves, max depth = 10, train loss: 0.01482, val loss: 0.04533, in 0.274s\n",
      "[592/3000] 1 tree, 31 leaves, max depth = 14, train loss: 0.01481, val loss: 0.04533, in 0.190s\n",
      "[593/3000] 1 tree, 31 leaves, max depth = 12, train loss: 0.01478, val loss: 0.04532, in 0.258s\n",
      "[594/3000] 1 tree, 31 leaves, max depth = 9, train loss: 0.01477, val loss: 0.04534, in 0.199s\n",
      "[595/3000] 1 tree, 31 leaves, max depth = 10, train loss: 0.01474, val loss: 0.04536, in 0.191s\n",
      "[596/3000] 1 tree, 31 leaves, max depth = 13, train loss: 0.01472, val loss: 0.04537, in 0.248s\n",
      "[597/3000] 1 tree, 31 leaves, max depth = 11, train loss: 0.01468, val loss: 0.04537, in 0.212s\n",
      "[598/3000] 1 tree, 31 leaves, max depth = 10, train loss: 0.01465, val loss: 0.04536, in 0.354s\n",
      "[599/3000] 1 tree, 31 leaves, max depth = 12, train loss: 0.01461, val loss: 0.04535, in 0.300s\n",
      "[600/3000] 1 tree, 31 leaves, max depth = 15, train loss: 0.01459, val loss: 0.04533, in 0.282s\n",
      "[601/3000] 1 tree, 31 leaves, max depth = 14, train loss: 0.01456, val loss: 0.04532, in 0.242s\n",
      "[602/3000] 1 tree, 31 leaves, max depth = 16, train loss: 0.01454, val loss: 0.04535, in 0.181s\n",
      "[603/3000] 1 tree, 31 leaves, max depth = 12, train loss: 0.01452, val loss: 0.04535, in 0.226s\n",
      "[604/3000] 1 tree, 31 leaves, max depth = 10, train loss: 0.01449, val loss: 0.04535, in 0.202s\n",
      "[605/3000] 1 tree, 31 leaves, max depth = 14, train loss: 0.01447, val loss: 0.04533, in 0.210s\n",
      "[606/3000] 1 tree, 31 leaves, max depth = 13, train loss: 0.01445, val loss: 0.04534, in 0.206s\n",
      "[607/3000] 1 tree, 31 leaves, max depth = 13, train loss: 0.01442, val loss: 0.04535, in 0.263s\n",
      "[608/3000] 1 tree, 31 leaves, max depth = 10, train loss: 0.01438, val loss: 0.04533, in 0.183s\n",
      "[609/3000] 1 tree, 31 leaves, max depth = 13, train loss: 0.01435, val loss: 0.04531, in 0.227s\n",
      "[610/3000] 1 tree, 31 leaves, max depth = 11, train loss: 0.01433, val loss: 0.04533, in 0.207s\n",
      "[611/3000] 1 tree, 31 leaves, max depth = 12, train loss: 0.01430, val loss: 0.04530, in 0.251s\n",
      "[612/3000] 1 tree, 31 leaves, max depth = 11, train loss: 0.01427, val loss: 0.04532, in 0.252s\n",
      "[613/3000] 1 tree, 31 leaves, max depth = 9, train loss: 0.01425, val loss: 0.04530, in 0.271s\n",
      "[614/3000] 1 tree, 31 leaves, max depth = 13, train loss: 0.01423, val loss: 0.04532, in 0.219s\n",
      "[615/3000] 1 tree, 31 leaves, max depth = 9, train loss: 0.01421, val loss: 0.04533, in 0.229s\n",
      "[616/3000] 1 tree, 31 leaves, max depth = 12, train loss: 0.01418, val loss: 0.04533, in 0.253s\n",
      "[617/3000] 1 tree, 31 leaves, max depth = 12, train loss: 0.01416, val loss: 0.04533, in 0.367s\n",
      "[618/3000] 1 tree, 31 leaves, max depth = 15, train loss: 0.01414, val loss: 0.04533, in 0.198s\n",
      "[619/3000] 1 tree, 31 leaves, max depth = 11, train loss: 0.01411, val loss: 0.04534, in 0.242s\n",
      "[620/3000] 1 tree, 31 leaves, max depth = 11, train loss: 0.01408, val loss: 0.04533, in 0.206s\n",
      "[621/3000] 1 tree, 31 leaves, max depth = 8, train loss: 0.01406, val loss: 0.04531, in 0.218s\n",
      "[622/3000] 1 tree, 31 leaves, max depth = 11, train loss: 0.01404, val loss: 0.04530, in 0.233s\n",
      "[623/3000] 1 tree, 31 leaves, max depth = 9, train loss: 0.01402, val loss: 0.04528, in 0.253s\n",
      "[624/3000] 1 tree, 31 leaves, max depth = 14, train loss: 0.01400, val loss: 0.04529, in 0.207s\n",
      "[625/3000] 1 tree, 31 leaves, max depth = 12, train loss: 0.01398, val loss: 0.04528, in 0.238s\n",
      "[626/3000] 1 tree, 31 leaves, max depth = 9, train loss: 0.01395, val loss: 0.04526, in 0.251s\n",
      "[627/3000] 1 tree, 31 leaves, max depth = 11, train loss: 0.01393, val loss: 0.04527, in 0.204s\n",
      "[628/3000] 1 tree, 31 leaves, max depth = 10, train loss: 0.01390, val loss: 0.04526, in 0.217s\n",
      "[629/3000] 1 tree, 31 leaves, max depth = 15, train loss: 0.01387, val loss: 0.04524, in 0.216s\n",
      "[630/3000] 1 tree, 31 leaves, max depth = 14, train loss: 0.01386, val loss: 0.04526, in 0.179s\n",
      "[631/3000] 1 tree, 31 leaves, max depth = 9, train loss: 0.01385, val loss: 0.04525, in 0.191s\n",
      "[632/3000] 1 tree, 31 leaves, max depth = 14, train loss: 0.01383, val loss: 0.04525, in 0.190s\n",
      "[633/3000] 1 tree, 31 leaves, max depth = 11, train loss: 0.01379, val loss: 0.04527, in 0.263s\n",
      "[634/3000] 1 tree, 31 leaves, max depth = 13, train loss: 0.01377, val loss: 0.04529, in 0.209s\n",
      "[635/3000] 1 tree, 31 leaves, max depth = 12, train loss: 0.01373, val loss: 0.04531, in 0.257s\n",
      "[636/3000] 1 tree, 31 leaves, max depth = 11, train loss: 0.01372, val loss: 0.04531, in 0.190s\n",
      "[637/3000] 1 tree, 31 leaves, max depth = 15, train loss: 0.01369, val loss: 0.04531, in 0.193s\n",
      "[638/3000] 1 tree, 31 leaves, max depth = 13, train loss: 0.01367, val loss: 0.04532, in 0.351s\n",
      "[639/3000] 1 tree, 31 leaves, max depth = 10, train loss: 0.01364, val loss: 0.04532, in 0.272s\n",
      "[640/3000] 1 tree, 31 leaves, max depth = 11, train loss: 0.01361, val loss: 0.04532, in 0.220s\n",
      "[641/3000] 1 tree, 31 leaves, max depth = 9, train loss: 0.01359, val loss: 0.04532, in 0.254s\n",
      "[642/3000] 1 tree, 31 leaves, max depth = 9, train loss: 0.01357, val loss: 0.04531, in 0.205s\n",
      "[643/3000] 1 tree, 31 leaves, max depth = 11, train loss: 0.01354, val loss: 0.04531, in 0.233s\n",
      "[644/3000] 1 tree, 31 leaves, max depth = 14, train loss: 0.01352, val loss: 0.04533, in 0.194s\n",
      "[645/3000] 1 tree, 31 leaves, max depth = 13, train loss: 0.01350, val loss: 0.04533, in 0.243s\n",
      "[646/3000] 1 tree, 31 leaves, max depth = 11, train loss: 0.01346, val loss: 0.04532, in 0.270s\n",
      "[647/3000] 1 tree, 31 leaves, max depth = 10, train loss: 0.01345, val loss: 0.04532, in 0.191s\n",
      "[648/3000] 1 tree, 31 leaves, max depth = 12, train loss: 0.01343, val loss: 0.04533, in 0.198s\n",
      "[649/3000] 1 tree, 31 leaves, max depth = 14, train loss: 0.01340, val loss: 0.04531, in 0.198s\n",
      "[650/3000] 1 tree, 31 leaves, max depth = 11, train loss: 0.01338, val loss: 0.04531, in 0.215s\n",
      "[651/3000] 1 tree, 31 leaves, max depth = 12, train loss: 0.01336, val loss: 0.04529, in 0.217s\n",
      "[652/3000] 1 tree, 31 leaves, max depth = 14, train loss: 0.01334, val loss: 0.04530, in 0.189s\n",
      "[653/3000] 1 tree, 31 leaves, max depth = 12, train loss: 0.01331, val loss: 0.04530, in 0.204s\n",
      "[654/3000] 1 tree, 31 leaves, max depth = 15, train loss: 0.01330, val loss: 0.04528, in 0.210s\n",
      "[655/3000] 1 tree, 31 leaves, max depth = 10, train loss: 0.01327, val loss: 0.04529, in 0.206s\n",
      "[656/3000] 1 tree, 31 leaves, max depth = 13, train loss: 0.01325, val loss: 0.04530, in 0.227s\n",
      "[657/3000] 1 tree, 31 leaves, max depth = 13, train loss: 0.01323, val loss: 0.04530, in 0.238s\n",
      "[658/3000] 1 tree, 31 leaves, max depth = 10, train loss: 0.01321, val loss: 0.04531, in 0.356s\n",
      "[659/3000] 1 tree, 31 leaves, max depth = 10, train loss: 0.01319, val loss: 0.04531, in 0.242s\n",
      "[660/3000] 1 tree, 31 leaves, max depth = 7, train loss: 0.01317, val loss: 0.04530, in 0.221s\n",
      "[661/3000] 1 tree, 31 leaves, max depth = 12, train loss: 0.01314, val loss: 0.04530, in 0.320s\n",
      "[662/3000] 1 tree, 31 leaves, max depth = 12, train loss: 0.01312, val loss: 0.04530, in 0.224s\n",
      "[663/3000] 1 tree, 31 leaves, max depth = 14, train loss: 0.01310, val loss: 0.04529, in 0.232s\n",
      "[664/3000] 1 tree, 31 leaves, max depth = 14, train loss: 0.01306, val loss: 0.04528, in 0.269s\n",
      "[665/3000] 1 tree, 31 leaves, max depth = 12, train loss: 0.01303, val loss: 0.04529, in 0.258s\n",
      "[666/3000] 1 tree, 31 leaves, max depth = 12, train loss: 0.01301, val loss: 0.04527, in 0.277s\n",
      "[667/3000] 1 tree, 31 leaves, max depth = 11, train loss: 0.01299, val loss: 0.04525, in 0.192s\n",
      "[668/3000] 1 tree, 31 leaves, max depth = 12, train loss: 0.01296, val loss: 0.04524, in 0.211s\n",
      "[669/3000] 1 tree, 31 leaves, max depth = 11, train loss: 0.01293, val loss: 0.04523, in 0.280s\n",
      "[670/3000] 1 tree, 31 leaves, max depth = 13, train loss: 0.01292, val loss: 0.04523, in 0.192s\n",
      "[671/3000] 1 tree, 31 leaves, max depth = 11, train loss: 0.01291, val loss: 0.04522, in 0.205s\n",
      "[672/3000] 1 tree, 31 leaves, max depth = 14, train loss: 0.01288, val loss: 0.04522, in 0.184s\n",
      "[673/3000] 1 tree, 31 leaves, max depth = 12, train loss: 0.01286, val loss: 0.04522, in 0.240s\n",
      "[674/3000] 1 tree, 31 leaves, max depth = 12, train loss: 0.01284, val loss: 0.04525, in 0.218s\n",
      "[675/3000] 1 tree, 31 leaves, max depth = 9, train loss: 0.01282, val loss: 0.04524, in 0.186s\n",
      "[676/3000] 1 tree, 31 leaves, max depth = 11, train loss: 0.01279, val loss: 0.04525, in 0.293s\n",
      "[677/3000] 1 tree, 31 leaves, max depth = 11, train loss: 0.01277, val loss: 0.04526, in 0.365s\n",
      "[678/3000] 1 tree, 31 leaves, max depth = 12, train loss: 0.01275, val loss: 0.04530, in 0.205s\n",
      "[679/3000] 1 tree, 31 leaves, max depth = 10, train loss: 0.01274, val loss: 0.04529, in 0.317s\n",
      "[680/3000] 1 tree, 31 leaves, max depth = 10, train loss: 0.01272, val loss: 0.04531, in 0.199s\n",
      "[681/3000] 1 tree, 31 leaves, max depth = 15, train loss: 0.01270, val loss: 0.04531, in 0.239s\n",
      "[682/3000] 1 tree, 31 leaves, max depth = 8, train loss: 0.01268, val loss: 0.04531, in 0.251s\n",
      "[683/3000] 1 tree, 31 leaves, max depth = 11, train loss: 0.01266, val loss: 0.04531, in 0.238s\n",
      "[684/3000] 1 tree, 31 leaves, max depth = 12, train loss: 0.01264, val loss: 0.04531, in 0.213s\n",
      "[685/3000] 1 tree, 31 leaves, max depth = 13, train loss: 0.01261, val loss: 0.04532, in 0.230s\n",
      "[686/3000] 1 tree, 31 leaves, max depth = 15, train loss: 0.01259, val loss: 0.04532, in 0.227s\n",
      "[687/3000] 1 tree, 31 leaves, max depth = 11, train loss: 0.01257, val loss: 0.04530, in 0.217s\n",
      "[688/3000] 1 tree, 31 leaves, max depth = 15, train loss: 0.01255, val loss: 0.04529, in 0.228s\n",
      "[689/3000] 1 tree, 31 leaves, max depth = 14, train loss: 0.01252, val loss: 0.04529, in 0.272s\n",
      "[690/3000] 1 tree, 31 leaves, max depth = 9, train loss: 0.01250, val loss: 0.04531, in 0.221s\n",
      "[691/3000] 1 tree, 31 leaves, max depth = 13, train loss: 0.01248, val loss: 0.04531, in 0.272s\n",
      "[692/3000] 1 tree, 31 leaves, max depth = 11, train loss: 0.01246, val loss: 0.04532, in 0.184s\n",
      "[693/3000] 1 tree, 31 leaves, max depth = 10, train loss: 0.01244, val loss: 0.04533, in 0.235s\n",
      "[694/3000] 1 tree, 31 leaves, max depth = 11, train loss: 0.01242, val loss: 0.04531, in 0.263s\n",
      "[695/3000] 1 tree, 31 leaves, max depth = 10, train loss: 0.01240, val loss: 0.04531, in 0.204s\n",
      "[696/3000] 1 tree, 31 leaves, max depth = 15, train loss: 0.01238, val loss: 0.04531, in 0.207s\n",
      "[697/3000] 1 tree, 31 leaves, max depth = 9, train loss: 0.01235, val loss: 0.04530, in 0.261s\n",
      "[698/3000] 1 tree, 31 leaves, max depth = 12, train loss: 0.01234, val loss: 0.04529, in 0.329s\n",
      "[699/3000] 1 tree, 31 leaves, max depth = 14, train loss: 0.01232, val loss: 0.04529, in 0.238s\n",
      "[700/3000] 1 tree, 31 leaves, max depth = 11, train loss: 0.01230, val loss: 0.04529, in 0.212s\n",
      "[701/3000] 1 tree, 31 leaves, max depth = 11, train loss: 0.01228, val loss: 0.04529, in 0.295s\n",
      "[702/3000] 1 tree, 31 leaves, max depth = 17, train loss: 0.01225, val loss: 0.04531, in 0.268s\n",
      "[703/3000] 1 tree, 31 leaves, max depth = 10, train loss: 0.01223, val loss: 0.04529, in 0.217s\n",
      "[704/3000] 1 tree, 31 leaves, max depth = 12, train loss: 0.01220, val loss: 0.04528, in 0.204s\n",
      "[705/3000] 1 tree, 31 leaves, max depth = 10, train loss: 0.01218, val loss: 0.04528, in 0.201s\n",
      "[706/3000] 1 tree, 31 leaves, max depth = 9, train loss: 0.01216, val loss: 0.04528, in 0.223s\n",
      "[707/3000] 1 tree, 31 leaves, max depth = 12, train loss: 0.01215, val loss: 0.04527, in 0.207s\n",
      "[708/3000] 1 tree, 31 leaves, max depth = 10, train loss: 0.01213, val loss: 0.04527, in 0.245s\n",
      "[709/3000] 1 tree, 31 leaves, max depth = 12, train loss: 0.01210, val loss: 0.04527, in 0.253s\n",
      "[710/3000] 1 tree, 31 leaves, max depth = 9, train loss: 0.01208, val loss: 0.04527, in 0.243s\n",
      "[711/3000] 1 tree, 31 leaves, max depth = 13, train loss: 0.01206, val loss: 0.04530, in 0.234s\n",
      "[712/3000] 1 tree, 31 leaves, max depth = 13, train loss: 0.01204, val loss: 0.04530, in 0.226s\n",
      "[713/3000] 1 tree, 31 leaves, max depth = 15, train loss: 0.01201, val loss: 0.04533, in 0.279s\n",
      "[714/3000] 1 tree, 31 leaves, max depth = 8, train loss: 0.01198, val loss: 0.04531, in 0.200s\n",
      "[715/3000] 1 tree, 31 leaves, max depth = 13, train loss: 0.01197, val loss: 0.04531, in 0.239s\n",
      "[716/3000] 1 tree, 31 leaves, max depth = 14, train loss: 0.01195, val loss: 0.04532, in 0.257s\n",
      "[717/3000] 1 tree, 31 leaves, max depth = 10, train loss: 0.01193, val loss: 0.04531, in 0.203s\n",
      "[718/3000] 1 tree, 31 leaves, max depth = 11, train loss: 0.01192, val loss: 0.04532, in 0.329s\n",
      "[719/3000] 1 tree, 31 leaves, max depth = 11, train loss: 0.01190, val loss: 0.04532, in 0.517s\n",
      "[720/3000] 1 tree, 31 leaves, max depth = 10, train loss: 0.01188, val loss: 0.04529, in 0.273s\n",
      "[721/3000] 1 tree, 31 leaves, max depth = 11, train loss: 0.01186, val loss: 0.04530, in 0.256s\n",
      "[722/3000] 1 tree, 31 leaves, max depth = 12, train loss: 0.01183, val loss: 0.04531, in 0.219s\n",
      "[723/3000] 1 tree, 31 leaves, max depth = 11, train loss: 0.01181, val loss: 0.04530, in 0.197s\n",
      "[724/3000] 1 tree, 31 leaves, max depth = 12, train loss: 0.01179, val loss: 0.04533, in 0.233s\n",
      "[725/3000] 1 tree, 31 leaves, max depth = 10, train loss: 0.01176, val loss: 0.04535, in 0.264s\n",
      "[726/3000] 1 tree, 31 leaves, max depth = 8, train loss: 0.01174, val loss: 0.04537, in 0.246s\n",
      "[727/3000] 1 tree, 31 leaves, max depth = 8, train loss: 0.01172, val loss: 0.04536, in 0.244s\n",
      "[728/3000] 1 tree, 31 leaves, max depth = 14, train loss: 0.01170, val loss: 0.04536, in 0.271s\n",
      "[729/3000] 1 tree, 31 leaves, max depth = 11, train loss: 0.01168, val loss: 0.04535, in 0.215s\n",
      "[730/3000] 1 tree, 31 leaves, max depth = 13, train loss: 0.01165, val loss: 0.04535, in 0.215s\n",
      "[731/3000] 1 tree, 31 leaves, max depth = 11, train loss: 0.01163, val loss: 0.04534, in 0.182s\n",
      "[732/3000] 1 tree, 31 leaves, max depth = 13, train loss: 0.01161, val loss: 0.04534, in 0.245s\n",
      "[733/3000] 1 tree, 31 leaves, max depth = 11, train loss: 0.01159, val loss: 0.04534, in 0.203s\n",
      "[734/3000] 1 tree, 31 leaves, max depth = 9, train loss: 0.01158, val loss: 0.04535, in 0.219s\n",
      "[735/3000] 1 tree, 31 leaves, max depth = 11, train loss: 0.01155, val loss: 0.04537, in 0.224s\n",
      "[736/3000] 1 tree, 31 leaves, max depth = 9, train loss: 0.01153, val loss: 0.04537, in 0.211s\n",
      "[737/3000] 1 tree, 31 leaves, max depth = 11, train loss: 0.01151, val loss: 0.04538, in 0.229s\n",
      "[738/3000] 1 tree, 31 leaves, max depth = 12, train loss: 0.01149, val loss: 0.04538, in 0.219s\n",
      "[739/3000] 1 tree, 31 leaves, max depth = 11, train loss: 0.01147, val loss: 0.04538, in 0.465s\n",
      "[740/3000] 1 tree, 31 leaves, max depth = 13, train loss: 0.01145, val loss: 0.04537, in 0.260s\n",
      "[741/3000] 1 tree, 31 leaves, max depth = 11, train loss: 0.01143, val loss: 0.04539, in 0.258s\n",
      "[742/3000] 1 tree, 31 leaves, max depth = 11, train loss: 0.01140, val loss: 0.04538, in 0.244s\n",
      "[743/3000] 1 tree, 31 leaves, max depth = 9, train loss: 0.01138, val loss: 0.04538, in 0.240s\n",
      "[744/3000] 1 tree, 31 leaves, max depth = 13, train loss: 0.01135, val loss: 0.04537, in 0.322s\n",
      "[745/3000] 1 tree, 31 leaves, max depth = 10, train loss: 0.01132, val loss: 0.04535, in 0.307s\n",
      "[746/3000] 1 tree, 31 leaves, max depth = 12, train loss: 0.01130, val loss: 0.04535, in 0.251s\n",
      "[747/3000] 1 tree, 31 leaves, max depth = 12, train loss: 0.01128, val loss: 0.04535, in 0.279s\n",
      "[748/3000] 1 tree, 31 leaves, max depth = 14, train loss: 0.01126, val loss: 0.04536, in 0.224s\n",
      "[749/3000] 1 tree, 31 leaves, max depth = 10, train loss: 0.01125, val loss: 0.04536, in 0.226s\n",
      "[750/3000] 1 tree, 31 leaves, max depth = 10, train loss: 0.01123, val loss: 0.04538, in 0.264s\n",
      "[751/3000] 1 tree, 31 leaves, max depth = 9, train loss: 0.01120, val loss: 0.04536, in 0.200s\n",
      "[752/3000] 1 tree, 31 leaves, max depth = 11, train loss: 0.01118, val loss: 0.04538, in 0.227s\n",
      "[753/3000] 1 tree, 31 leaves, max depth = 10, train loss: 0.01116, val loss: 0.04537, in 0.207s\n",
      "[754/3000] 1 tree, 31 leaves, max depth = 10, train loss: 0.01115, val loss: 0.04539, in 0.217s\n",
      "[755/3000] 1 tree, 31 leaves, max depth = 9, train loss: 0.01113, val loss: 0.04539, in 0.203s\n",
      "[756/3000] 1 tree, 31 leaves, max depth = 11, train loss: 0.01111, val loss: 0.04540, in 0.243s\n",
      "[757/3000] 1 tree, 31 leaves, max depth = 11, train loss: 0.01110, val loss: 0.04540, in 0.204s\n",
      "[758/3000] 1 tree, 31 leaves, max depth = 13, train loss: 0.01108, val loss: 0.04541, in 0.319s\n",
      "[759/3000] 1 tree, 31 leaves, max depth = 10, train loss: 0.01106, val loss: 0.04543, in 0.333s\n",
      "[760/3000] 1 tree, 31 leaves, max depth = 11, train loss: 0.01104, val loss: 0.04544, in 0.255s\n",
      "[761/3000] 1 tree, 31 leaves, max depth = 16, train loss: 0.01101, val loss: 0.04545, in 0.211s\n",
      "[762/3000] 1 tree, 31 leaves, max depth = 13, train loss: 0.01099, val loss: 0.04546, in 0.219s\n",
      "[763/3000] 1 tree, 31 leaves, max depth = 11, train loss: 0.01097, val loss: 0.04546, in 0.205s\n",
      "[764/3000] 1 tree, 31 leaves, max depth = 12, train loss: 0.01095, val loss: 0.04548, in 0.232s\n",
      "[765/3000] 1 tree, 31 leaves, max depth = 12, train loss: 0.01094, val loss: 0.04546, in 0.220s\n",
      "[766/3000] 1 tree, 31 leaves, max depth = 14, train loss: 0.01092, val loss: 0.04545, in 0.326s\n",
      "[767/3000] 1 tree, 31 leaves, max depth = 10, train loss: 0.01089, val loss: 0.04546, in 0.332s\n",
      "[768/3000] 1 tree, 31 leaves, max depth = 10, train loss: 0.01087, val loss: 0.04547, in 0.262s\n",
      "[769/3000] 1 tree, 31 leaves, max depth = 16, train loss: 0.01085, val loss: 0.04549, in 0.350s\n",
      "[770/3000] 1 tree, 31 leaves, max depth = 10, train loss: 0.01083, val loss: 0.04547, in 0.253s\n",
      "[771/3000] 1 tree, 31 leaves, max depth = 16, train loss: 0.01081, val loss: 0.04546, in 0.255s\n",
      "[772/3000] 1 tree, 31 leaves, max depth = 10, train loss: 0.01079, val loss: 0.04546, in 0.346s\n",
      "Fit 772 trees in 194.491 s, (23932 total leaves)\n",
      "Time spent computing histograms: 89.753s\n",
      "Time spent finding best splits:  8.975s\n",
      "Time spent applying splits:      9.510s\n",
      "Time spent predicting:           0.278s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "HistGradientBoostingClassifier(max_iter=3000, n_iter_no_change=100, verbose=1)"
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "histgbc = ensemble.HistGradientBoostingClassifier(max_iter=3000, n_iter_no_change=100,\n",
    "                                                  learning_rate=0.1, min_samples_leaf=20, \n",
    "                                                  warm_start=False, verbose=1)\n",
    "histgbc.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true,
     "source_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing results:\n",
      "accuracy = 0.9977542572115305\n",
      "adjusted balanced accuracy = 0.9702127321924627\n",
      "balanced accuracy = 0.9851063660962314\n",
      "average precision = 0.9552713483213487\n",
      "f1 score = 0.9766487743170706\n",
      "precision = 0.9822570662265319\n",
      "recall = 0.9711041610008159\n",
      "roc = 0.9851063660962314\n",
      "confusion matrix\n",
      " = [[289165    258]\n",
      " [   425  14283]]\n",
      "Testing results:\n",
      "accuracy = 0.9823040274613085\n",
      "adjusted balanced accuracy = 0.7433012788201654\n",
      "balanced accuracy = 0.8716506394100827\n",
      "average precision = 0.6628814010907589\n",
      "f1 score = 0.804447351209941\n",
      "precision = 0.8686440677966102\n",
      "recall = 0.7490864799025578\n",
      "roc = 0.8716506394100827\n",
      "confusion matrix\n",
      " = [[31965   186]\n",
      " [  412  1230]]\n"
     ]
    }
   ],
   "source": [
    "evaluate_classifier(histgbc, X_train, y_train)\n",
    "evaluate_classifier(histgbc, X_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true,
     "source_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 3 folds for each of 10 candidates, totalling 30 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done   2 tasks      | elapsed:   35.6s\n",
      "[Parallel(n_jobs=-1)]: Done  22 out of  30 | elapsed: 17.9min remaining:  6.5min\n",
      "[Parallel(n_jobs=-1)]: Done  30 out of  30 | elapsed: 25.8min finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RF grid search saved\n"
     ]
    }
   ],
   "source": [
    "rf_search = model_selection.RandomizedSearchCV(\n",
    "    ensemble.RandomForestClassifier(), \n",
    "    param_distributions={\n",
    "        'n_estimators': [10, 100, 300],\n",
    "        'criterion': ['gini', 'entropy'],\n",
    "        'class_weight': [{True: 2, False: 1}, {True: 3, False: 1}, 'balanced', None],\n",
    "        'max_depth': [None],\n",
    "        'min_samples_leaf': [20, 50],\n",
    "        'min_samples_split': [20, 50]\n",
    "    },\n",
    "    n_jobs=-1,\n",
    "    cv=3,\n",
    "    refit='f1',\n",
    "    scoring=['neg_log_loss', 'f1', 'roc_auc'],\n",
    "    verbose=5)\n",
    "rf_search.fit(X_train, y_train)\n",
    "\n",
    "with open(f'./model_gridsearch/{loss_type}/rf', 'wb') as f:\n",
    "    pickle.dump(rf_search, f)\n",
    "    print('RF grid search saved')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "with open(f'./model_gridsearch/{loss_type}/rf', 'rb') as f:\n",
    "    rf_search = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true,
     "source_hidden": true
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>params</th>\n",
       "      <th>overall_rank</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>41</td>\n",
       "      <td>{'class_weight': {True: 3, False: 1}, 'criterion': 'entropy', 'max_depth': None, 'min_samples_leaf': 20, 'min_samples_split': 50, 'n_estimators': 300}</td>\n",
       "      <td>14.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>38</td>\n",
       "      <td>{'class_weight': {True: 3, False: 1}, 'criterion': 'entropy', 'max_depth': None, 'min_samples_leaf': 20, 'min_samples_split': 20, 'n_estimators': 300}</td>\n",
       "      <td>14.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>40</td>\n",
       "      <td>{'class_weight': {True: 3, False: 1}, 'criterion': 'entropy', 'max_depth': None, 'min_samples_leaf': 20, 'min_samples_split': 50, 'n_estimators': 100}</td>\n",
       "      <td>15.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                                                                                                                    params  \\\n",
       "41  {'class_weight': {True: 3, False: 1}, 'criterion': 'entropy', 'max_depth': None, 'min_samples_leaf': 20, 'min_samples_split': 50, 'n_estimators': 300}   \n",
       "38  {'class_weight': {True: 3, False: 1}, 'criterion': 'entropy', 'max_depth': None, 'min_samples_leaf': 20, 'min_samples_split': 20, 'n_estimators': 300}   \n",
       "40  {'class_weight': {True: 3, False: 1}, 'criterion': 'entropy', 'max_depth': None, 'min_samples_leaf': 20, 'min_samples_split': 50, 'n_estimators': 100}   \n",
       "\n",
       "    overall_rank  \n",
       "41  14.0          \n",
       "38  14.0          \n",
       "40  15.0          "
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_search_result(rf_search.cv_results_).iloc[:3][['params', 'overall_rank']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true,
     "source_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend ThreadingBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done  34 tasks      | elapsed:   16.6s\n",
      "[Parallel(n_jobs=-1)]: Done 100 out of 100 | elapsed:   43.5s finished\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "RandomForestClassifier(class_weight={False: 1, True: 3}, criterion='entropy',\n",
       "                       min_samples_leaf=20, min_samples_split=50, n_jobs=-1,\n",
       "                       verbose=1)"
      ]
     },
     "execution_count": 155,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rfc = ensemble.RandomForestClassifier(n_estimators=100, criterion='entropy', min_samples_split=50, min_samples_leaf=20,\n",
    "                                      class_weight={True: 3, False: 1}, max_depth=None,\n",
    "                                      verbose=1, n_jobs=-1)\n",
    "rfc.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true,
     "source_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing results:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=8)]: Using backend ThreadingBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=8)]: Done  34 tasks      | elapsed:    0.5s\n",
      "[Parallel(n_jobs=8)]: Done 100 out of 100 | elapsed:    1.6s finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy = 0.9803012517632205\n",
      "adjusted balanced accuracy = 0.8201195435935185\n",
      "balanced accuracy = 0.9100597717967592\n",
      "average precision = 0.6539043683531418\n",
      "f1 score = 0.8031283888140384\n",
      "precision = 0.775922280779732\n",
      "recall = 0.8323116741588339\n",
      "roc = 0.9100597717967592\n",
      "confusion matrix\n",
      " = [[285920   3529]\n",
      " [  2462  12220]]\n",
      "Testing results:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=8)]: Using backend ThreadingBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=8)]: Done  34 tasks      | elapsed:    0.0s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy = 0.9747284940668186\n",
      "adjusted balanced accuracy = 0.7432175909078185\n",
      "balanced accuracy = 0.8716087954539092\n",
      "average precision = 0.570592400504317\n",
      "f1 score = 0.7473372781065089\n",
      "precision = 0.7377336448598131\n",
      "recall = 0.7571942446043165\n",
      "roc = 0.8716087954539092\n",
      "confusion matrix\n",
      " = [[31676   449]\n",
      " [  405  1263]]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=8)]: Done 100 out of 100 | elapsed:    0.1s finished\n"
     ]
    }
   ],
   "source": [
    "evaluate_classifier(rfc, X_train, y_train)\n",
    "evaluate_classifier(rfc, X_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Preparing length_loss training/test split:\n",
      "Train size: 304131; test size: 33793\n"
     ]
    }
   ],
   "source": [
    "# Preparing train / test split\n",
    "loss_target = 'length_loss'\n",
    "print(f'Preparing {loss_target} training/test split:')\n",
    "X_train, X_test, y_train, y_test = sklearn.model_selection.train_test_split(\n",
    "    X, y[loss_target], test_size=0.1)\n",
    "print(f'Train size: {len(y_train)}; test size: {len(y_test)}')\n",
    "\n",
    "loss_type = 'length'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cross Validation of Classifiers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true,
     "source_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logistic Regression: \n",
      "Cross validation results:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done   2 out of   5 | elapsed:  4.9min remaining:  7.3min\n",
      "[Parallel(n_jobs=-1)]: Done   5 out of   5 | elapsed:  5.0min finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fit_time = 290.5066019058228 +- 3.948521008073799\n",
      "score_time = 0.25540361404418943 +- 0.1858980041918588\n",
      "test_accuracy = 0.9024188861203261 +- 0.0015908650531483324\n",
      "test_adjusted balanced accuracy = 0.8022275905322817 +- 0.003502771893031312\n",
      "test_balanced accuracy = 0.9011137952661408 +- 0.001751385946515656\n",
      "test_average precision = 0.8771733815097837 +- 0.0012902790076594222\n",
      "test_f1 score = 0.9112826797647795 +- 0.0010139233938009625\n",
      "test_precision = 0.9078736845441302 +- 0.002339758666922401\n",
      "test_recall = 0.9147212521360147 +- 0.003268234101030392\n",
      "test_roc = 0.9011137952661408 +- 0.0017513859465156092\n",
      "\n",
      "Neural Network: \n",
      "Cross validation results:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done   2 out of   5 | elapsed:  9.8min remaining: 14.6min\n",
      "[Parallel(n_jobs=-1)]: Done   5 out of   5 | elapsed: 13.0min finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fit_time = 631.7624930381775 +- 201.59075639999045\n",
      "score_time = 0.6668417453765869 +- 0.14145483104345852\n",
      "test_accuracy = 0.9400545701621386 +- 0.001935650272816269\n",
      "test_adjusted balanced accuracy = 0.8793885089011508 +- 0.003441587786071949\n",
      "test_balanced accuracy = 0.9396942544505753 +- 0.0017207938930359746\n",
      "test_average precision = 0.9243378605728948 +- 0.0027080062592122474\n",
      "test_f1 score = 0.9451976197896871 +- 0.001787115276784873\n",
      "test_precision = 0.9468576731138849 +- 0.004226756596768771\n",
      "test_recall = 0.9435555309301649 +- 0.005877042873594234\n",
      "test_roc = 0.9396942544505753 +- 0.0017207938930359746\n",
      "\n",
      "Decision Tree: \n",
      "Cross validation results:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done   2 out of   5 | elapsed:   52.1s remaining:  1.3min\n",
      "[Parallel(n_jobs=-1)]: Done   5 out of   5 | elapsed:   53.4s finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fit_time = 49.074630165100096 +- 3.148307690779517\n",
      "score_time = 0.28180809020996095 +- 0.05426528732141355\n",
      "test_accuracy = 0.9174962361509973 +- 0.0038551847523225664\n",
      "test_adjusted balanced accuracy = 0.8349881619169418 +- 0.007198401898707589\n",
      "test_balanced accuracy = 0.917494080958471 +- 0.0035992009493537947\n",
      "test_average precision = 0.8992976505484096 +- 0.0032657762753574788\n",
      "test_f1 score = 0.9241668056751967 +- 0.003424243903860505\n",
      "test_precision = 0.9308586541715866 +- 0.003482307583501428\n",
      "test_recall = 0.9175819162954376 +- 0.0073002263788483086\n",
      "test_roc = 0.917494080958471 +- 0.003599200949353778\n",
      "\n",
      "Gradient Boosting 3000: \n",
      "Cross validation results:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done   2 out of   5 | elapsed: 29.4min remaining: 44.2min\n",
      "[Parallel(n_jobs=-1)]: Done   5 out of   5 | elapsed: 32.9min finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fit_time = 1820.4230882644654 +- 210.33956796884036\n",
      "score_time = 22.420446634292603 +- 3.98810454941567\n",
      "test_accuracy = 0.9488405699153954 +- 0.0019744162828072578\n",
      "test_adjusted balanced accuracy = 0.897364916971244 +- 0.0038716990452632993\n",
      "test_balanced accuracy = 0.9486824584856219 +- 0.0019358495226316497\n",
      "test_average precision = 0.9357531627379453 +- 0.0022758546186888353\n",
      "test_f1 score = 0.9531731466644828 +- 0.001750735767607105\n",
      "test_precision = 0.9560161903023335 +- 0.0021511186290022848\n",
      "test_recall = 0.9503490041003844 +- 0.0030505095486591752\n",
      "test_roc = 0.9486824584856219 +- 0.0019358495226316497\n",
      "\n",
      "Gradient Boosting 500: \n",
      "Cross validation results:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done   2 out of   5 | elapsed:  8.8min remaining: 13.2min\n",
      "[Parallel(n_jobs=-1)]: Done   5 out of   5 | elapsed:  8.9min finished\n",
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 8 concurrent workers.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fit_time = 514.4974844455719 +- 7.983372244866797\n",
      "score_time = 9.463599920272827 +- 0.9137246103907198\n",
      "test_accuracy = 0.9428895255322424 +- 0.000789689007064461\n",
      "test_adjusted balanced accuracy = 0.8855600440910578 +- 0.0016113503007030175\n",
      "test_balanced accuracy = 0.9427800220455289 +- 0.0008056751503515087\n",
      "test_average precision = 0.9288215440948392 +- 0.0015587473134579634\n",
      "test_f1 score = 0.9476738677551799 +- 0.0006796041867783141\n",
      "test_precision = 0.951449858732453 +- 0.0024743564073439954\n",
      "test_recall = 0.9439311010094471 +- 0.0027701033624141004\n",
      "test_roc = 0.9427800220455289 +- 0.0008056751503516057\n",
      "\n",
      "Random Forest: \n",
      "Cross validation results:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Done   2 out of   5 | elapsed:  4.9min remaining:  7.3min\n",
      "[Parallel(n_jobs=-1)]: Done   5 out of   5 | elapsed:  4.9min finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fit_time = 287.04450035095215 +- 2.8866890331468635\n",
      "score_time = 2.5313113212585447 +- 0.778642261984936\n",
      "test_accuracy = 0.9172506197390182 +- 0.002700119680134159\n",
      "test_adjusted balanced accuracy = 0.8343103831976263 +- 0.005185379489086456\n",
      "test_balanced accuracy = 0.9171551915988132 +- 0.002592689744543228\n",
      "test_average precision = 0.8986539417785083 +- 0.002156525543034616\n",
      "test_f1 score = 0.9240045629987911 +- 0.002234646814028764\n",
      "test_precision = 0.9299093862536513 +- 0.001798465381408605\n",
      "test_recall = 0.9181770169897341 +- 0.004102181965911519\n",
      "test_roc = 0.9171551915988132 +- 0.0025926897445433393\n",
      "\n"
     ]
    }
   ],
   "source": [
    "loss_target = 'length_loss'\n",
    "energy_models = {\n",
    "    'Logistic Regression': linear_model.LogisticRegression(C=10.0, class_weight=None, max_iter=1000),\n",
    "    'Neural Network': neural_network.MLPClassifier(alpha=1e-1,\n",
    "                                  hidden_layer_sizes=(100,),\n",
    "                                  max_iter=1000,\n",
    "                                  learning_rate_init=1e-4,\n",
    "                                  early_stopping=True, \n",
    "                                  n_iter_no_change=25),\n",
    "    'Decision Tree': tree.DecisionTreeClassifier(max_depth=None, min_samples_leaf=20, min_samples_split=50,\n",
    "                                       splitter='random', criterion='entropy', class_weight=None),\n",
    "    'Gradient Boosting 3000': ensemble.HistGradientBoostingClassifier(max_iter=3000, learning_rate=0.1, n_iter_no_change=100,\n",
    "                                                  min_samples_leaf=50, \n",
    "                                                  warm_start=False),\n",
    "    'Gradient Boosting 500': ensemble.HistGradientBoostingClassifier(max_iter=500, learning_rate=0.1, n_iter_no_change=100,\n",
    "                                                  min_samples_leaf=50, \n",
    "                                                  warm_start=False),\n",
    "    'Random Forest': ensemble.RandomForestClassifier(n_estimators=100, criterion='entropy', min_samples_split=20, min_samples_leaf=20,\n",
    "                                      class_weight=None, max_depth=None)\n",
    "}\n",
    "\n",
    "for name, model in energy_models.items():\n",
    "    print(f'{name}: ')\n",
    "    cross_validate_classifier(model, X, y[loss_target], cv=sklearn.model_selection.KFold(n_splits=5, shuffle=True, random_state=42), verbose=5, n_jobs=-1)\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true,
     "source_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 3 folds for each of 10 candidates, totalling 30 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done   2 tasks      | elapsed:  1.3min\n",
      "[Parallel(n_jobs=-1)]: Done  22 out of  30 | elapsed: 14.7min remaining:  5.3min\n",
      "[Parallel(n_jobs=-1)]: Done  30 out of  30 | elapsed: 16.8min finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LRC grid search saved\n"
     ]
    }
   ],
   "source": [
    "lrc_search = model_selection.RandomizedSearchCV(\n",
    "    linear_model.LogisticRegression(), \n",
    "    param_distributions={\n",
    "        'C': [0.01, 0.10, 1.00, 10.00],\n",
    "        'class_weight': [{True: 2, False: 1}, {True: 3, False: 1}, 'balanced', None],\n",
    "        'max_iter': [1000]\n",
    "    }, \n",
    "    n_jobs=-1,\n",
    "    refit='f1',\n",
    "    scoring=['neg_log_loss', 'f1', 'roc_auc'],\n",
    "    cv=3,\n",
    "    verbose=5)\n",
    "lrc_search.fit(X_train, y_train)\n",
    "\n",
    "with open(f'./model_gridsearch/{loss_type}/lrc', 'wb') as f:\n",
    "    pickle.dump(lrc_search, f)\n",
    "    print('LRC grid search saved')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true,
     "source_hidden": true
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>params</th>\n",
       "      <th>overall_rank</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>{'C': 1.0, 'class_weight': 'balanced', 'max_iter': 1000}</td>\n",
       "      <td>2.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>15</td>\n",
       "      <td>{'C': 10.0, 'class_weight': None, 'max_iter': 1000}</td>\n",
       "      <td>2.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>11</td>\n",
       "      <td>{'C': 1.0, 'class_weight': None, 'max_iter': 1000}</td>\n",
       "      <td>3.5</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                      params  overall_rank\n",
       "10  {'C': 1.0, 'class_weight': 'balanced', 'max_iter': 1000}  2.5         \n",
       "15  {'C': 10.0, 'class_weight': None, 'max_iter': 1000}       2.5         \n",
       "11  {'C': 1.0, 'class_weight': None, 'max_iter': 1000}        3.5         "
      ]
     },
     "execution_count": 109,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "with open(f'./model_gridsearch/{loss_type}/lrc', 'rb') as f:\n",
    "    lrc_search = pickle.load(f)\n",
    "get_search_result(lrc_search.cv_results_).iloc[:3][['params', 'overall_rank']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true,
     "source_hidden": true
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>mean_fit_time</th>\n",
       "      <th>mean_score_time</th>\n",
       "      <th>params</th>\n",
       "      <th>mean_test_neg_log_loss</th>\n",
       "      <th>rank_test_neg_log_loss</th>\n",
       "      <th>mean_test_f1</th>\n",
       "      <th>rank_test_f1</th>\n",
       "      <th>mean_test_roc_auc</th>\n",
       "      <th>rank_test_roc_auc</th>\n",
       "      <th>mean_test_balanced_accuracy</th>\n",
       "      <th>rank_test_balanced_accuracy</th>\n",
       "      <th>mean_test_accuracy</th>\n",
       "      <th>rank_test_accuracy</th>\n",
       "      <th>mean_test_recall</th>\n",
       "      <th>rank_test_recall</th>\n",
       "      <th>mean_test_precision</th>\n",
       "      <th>rank_test_precision</th>\n",
       "      <th>overall_rank</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>366.617361</td>\n",
       "      <td>1.293812</td>\n",
       "      <td>{'C': 1.0, 'class_weight': 'balanced', 'max_iter': 1000}</td>\n",
       "      <td>-0.240357</td>\n",
       "      <td>5</td>\n",
       "      <td>0.909953</td>\n",
       "      <td>4</td>\n",
       "      <td>0.965043</td>\n",
       "      <td>3</td>\n",
       "      <td>0.902237</td>\n",
       "      <td>1</td>\n",
       "      <td>0.902223</td>\n",
       "      <td>3</td>\n",
       "      <td>0.902094</td>\n",
       "      <td>13</td>\n",
       "      <td>0.917951</td>\n",
       "      <td>2</td>\n",
       "      <td>2.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>15</td>\n",
       "      <td>287.711773</td>\n",
       "      <td>0.427665</td>\n",
       "      <td>{'C': 10.0, 'class_weight': None, 'max_iter': 1000}</td>\n",
       "      <td>-0.238882</td>\n",
       "      <td>1</td>\n",
       "      <td>0.911485</td>\n",
       "      <td>1</td>\n",
       "      <td>0.965070</td>\n",
       "      <td>2</td>\n",
       "      <td>0.901374</td>\n",
       "      <td>4</td>\n",
       "      <td>0.902674</td>\n",
       "      <td>1</td>\n",
       "      <td>0.915009</td>\n",
       "      <td>9</td>\n",
       "      <td>0.907989</td>\n",
       "      <td>5</td>\n",
       "      <td>2.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>11</td>\n",
       "      <td>368.384006</td>\n",
       "      <td>1.252101</td>\n",
       "      <td>{'C': 1.0, 'class_weight': None, 'max_iter': 1000}</td>\n",
       "      <td>-0.238978</td>\n",
       "      <td>2</td>\n",
       "      <td>0.911409</td>\n",
       "      <td>2</td>\n",
       "      <td>0.965036</td>\n",
       "      <td>4</td>\n",
       "      <td>0.901306</td>\n",
       "      <td>5</td>\n",
       "      <td>0.902598</td>\n",
       "      <td>2</td>\n",
       "      <td>0.914864</td>\n",
       "      <td>10</td>\n",
       "      <td>0.907981</td>\n",
       "      <td>6</td>\n",
       "      <td>3.5</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    mean_fit_time  mean_score_time  \\\n",
       "10  366.617361     1.293812          \n",
       "15  287.711773     0.427665          \n",
       "11  368.384006     1.252101          \n",
       "\n",
       "                                                      params  \\\n",
       "10  {'C': 1.0, 'class_weight': 'balanced', 'max_iter': 1000}   \n",
       "15  {'C': 10.0, 'class_weight': None, 'max_iter': 1000}        \n",
       "11  {'C': 1.0, 'class_weight': None, 'max_iter': 1000}         \n",
       "\n",
       "    mean_test_neg_log_loss  rank_test_neg_log_loss  mean_test_f1  \\\n",
       "10 -0.240357                5                       0.909953       \n",
       "15 -0.238882                1                       0.911485       \n",
       "11 -0.238978                2                       0.911409       \n",
       "\n",
       "    rank_test_f1  mean_test_roc_auc  rank_test_roc_auc  \\\n",
       "10  4             0.965043           3                   \n",
       "15  1             0.965070           2                   \n",
       "11  2             0.965036           4                   \n",
       "\n",
       "    mean_test_balanced_accuracy  rank_test_balanced_accuracy  \\\n",
       "10  0.902237                     1                             \n",
       "15  0.901374                     4                             \n",
       "11  0.901306                     5                             \n",
       "\n",
       "    mean_test_accuracy  rank_test_accuracy  mean_test_recall  \\\n",
       "10  0.902223            3                   0.902094           \n",
       "15  0.902674            1                   0.915009           \n",
       "11  0.902598            2                   0.914864           \n",
       "\n",
       "    rank_test_recall  mean_test_precision  rank_test_precision  overall_rank  \n",
       "10  13                0.917951             2                    2.5           \n",
       "15  9                 0.907989             5                    2.5           \n",
       "11  10                0.907981             6                    3.5           "
      ]
     },
     "execution_count": 110,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_search_result(lrc_search.cv_results_).iloc[:3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true,
     "source_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "[Parallel(n_jobs=1)]: Done   1 out of   1 | elapsed:  2.1min finished\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "LogisticRegression(C=10.0, max_iter=1000, verbose=1)"
      ]
     },
     "execution_count": 115,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lrc = linear_model.LogisticRegression(C=10.0, class_weight=None, max_iter=1000, verbose=1)\n",
    "lrc.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true,
     "source_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing results:\n",
      "accuracy = 0.8999201017962299\n",
      "adjusted balanced accuracy = 0.7966716394014481\n",
      "balanced accuracy = 0.8983358197007241\n",
      "average precision = 0.8730559776833398\n",
      "f1 score = 0.9091543999140432\n",
      "precision = 0.9032826261008807\n",
      "recall = 0.9151030119504677\n",
      "roc = 0.8983358197007241\n",
      "confusion matrix\n",
      " = [[13488  1812]\n",
      " [ 1570 16923]]\n"
     ]
    }
   ],
   "source": [
    "# with C = 10.0\n",
    "evaluate_classifier(lrc, X_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Neural Networks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true,
     "source_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 3 folds for each of 10 candidates, totalling 30 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done   2 tasks      | elapsed:  2.2min\n",
      "[Parallel(n_jobs=-1)]: Done  22 out of  30 | elapsed: 41.5min remaining: 15.1min\n",
      "[Parallel(n_jobs=-1)]: Done  30 out of  30 | elapsed: 74.2min finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NN grid search saved\n"
     ]
    }
   ],
   "source": [
    "nn_search = model_selection.RandomizedSearchCV(\n",
    "    neural_network.MLPClassifier(), \n",
    "    param_distributions={\n",
    "        'alpha': [1e-5, 1e-4, 1e-3, 1e-2],\n",
    "        'hidden_layer_sizes': [(100,), (150,), (200,)],\n",
    "        'learning_rate_init': [1e-4, 1e-3, 1e-2, 1e-1],\n",
    "        'max_iter': [1000],\n",
    "        'early_stopping': [True],\n",
    "        'n_iter_no_change': [5, 25]\n",
    "    },\n",
    "    n_jobs=-1,\n",
    "    refit='f1',\n",
    "    scoring=['neg_log_loss', 'f1', 'roc_auc'],\n",
    "    cv=3,\n",
    "    verbose=5)\n",
    "nn_search.fit(X_train, y_train)\n",
    "\n",
    "with open(f'./model_gridsearch/{loss_type}/nn', 'wb') as f:\n",
    "    pickle.dump(nn_search, f)\n",
    "    print('NN grid search saved')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(f'./model_gridsearch/{loss_type}/nn', 'rb') as f:\n",
    "    nn_length_search = pickle.load(f)\n",
    "    \n",
    "nn_length = nn_length_search.best_estimator_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true,
     "source_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 1, loss = 0.38765793\n",
      "Validation score: 0.893010\n",
      "Iteration 2, loss = 0.28284540\n",
      "Validation score: 0.907674\n",
      "Iteration 3, loss = 0.25682546\n",
      "Validation score: 0.912507\n",
      "Iteration 4, loss = 0.24117427\n",
      "Validation score: 0.916716\n",
      "Iteration 5, loss = 0.23002177\n",
      "Validation score: 0.919741\n",
      "Iteration 6, loss = 0.22143328\n",
      "Validation score: 0.922667\n",
      "Iteration 7, loss = 0.21442067\n",
      "Validation score: 0.924969\n",
      "Iteration 8, loss = 0.20837236\n",
      "Validation score: 0.924837\n",
      "Iteration 9, loss = 0.20334921\n",
      "Validation score: 0.927764\n",
      "Iteration 10, loss = 0.19901423\n",
      "Validation score: 0.928388\n",
      "Iteration 11, loss = 0.19504558\n",
      "Validation score: 0.930262\n",
      "Iteration 12, loss = 0.19158815\n",
      "Validation score: 0.930788\n",
      "Iteration 13, loss = 0.18853192\n",
      "Validation score: 0.930854\n",
      "Iteration 14, loss = 0.18576522\n",
      "Validation score: 0.931347\n",
      "Iteration 15, loss = 0.18320153\n",
      "Validation score: 0.932169\n",
      "Iteration 16, loss = 0.18093301\n",
      "Validation score: 0.932531\n",
      "Iteration 17, loss = 0.17885199\n",
      "Validation score: 0.933846\n",
      "Iteration 18, loss = 0.17686308\n",
      "Validation score: 0.933452\n",
      "Iteration 19, loss = 0.17524261\n",
      "Validation score: 0.935161\n",
      "Iteration 20, loss = 0.17349231\n",
      "Validation score: 0.934438\n",
      "Iteration 21, loss = 0.17175674\n",
      "Validation score: 0.934898\n",
      "Iteration 22, loss = 0.17046409\n",
      "Validation score: 0.936082\n",
      "Iteration 23, loss = 0.16890290\n",
      "Validation score: 0.935622\n",
      "Iteration 24, loss = 0.16760115\n",
      "Validation score: 0.936674\n",
      "Iteration 25, loss = 0.16651610\n",
      "Validation score: 0.937200\n",
      "Iteration 26, loss = 0.16548153\n",
      "Validation score: 0.937627\n",
      "Iteration 27, loss = 0.16432177\n",
      "Validation score: 0.937660\n",
      "Iteration 28, loss = 0.16326120\n",
      "Validation score: 0.937726\n",
      "Iteration 29, loss = 0.16229413\n",
      "Validation score: 0.938121\n",
      "Iteration 30, loss = 0.16128128\n",
      "Validation score: 0.938482\n",
      "Iteration 31, loss = 0.16058687\n",
      "Validation score: 0.938219\n",
      "Iteration 32, loss = 0.15978998\n",
      "Validation score: 0.938680\n",
      "Iteration 33, loss = 0.15886883\n",
      "Validation score: 0.939502\n",
      "Iteration 34, loss = 0.15819266\n",
      "Validation score: 0.939732\n",
      "Iteration 35, loss = 0.15753721\n",
      "Validation score: 0.939962\n",
      "Iteration 36, loss = 0.15675923\n",
      "Validation score: 0.939436\n",
      "Iteration 37, loss = 0.15619284\n",
      "Validation score: 0.940060\n",
      "Iteration 38, loss = 0.15553162\n",
      "Validation score: 0.941047\n",
      "Iteration 39, loss = 0.15488870\n",
      "Validation score: 0.939699\n",
      "Iteration 40, loss = 0.15424484\n",
      "Validation score: 0.940619\n",
      "Iteration 41, loss = 0.15365112\n",
      "Validation score: 0.940455\n",
      "Iteration 42, loss = 0.15311224\n",
      "Validation score: 0.939995\n",
      "Iteration 43, loss = 0.15271768\n",
      "Validation score: 0.941047\n",
      "Iteration 44, loss = 0.15224740\n",
      "Validation score: 0.941704\n",
      "Iteration 45, loss = 0.15157393\n",
      "Validation score: 0.940685\n",
      "Iteration 46, loss = 0.15135305\n",
      "Validation score: 0.942263\n",
      "Iteration 47, loss = 0.15081878\n",
      "Validation score: 0.941343\n",
      "Iteration 48, loss = 0.15032604\n",
      "Validation score: 0.941178\n",
      "Iteration 49, loss = 0.14989979\n",
      "Validation score: 0.941968\n",
      "Iteration 50, loss = 0.14949399\n",
      "Validation score: 0.941474\n",
      "Iteration 51, loss = 0.14904280\n",
      "Validation score: 0.941507\n",
      "Iteration 52, loss = 0.14866686\n",
      "Validation score: 0.941540\n",
      "Iteration 53, loss = 0.14828730\n",
      "Validation score: 0.941935\n",
      "Iteration 54, loss = 0.14794977\n",
      "Validation score: 0.941277\n",
      "Iteration 55, loss = 0.14762308\n",
      "Validation score: 0.942099\n",
      "Iteration 56, loss = 0.14738833\n",
      "Validation score: 0.941869\n",
      "Iteration 57, loss = 0.14706181\n",
      "Validation score: 0.941935\n",
      "Iteration 58, loss = 0.14675236\n",
      "Validation score: 0.940915\n",
      "Iteration 59, loss = 0.14657798\n",
      "Validation score: 0.941770\n",
      "Iteration 60, loss = 0.14610214\n",
      "Validation score: 0.941507\n",
      "Iteration 61, loss = 0.14576775\n",
      "Validation score: 0.940554\n",
      "Iteration 62, loss = 0.14562066\n",
      "Validation score: 0.941639\n",
      "Iteration 63, loss = 0.14528665\n",
      "Validation score: 0.941639\n",
      "Iteration 64, loss = 0.14507580\n",
      "Validation score: 0.941704\n",
      "Iteration 65, loss = 0.14469498\n",
      "Validation score: 0.942099\n",
      "Iteration 66, loss = 0.14472978\n",
      "Validation score: 0.942790\n",
      "Iteration 67, loss = 0.14430019\n",
      "Validation score: 0.942592\n",
      "Iteration 68, loss = 0.14403368\n",
      "Validation score: 0.943184\n",
      "Iteration 69, loss = 0.14388987\n",
      "Validation score: 0.942231\n",
      "Iteration 70, loss = 0.14372362\n",
      "Validation score: 0.942592\n",
      "Iteration 71, loss = 0.14353236\n",
      "Validation score: 0.941376\n",
      "Iteration 72, loss = 0.14334131\n",
      "Validation score: 0.942263\n",
      "Iteration 73, loss = 0.14319054\n",
      "Validation score: 0.941639\n",
      "Iteration 74, loss = 0.14298750\n",
      "Validation score: 0.942296\n",
      "Iteration 75, loss = 0.14274634\n",
      "Validation score: 0.941113\n",
      "Iteration 76, loss = 0.14255251\n",
      "Validation score: 0.941704\n",
      "Iteration 77, loss = 0.14224033\n",
      "Validation score: 0.942000\n",
      "Iteration 78, loss = 0.14211250\n",
      "Validation score: 0.942296\n",
      "Iteration 79, loss = 0.14205731\n",
      "Validation score: 0.942000\n",
      "Iteration 80, loss = 0.14174364\n",
      "Validation score: 0.942033\n",
      "Iteration 81, loss = 0.14185219\n",
      "Validation score: 0.941672\n",
      "Iteration 82, loss = 0.14145838\n",
      "Validation score: 0.941770\n",
      "Iteration 83, loss = 0.14147803\n",
      "Validation score: 0.941869\n",
      "Iteration 84, loss = 0.14122499\n",
      "Validation score: 0.941836\n",
      "Iteration 85, loss = 0.14094003\n",
      "Validation score: 0.942329\n",
      "Iteration 86, loss = 0.14079096\n",
      "Validation score: 0.942263\n",
      "Iteration 87, loss = 0.14087385\n",
      "Validation score: 0.942066\n",
      "Iteration 88, loss = 0.14069236\n",
      "Validation score: 0.942494\n",
      "Iteration 89, loss = 0.14069981\n",
      "Validation score: 0.942066\n",
      "Iteration 90, loss = 0.14038564\n",
      "Validation score: 0.942296\n",
      "Iteration 91, loss = 0.14013022\n",
      "Validation score: 0.942790\n",
      "Iteration 92, loss = 0.14019510\n",
      "Validation score: 0.942329\n",
      "Iteration 93, loss = 0.14019731\n",
      "Validation score: 0.942000\n",
      "Iteration 94, loss = 0.13976920\n",
      "Validation score: 0.942822\n",
      "Validation score did not improve more than tol=0.000100 for 25 consecutive epochs. Stopping.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "MLPClassifier(alpha=0.1, early_stopping=True, learning_rate_init=0.0001,\n",
       "              max_iter=1000, n_iter_no_change=25, verbose=True)"
      ]
     },
     "execution_count": 120,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# alpha and hidden layer sizes chosen from parameter search\n",
    "nn = neural_network.MLPClassifier(alpha=1e-1,\n",
    "                                  hidden_layer_sizes=(100,),\n",
    "                                  max_iter=1000,\n",
    "                                  learning_rate_init=1e-4,\n",
    "                                  early_stopping=True, \n",
    "                                  n_iter_no_change=25,\n",
    "                                  verbose=True)\n",
    "nn.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true,
     "source_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing results:\n",
      "accuracy = 0.9398100198265913\n",
      "adjusted balanced accuracy = 0.8787049047705384\n",
      "balanced accuracy = 0.9393524523852692\n",
      "average precision = 0.9234918374157471\n",
      "f1 score = 0.9449615759281307\n",
      "precision = 0.9457292964306993\n",
      "recall = 0.9441951008489698\n",
      "roc = 0.9393524523852692\n",
      "confusion matrix\n",
      " = [[14298  1002]\n",
      " [ 1032 17461]]\n"
     ]
    }
   ],
   "source": [
    "evaluate_classifier(nn, X_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Decision Trees"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true,
     "source_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 3 folds for each of 10 candidates, totalling 30 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done   2 tasks      | elapsed:  1.3min\n",
      "[Parallel(n_jobs=-1)]: Done  22 out of  30 | elapsed:  5.2min remaining:  1.9min\n",
      "[Parallel(n_jobs=-1)]: Done  30 out of  30 | elapsed:  6.6min finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DT grid search saved\n"
     ]
    }
   ],
   "source": [
    "tree_search = model_selection.RandomizedSearchCV(\n",
    "    tree.DecisionTreeClassifier(),\n",
    "    param_distributions={\n",
    "        'criterion': ['gini', 'entropy'],\n",
    "        'splitter': ['random', 'best'],\n",
    "        'class_weight': [{True: 2, False: 1}, {True: 3, False: 1}, 'balanced', None],\n",
    "        'max_depth': [None],\n",
    "        'min_samples_leaf': [20, 50],\n",
    "        'min_samples_split': [20, 50]\n",
    "    },\n",
    "    n_jobs=-1,\n",
    "    cv=3,\n",
    "    refit='f1',\n",
    "    scoring=['neg_log_loss', 'f1', 'roc_auc'],\n",
    "    verbose=5)\n",
    "tree_search.fit(X_train, y_train)\n",
    "\n",
    "with open(f'./model_gridsearch/{loss_type}/dt', 'wb') as f:\n",
    "    pickle.dump(tree_search, f)\n",
    "    print('DT grid search saved')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true,
     "source_hidden": true
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>mean_fit_time</th>\n",
       "      <th>mean_score_time</th>\n",
       "      <th>params</th>\n",
       "      <th>mean_test_neg_log_loss</th>\n",
       "      <th>rank_test_neg_log_loss</th>\n",
       "      <th>mean_test_f1</th>\n",
       "      <th>rank_test_f1</th>\n",
       "      <th>mean_test_roc_auc</th>\n",
       "      <th>rank_test_roc_auc</th>\n",
       "      <th>mean_test_balanced_accuracy</th>\n",
       "      <th>rank_test_balanced_accuracy</th>\n",
       "      <th>mean_test_accuracy</th>\n",
       "      <th>rank_test_accuracy</th>\n",
       "      <th>mean_test_recall</th>\n",
       "      <th>rank_test_recall</th>\n",
       "      <th>mean_test_precision</th>\n",
       "      <th>rank_test_precision</th>\n",
       "      <th>overall_rank</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>42</td>\n",
       "      <td>62.163097</td>\n",
       "      <td>1.064836</td>\n",
       "      <td>{'class_weight': 'balanced', 'criterion': 'entropy', 'max_depth': None, 'min_samples_leaf': 20, 'min_samples_split': 50, 'splitter': 'random'}</td>\n",
       "      <td>-0.528611</td>\n",
       "      <td>38</td>\n",
       "      <td>0.921374</td>\n",
       "      <td>3</td>\n",
       "      <td>0.969659</td>\n",
       "      <td>32</td>\n",
       "      <td>0.915993</td>\n",
       "      <td>1</td>\n",
       "      <td>0.915181</td>\n",
       "      <td>1</td>\n",
       "      <td>0.907480</td>\n",
       "      <td>52</td>\n",
       "      <td>0.935702</td>\n",
       "      <td>6</td>\n",
       "      <td>2.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>40</td>\n",
       "      <td>64.262230</td>\n",
       "      <td>1.049481</td>\n",
       "      <td>{'class_weight': 'balanced', 'criterion': 'entropy', 'max_depth': None, 'min_samples_leaf': 20, 'min_samples_split': 20, 'splitter': 'random'}</td>\n",
       "      <td>-0.558366</td>\n",
       "      <td>43</td>\n",
       "      <td>0.921369</td>\n",
       "      <td>4</td>\n",
       "      <td>0.969011</td>\n",
       "      <td>41</td>\n",
       "      <td>0.915648</td>\n",
       "      <td>3</td>\n",
       "      <td>0.915023</td>\n",
       "      <td>3</td>\n",
       "      <td>0.909095</td>\n",
       "      <td>48</td>\n",
       "      <td>0.933980</td>\n",
       "      <td>12</td>\n",
       "      <td>3.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>35</td>\n",
       "      <td>145.937758</td>\n",
       "      <td>1.018453</td>\n",
       "      <td>{'class_weight': 'balanced', 'criterion': 'gini', 'max_depth': None, 'min_samples_leaf': 20, 'min_samples_split': 50, 'splitter': 'best'}</td>\n",
       "      <td>-0.697075</td>\n",
       "      <td>49</td>\n",
       "      <td>0.921337</td>\n",
       "      <td>7</td>\n",
       "      <td>0.965207</td>\n",
       "      <td>55</td>\n",
       "      <td>0.915881</td>\n",
       "      <td>2</td>\n",
       "      <td>0.915109</td>\n",
       "      <td>2</td>\n",
       "      <td>0.907780</td>\n",
       "      <td>51</td>\n",
       "      <td>0.935309</td>\n",
       "      <td>8</td>\n",
       "      <td>4.5</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    mean_fit_time  mean_score_time  \\\n",
       "42  62.163097      1.064836          \n",
       "40  64.262230      1.049481          \n",
       "35  145.937758     1.018453          \n",
       "\n",
       "                                                                                                                                            params  \\\n",
       "42  {'class_weight': 'balanced', 'criterion': 'entropy', 'max_depth': None, 'min_samples_leaf': 20, 'min_samples_split': 50, 'splitter': 'random'}   \n",
       "40  {'class_weight': 'balanced', 'criterion': 'entropy', 'max_depth': None, 'min_samples_leaf': 20, 'min_samples_split': 20, 'splitter': 'random'}   \n",
       "35  {'class_weight': 'balanced', 'criterion': 'gini', 'max_depth': None, 'min_samples_leaf': 20, 'min_samples_split': 50, 'splitter': 'best'}        \n",
       "\n",
       "    mean_test_neg_log_loss  rank_test_neg_log_loss  mean_test_f1  \\\n",
       "42 -0.528611                38                      0.921374       \n",
       "40 -0.558366                43                      0.921369       \n",
       "35 -0.697075                49                      0.921337       \n",
       "\n",
       "    rank_test_f1  mean_test_roc_auc  rank_test_roc_auc  \\\n",
       "42  3             0.969659           32                  \n",
       "40  4             0.969011           41                  \n",
       "35  7             0.965207           55                  \n",
       "\n",
       "    mean_test_balanced_accuracy  rank_test_balanced_accuracy  \\\n",
       "42  0.915993                     1                             \n",
       "40  0.915648                     3                             \n",
       "35  0.915881                     2                             \n",
       "\n",
       "    mean_test_accuracy  rank_test_accuracy  mean_test_recall  \\\n",
       "42  0.915181            1                   0.907480           \n",
       "40  0.915023            3                   0.909095           \n",
       "35  0.915109            2                   0.907780           \n",
       "\n",
       "    rank_test_recall  mean_test_precision  rank_test_precision  overall_rank  \n",
       "42  52                0.935702             6                    2.0           \n",
       "40  48                0.933980             12                   3.5           \n",
       "35  51                0.935309             8                    4.5           "
      ]
     },
     "execution_count": 112,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "with open(f'./model_gridsearch/{loss_type}/dt', 'rb') as f:\n",
    "    dt_search = pickle.load(f)\n",
    "get_search_result(dt_search.cv_results_).iloc[:3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true,
     "source_hidden": true
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DecisionTreeClassifier(criterion='entropy', min_samples_leaf=20,\n",
       "                       min_samples_split=50, splitter='random')"
      ]
     },
     "execution_count": 130,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tree_clf = tree.DecisionTreeClassifier(max_depth=None, min_samples_leaf=20, min_samples_split=50,\n",
    "                                       splitter='random', criterion='entropy', class_weight=None)\n",
    "tree_clf.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true,
     "source_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing results:\n",
      "accuracy = 0.9199538365933774\n",
      "adjusted balanced accuracy = 0.8399156437570974\n",
      "balanced accuracy = 0.9199578218785487\n",
      "average precision = 0.9019965662547493\n",
      "f1 score = 0.9263524735222849\n",
      "precision = 0.9328800175477079\n",
      "recall = 0.9199156437570973\n",
      "roc = 0.9199578218785487\n",
      "confusion matrix\n",
      " = [[14076  1224]\n",
      " [ 1481 17012]]\n"
     ]
    }
   ],
   "source": [
    "evaluate_classifier(tree_clf, X_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Histogram-Based Boosting (Similar to LightGBM)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true,
     "source_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 3 folds for each of 10 candidates, totalling 30 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done   2 tasks      | elapsed: 10.5min\n",
      "[Parallel(n_jobs=-1)]: Done  22 out of  30 | elapsed: 90.1min remaining: 32.8min\n",
      "[Parallel(n_jobs=-1)]: Done  30 out of  30 | elapsed: 117.4min finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "HGB grid search saved\n"
     ]
    }
   ],
   "source": [
    "hgb_search = model_selection.RandomizedSearchCV(\n",
    "    ensemble.HistGradientBoostingClassifier(), \n",
    "    param_distributions={\n",
    "        'max_iter': [500, 1000, 1500, 3000],\n",
    "        'learning_rate': [0.05, 0.1],\n",
    "        'min_samples_leaf': [20, 50],\n",
    "        'warm_start': [False],\n",
    "        'n_iter_no_change': [100]\n",
    "    },\n",
    "    n_jobs=-1,\n",
    "    cv=3,\n",
    "    refit='f1',\n",
    "    scoring=['neg_log_loss', 'f1', 'roc_auc'],\n",
    "    verbose=5)\n",
    "hgb_search.fit(X_train, y_train)\n",
    "\n",
    "with open(f'./model_gridsearch/{loss_type}/hgb', 'wb') as f:\n",
    "    pickle.dump(hgb_search, f)\n",
    "    print('HGB grid search saved')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(f'./model_gridsearch/{loss_type}/hgb', 'rb') as f:\n",
    "    hgb_length_search = pickle.load(f)\n",
    "hgb_length = hgb_length_search.best_estimator_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true,
     "source_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Binning 0.854 GB of training data: 6.564 s\n",
      "Binning 0.095 GB of validation data: 0.082 s\n",
      "Fitting gradient boosted rounds:\n",
      "[1/3000] 1 tree, 31 leaves, max depth = 10, train loss: 0.62780, val loss: 0.62728, in 0.373s\n",
      "[2/3000] 1 tree, 31 leaves, max depth = 10, train loss: 0.57824, val loss: 0.57728, in 0.281s\n",
      "[3/3000] 1 tree, 31 leaves, max depth = 10, train loss: 0.53659, val loss: 0.53529, in 0.286s\n",
      "[4/3000] 1 tree, 31 leaves, max depth = 10, train loss: 0.50147, val loss: 0.49996, in 0.290s\n",
      "[5/3000] 1 tree, 31 leaves, max depth = 9, train loss: 0.47165, val loss: 0.46979, in 0.298s\n",
      "[6/3000] 1 tree, 31 leaves, max depth = 10, train loss: 0.44586, val loss: 0.44388, in 0.343s\n",
      "[7/3000] 1 tree, 31 leaves, max depth = 8, train loss: 0.42336, val loss: 0.42110, in 0.326s\n",
      "[8/3000] 1 tree, 31 leaves, max depth = 8, train loss: 0.40377, val loss: 0.40129, in 0.286s\n",
      "[9/3000] 1 tree, 31 leaves, max depth = 7, train loss: 0.38663, val loss: 0.38394, in 0.313s\n",
      "[10/3000] 1 tree, 31 leaves, max depth = 8, train loss: 0.37078, val loss: 0.36785, in 0.287s\n",
      "[11/3000] 1 tree, 31 leaves, max depth = 9, train loss: 0.35724, val loss: 0.35430, in 0.292s\n",
      "[12/3000] 1 tree, 31 leaves, max depth = 9, train loss: 0.34446, val loss: 0.34141, in 0.304s\n",
      "[13/3000] 1 tree, 31 leaves, max depth = 9, train loss: 0.33376, val loss: 0.33073, in 0.302s\n",
      "[14/3000] 1 tree, 31 leaves, max depth = 10, train loss: 0.32291, val loss: 0.31986, in 0.327s\n",
      "[15/3000] 1 tree, 31 leaves, max depth = 8, train loss: 0.31407, val loss: 0.31093, in 0.305s\n",
      "[16/3000] 1 tree, 31 leaves, max depth = 8, train loss: 0.30528, val loss: 0.30215, in 0.308s\n",
      "[17/3000] 1 tree, 31 leaves, max depth = 11, train loss: 0.29809, val loss: 0.29497, in 0.335s\n",
      "[18/3000] 1 tree, 31 leaves, max depth = 9, train loss: 0.29108, val loss: 0.28787, in 0.306s\n",
      "[19/3000] 1 tree, 31 leaves, max depth = 8, train loss: 0.28491, val loss: 0.28180, in 0.359s\n",
      "[20/3000] 1 tree, 31 leaves, max depth = 8, train loss: 0.27880, val loss: 0.27568, in 0.439s\n",
      "[21/3000] 1 tree, 31 leaves, max depth = 7, train loss: 0.27379, val loss: 0.27085, in 0.347s\n",
      "[22/3000] 1 tree, 31 leaves, max depth = 9, train loss: 0.26915, val loss: 0.26623, in 0.376s\n",
      "[23/3000] 1 tree, 31 leaves, max depth = 11, train loss: 0.26458, val loss: 0.26177, in 0.299s\n",
      "[24/3000] 1 tree, 31 leaves, max depth = 9, train loss: 0.26064, val loss: 0.25791, in 0.321s\n",
      "[25/3000] 1 tree, 31 leaves, max depth = 7, train loss: 0.25685, val loss: 0.25420, in 0.292s\n",
      "[26/3000] 1 tree, 31 leaves, max depth = 9, train loss: 0.25330, val loss: 0.25069, in 0.320s\n",
      "[27/3000] 1 tree, 31 leaves, max depth = 8, train loss: 0.24948, val loss: 0.24698, in 0.340s\n",
      "[28/3000] 1 tree, 31 leaves, max depth = 10, train loss: 0.24612, val loss: 0.24359, in 0.319s\n",
      "[29/3000] 1 tree, 31 leaves, max depth = 7, train loss: 0.24325, val loss: 0.24084, in 0.376s\n",
      "[30/3000] 1 tree, 31 leaves, max depth = 11, train loss: 0.24068, val loss: 0.23838, in 0.289s\n",
      "[31/3000] 1 tree, 31 leaves, max depth = 12, train loss: 0.23846, val loss: 0.23613, in 0.302s\n",
      "[32/3000] 1 tree, 31 leaves, max depth = 10, train loss: 0.23581, val loss: 0.23340, in 0.380s\n",
      "[33/3000] 1 tree, 31 leaves, max depth = 11, train loss: 0.23360, val loss: 0.23131, in 0.391s\n",
      "[34/3000] 1 tree, 31 leaves, max depth = 13, train loss: 0.23152, val loss: 0.22921, in 0.299s\n",
      "[35/3000] 1 tree, 31 leaves, max depth = 8, train loss: 0.22936, val loss: 0.22718, in 0.326s\n",
      "[36/3000] 1 tree, 31 leaves, max depth = 9, train loss: 0.22715, val loss: 0.22505, in 0.308s\n",
      "[37/3000] 1 tree, 31 leaves, max depth = 10, train loss: 0.22545, val loss: 0.22343, in 0.348s\n",
      "[38/3000] 1 tree, 31 leaves, max depth = 9, train loss: 0.22391, val loss: 0.22195, in 0.286s\n",
      "[39/3000] 1 tree, 31 leaves, max depth = 10, train loss: 0.22181, val loss: 0.21999, in 0.294s\n",
      "[40/3000] 1 tree, 31 leaves, max depth = 13, train loss: 0.22019, val loss: 0.21843, in 0.473s\n",
      "[41/3000] 1 tree, 31 leaves, max depth = 11, train loss: 0.21834, val loss: 0.21655, in 0.340s\n",
      "[42/3000] 1 tree, 31 leaves, max depth = 10, train loss: 0.21703, val loss: 0.21517, in 0.377s\n",
      "[43/3000] 1 tree, 31 leaves, max depth = 8, train loss: 0.21580, val loss: 0.21403, in 0.277s\n",
      "[44/3000] 1 tree, 31 leaves, max depth = 11, train loss: 0.21463, val loss: 0.21298, in 0.304s\n",
      "[45/3000] 1 tree, 31 leaves, max depth = 11, train loss: 0.21309, val loss: 0.21150, in 0.298s\n",
      "[46/3000] 1 tree, 31 leaves, max depth = 13, train loss: 0.21205, val loss: 0.21059, in 0.302s\n",
      "[47/3000] 1 tree, 31 leaves, max depth = 9, train loss: 0.21087, val loss: 0.20956, in 0.312s\n",
      "[48/3000] 1 tree, 31 leaves, max depth = 12, train loss: 0.20905, val loss: 0.20780, in 0.266s\n",
      "[49/3000] 1 tree, 31 leaves, max depth = 10, train loss: 0.20788, val loss: 0.20664, in 0.277s\n",
      "[50/3000] 1 tree, 31 leaves, max depth = 9, train loss: 0.20692, val loss: 0.20584, in 0.311s\n",
      "[51/3000] 1 tree, 31 leaves, max depth = 9, train loss: 0.20606, val loss: 0.20511, in 0.282s\n",
      "[52/3000] 1 tree, 31 leaves, max depth = 12, train loss: 0.20476, val loss: 0.20383, in 0.263s\n",
      "[53/3000] 1 tree, 31 leaves, max depth = 9, train loss: 0.20394, val loss: 0.20314, in 0.311s\n",
      "[54/3000] 1 tree, 31 leaves, max depth = 8, train loss: 0.20297, val loss: 0.20220, in 0.312s\n",
      "[55/3000] 1 tree, 31 leaves, max depth = 8, train loss: 0.20212, val loss: 0.20133, in 0.305s\n",
      "[56/3000] 1 tree, 31 leaves, max depth = 8, train loss: 0.20128, val loss: 0.20060, in 0.300s\n",
      "[57/3000] 1 tree, 31 leaves, max depth = 9, train loss: 0.20037, val loss: 0.19986, in 0.276s\n",
      "[58/3000] 1 tree, 31 leaves, max depth = 12, train loss: 0.19960, val loss: 0.19914, in 0.286s\n",
      "[59/3000] 1 tree, 31 leaves, max depth = 14, train loss: 0.19851, val loss: 0.19817, in 0.262s\n",
      "[60/3000] 1 tree, 31 leaves, max depth = 10, train loss: 0.19787, val loss: 0.19755, in 0.457s\n",
      "[61/3000] 1 tree, 31 leaves, max depth = 10, train loss: 0.19696, val loss: 0.19672, in 0.339s\n",
      "[62/3000] 1 tree, 31 leaves, max depth = 8, train loss: 0.19631, val loss: 0.19610, in 0.282s\n",
      "[63/3000] 1 tree, 31 leaves, max depth = 8, train loss: 0.19540, val loss: 0.19520, in 0.323s\n",
      "[64/3000] 1 tree, 31 leaves, max depth = 10, train loss: 0.19479, val loss: 0.19469, in 0.294s\n",
      "[65/3000] 1 tree, 31 leaves, max depth = 9, train loss: 0.19413, val loss: 0.19414, in 0.276s\n",
      "[66/3000] 1 tree, 31 leaves, max depth = 8, train loss: 0.19353, val loss: 0.19361, in 0.309s\n",
      "[67/3000] 1 tree, 31 leaves, max depth = 11, train loss: 0.19271, val loss: 0.19289, in 0.287s\n",
      "[68/3000] 1 tree, 31 leaves, max depth = 10, train loss: 0.19207, val loss: 0.19229, in 0.305s\n",
      "[69/3000] 1 tree, 31 leaves, max depth = 8, train loss: 0.19159, val loss: 0.19185, in 0.287s\n",
      "[70/3000] 1 tree, 31 leaves, max depth = 12, train loss: 0.19081, val loss: 0.19111, in 0.289s\n",
      "[71/3000] 1 tree, 31 leaves, max depth = 8, train loss: 0.19003, val loss: 0.19044, in 0.338s\n",
      "[72/3000] 1 tree, 31 leaves, max depth = 11, train loss: 0.18953, val loss: 0.19005, in 0.318s\n",
      "[73/3000] 1 tree, 31 leaves, max depth = 10, train loss: 0.18884, val loss: 0.18939, in 0.280s\n",
      "[74/3000] 1 tree, 31 leaves, max depth = 11, train loss: 0.18814, val loss: 0.18871, in 0.273s\n",
      "[75/3000] 1 tree, 31 leaves, max depth = 14, train loss: 0.18735, val loss: 0.18793, in 0.217s\n",
      "[76/3000] 1 tree, 31 leaves, max depth = 9, train loss: 0.18684, val loss: 0.18743, in 0.325s\n",
      "[77/3000] 1 tree, 31 leaves, max depth = 13, train loss: 0.18622, val loss: 0.18683, in 0.243s\n",
      "[78/3000] 1 tree, 31 leaves, max depth = 11, train loss: 0.18562, val loss: 0.18629, in 0.311s\n",
      "[79/3000] 1 tree, 31 leaves, max depth = 10, train loss: 0.18514, val loss: 0.18586, in 0.473s\n",
      "[80/3000] 1 tree, 31 leaves, max depth = 7, train loss: 0.18453, val loss: 0.18544, in 0.314s\n",
      "[81/3000] 1 tree, 31 leaves, max depth = 10, train loss: 0.18390, val loss: 0.18490, in 0.387s\n",
      "[82/3000] 1 tree, 31 leaves, max depth = 10, train loss: 0.18340, val loss: 0.18455, in 0.384s\n",
      "[83/3000] 1 tree, 31 leaves, max depth = 11, train loss: 0.18296, val loss: 0.18416, in 0.323s\n",
      "[84/3000] 1 tree, 31 leaves, max depth = 13, train loss: 0.18218, val loss: 0.18345, in 0.346s\n",
      "[85/3000] 1 tree, 31 leaves, max depth = 13, train loss: 0.18163, val loss: 0.18288, in 0.312s\n",
      "[86/3000] 1 tree, 31 leaves, max depth = 12, train loss: 0.18110, val loss: 0.18238, in 0.335s\n",
      "[87/3000] 1 tree, 31 leaves, max depth = 15, train loss: 0.18048, val loss: 0.18177, in 0.326s\n",
      "[88/3000] 1 tree, 31 leaves, max depth = 9, train loss: 0.17994, val loss: 0.18126, in 0.310s\n",
      "[89/3000] 1 tree, 31 leaves, max depth = 10, train loss: 0.17956, val loss: 0.18097, in 0.275s\n",
      "[90/3000] 1 tree, 31 leaves, max depth = 14, train loss: 0.17902, val loss: 0.18046, in 0.308s\n",
      "[91/3000] 1 tree, 31 leaves, max depth = 12, train loss: 0.17847, val loss: 0.17992, in 0.261s\n",
      "[92/3000] 1 tree, 31 leaves, max depth = 9, train loss: 0.17797, val loss: 0.17957, in 0.276s\n",
      "[93/3000] 1 tree, 31 leaves, max depth = 15, train loss: 0.17743, val loss: 0.17906, in 0.261s\n",
      "[94/3000] 1 tree, 31 leaves, max depth = 12, train loss: 0.17689, val loss: 0.17858, in 0.301s\n",
      "[95/3000] 1 tree, 31 leaves, max depth = 10, train loss: 0.17641, val loss: 0.17818, in 0.253s\n",
      "[96/3000] 1 tree, 31 leaves, max depth = 9, train loss: 0.17594, val loss: 0.17769, in 0.277s\n",
      "[97/3000] 1 tree, 31 leaves, max depth = 13, train loss: 0.17526, val loss: 0.17707, in 0.323s\n",
      "[98/3000] 1 tree, 31 leaves, max depth = 12, train loss: 0.17484, val loss: 0.17668, in 0.290s\n",
      "[99/3000] 1 tree, 31 leaves, max depth = 9, train loss: 0.17446, val loss: 0.17639, in 0.460s\n",
      "[100/3000] 1 tree, 31 leaves, max depth = 14, train loss: 0.17407, val loss: 0.17601, in 0.311s\n",
      "[101/3000] 1 tree, 31 leaves, max depth = 9, train loss: 0.17354, val loss: 0.17550, in 0.272s\n",
      "[102/3000] 1 tree, 31 leaves, max depth = 13, train loss: 0.17318, val loss: 0.17520, in 0.294s\n",
      "[103/3000] 1 tree, 31 leaves, max depth = 9, train loss: 0.17278, val loss: 0.17487, in 0.263s\n",
      "[104/3000] 1 tree, 31 leaves, max depth = 11, train loss: 0.17239, val loss: 0.17456, in 0.262s\n",
      "[105/3000] 1 tree, 31 leaves, max depth = 15, train loss: 0.17207, val loss: 0.17429, in 0.253s\n",
      "[106/3000] 1 tree, 31 leaves, max depth = 12, train loss: 0.17167, val loss: 0.17391, in 0.285s\n",
      "[107/3000] 1 tree, 31 leaves, max depth = 12, train loss: 0.17136, val loss: 0.17367, in 0.323s\n",
      "[108/3000] 1 tree, 31 leaves, max depth = 14, train loss: 0.17093, val loss: 0.17329, in 0.299s\n",
      "[109/3000] 1 tree, 31 leaves, max depth = 13, train loss: 0.17058, val loss: 0.17298, in 0.302s\n",
      "[110/3000] 1 tree, 31 leaves, max depth = 10, train loss: 0.17016, val loss: 0.17261, in 0.263s\n",
      "[111/3000] 1 tree, 31 leaves, max depth = 11, train loss: 0.16976, val loss: 0.17228, in 0.260s\n",
      "[112/3000] 1 tree, 31 leaves, max depth = 10, train loss: 0.16942, val loss: 0.17202, in 0.255s\n",
      "[113/3000] 1 tree, 31 leaves, max depth = 8, train loss: 0.16899, val loss: 0.17162, in 0.271s\n",
      "[114/3000] 1 tree, 31 leaves, max depth = 17, train loss: 0.16861, val loss: 0.17129, in 0.223s\n",
      "[115/3000] 1 tree, 31 leaves, max depth = 9, train loss: 0.16837, val loss: 0.17117, in 0.283s\n",
      "[116/3000] 1 tree, 31 leaves, max depth = 12, train loss: 0.16807, val loss: 0.17093, in 0.299s\n",
      "[117/3000] 1 tree, 31 leaves, max depth = 10, train loss: 0.16762, val loss: 0.17064, in 0.298s\n",
      "[118/3000] 1 tree, 31 leaves, max depth = 11, train loss: 0.16733, val loss: 0.17045, in 0.436s\n",
      "[119/3000] 1 tree, 31 leaves, max depth = 13, train loss: 0.16703, val loss: 0.17019, in 0.283s\n",
      "[120/3000] 1 tree, 31 leaves, max depth = 12, train loss: 0.16664, val loss: 0.16981, in 0.312s\n",
      "[121/3000] 1 tree, 31 leaves, max depth = 13, train loss: 0.16630, val loss: 0.16953, in 0.292s\n",
      "[122/3000] 1 tree, 31 leaves, max depth = 10, train loss: 0.16601, val loss: 0.16929, in 0.297s\n",
      "[123/3000] 1 tree, 31 leaves, max depth = 10, train loss: 0.16566, val loss: 0.16905, in 0.286s\n",
      "[124/3000] 1 tree, 31 leaves, max depth = 11, train loss: 0.16537, val loss: 0.16884, in 0.256s\n",
      "[125/3000] 1 tree, 31 leaves, max depth = 12, train loss: 0.16493, val loss: 0.16846, in 0.257s\n",
      "[126/3000] 1 tree, 31 leaves, max depth = 12, train loss: 0.16457, val loss: 0.16810, in 0.283s\n",
      "[127/3000] 1 tree, 31 leaves, max depth = 10, train loss: 0.16427, val loss: 0.16780, in 0.259s\n",
      "[128/3000] 1 tree, 31 leaves, max depth = 13, train loss: 0.16396, val loss: 0.16757, in 0.270s\n",
      "[129/3000] 1 tree, 31 leaves, max depth = 13, train loss: 0.16373, val loss: 0.16740, in 0.266s\n",
      "[130/3000] 1 tree, 31 leaves, max depth = 9, train loss: 0.16344, val loss: 0.16721, in 0.249s\n",
      "[131/3000] 1 tree, 31 leaves, max depth = 12, train loss: 0.16311, val loss: 0.16694, in 0.246s\n",
      "[132/3000] 1 tree, 31 leaves, max depth = 12, train loss: 0.16287, val loss: 0.16675, in 0.262s\n",
      "[133/3000] 1 tree, 31 leaves, max depth = 12, train loss: 0.16262, val loss: 0.16658, in 0.389s\n",
      "[134/3000] 1 tree, 31 leaves, max depth = 16, train loss: 0.16229, val loss: 0.16633, in 0.336s\n",
      "[135/3000] 1 tree, 31 leaves, max depth = 10, train loss: 0.16204, val loss: 0.16612, in 0.351s\n",
      "[136/3000] 1 tree, 31 leaves, max depth = 11, train loss: 0.16177, val loss: 0.16588, in 0.391s\n",
      "[137/3000] 1 tree, 31 leaves, max depth = 11, train loss: 0.16141, val loss: 0.16554, in 0.465s\n",
      "[138/3000] 1 tree, 31 leaves, max depth = 11, train loss: 0.16107, val loss: 0.16528, in 0.371s\n",
      "[139/3000] 1 tree, 31 leaves, max depth = 13, train loss: 0.16081, val loss: 0.16507, in 0.352s\n",
      "[140/3000] 1 tree, 31 leaves, max depth = 11, train loss: 0.16054, val loss: 0.16481, in 0.390s\n",
      "[141/3000] 1 tree, 31 leaves, max depth = 11, train loss: 0.16031, val loss: 0.16466, in 0.355s\n",
      "[142/3000] 1 tree, 31 leaves, max depth = 13, train loss: 0.16005, val loss: 0.16446, in 0.373s\n",
      "[143/3000] 1 tree, 31 leaves, max depth = 13, train loss: 0.15962, val loss: 0.16407, in 0.381s\n",
      "[144/3000] 1 tree, 31 leaves, max depth = 10, train loss: 0.15941, val loss: 0.16393, in 0.375s\n",
      "[145/3000] 1 tree, 31 leaves, max depth = 12, train loss: 0.15912, val loss: 0.16371, in 0.339s\n",
      "[146/3000] 1 tree, 31 leaves, max depth = 9, train loss: 0.15888, val loss: 0.16356, in 0.336s\n",
      "[147/3000] 1 tree, 31 leaves, max depth = 9, train loss: 0.15853, val loss: 0.16324, in 0.358s\n",
      "[148/3000] 1 tree, 31 leaves, max depth = 12, train loss: 0.15821, val loss: 0.16298, in 0.342s\n",
      "[149/3000] 1 tree, 31 leaves, max depth = 10, train loss: 0.15796, val loss: 0.16280, in 0.350s\n",
      "[150/3000] 1 tree, 31 leaves, max depth = 12, train loss: 0.15769, val loss: 0.16261, in 0.332s\n",
      "[151/3000] 1 tree, 31 leaves, max depth = 11, train loss: 0.15743, val loss: 0.16243, in 0.330s\n",
      "[152/3000] 1 tree, 31 leaves, max depth = 14, train loss: 0.15716, val loss: 0.16215, in 0.261s\n",
      "[153/3000] 1 tree, 31 leaves, max depth = 9, train loss: 0.15689, val loss: 0.16191, in 0.372s\n",
      "[154/3000] 1 tree, 31 leaves, max depth = 10, train loss: 0.15665, val loss: 0.16178, in 0.252s\n",
      "[155/3000] 1 tree, 31 leaves, max depth = 9, train loss: 0.15633, val loss: 0.16156, in 0.273s\n",
      "[156/3000] 1 tree, 31 leaves, max depth = 11, train loss: 0.15611, val loss: 0.16139, in 0.253s\n",
      "[157/3000] 1 tree, 31 leaves, max depth = 11, train loss: 0.15590, val loss: 0.16124, in 0.255s\n",
      "[158/3000] 1 tree, 31 leaves, max depth = 13, train loss: 0.15567, val loss: 0.16106, in 0.461s\n",
      "[159/3000] 1 tree, 31 leaves, max depth = 12, train loss: 0.15543, val loss: 0.16086, in 0.258s\n",
      "[160/3000] 1 tree, 31 leaves, max depth = 10, train loss: 0.15520, val loss: 0.16071, in 0.299s\n",
      "[161/3000] 1 tree, 31 leaves, max depth = 11, train loss: 0.15500, val loss: 0.16052, in 0.254s\n",
      "[162/3000] 1 tree, 31 leaves, max depth = 8, train loss: 0.15483, val loss: 0.16042, in 0.233s\n",
      "[163/3000] 1 tree, 31 leaves, max depth = 9, train loss: 0.15456, val loss: 0.16023, in 0.251s\n",
      "[164/3000] 1 tree, 31 leaves, max depth = 12, train loss: 0.15432, val loss: 0.16003, in 0.305s\n",
      "[165/3000] 1 tree, 31 leaves, max depth = 12, train loss: 0.15410, val loss: 0.15989, in 0.391s\n",
      "[166/3000] 1 tree, 31 leaves, max depth = 8, train loss: 0.15389, val loss: 0.15977, in 0.363s\n",
      "[167/3000] 1 tree, 31 leaves, max depth = 11, train loss: 0.15368, val loss: 0.15955, in 0.269s\n",
      "[168/3000] 1 tree, 31 leaves, max depth = 10, train loss: 0.15349, val loss: 0.15945, in 0.287s\n",
      "[169/3000] 1 tree, 31 leaves, max depth = 12, train loss: 0.15330, val loss: 0.15933, in 0.289s\n",
      "[170/3000] 1 tree, 31 leaves, max depth = 13, train loss: 0.15303, val loss: 0.15912, in 0.287s\n",
      "[171/3000] 1 tree, 31 leaves, max depth = 10, train loss: 0.15285, val loss: 0.15905, in 0.318s\n",
      "[172/3000] 1 tree, 31 leaves, max depth = 9, train loss: 0.15261, val loss: 0.15882, in 0.239s\n",
      "[173/3000] 1 tree, 31 leaves, max depth = 14, train loss: 0.15240, val loss: 0.15870, in 0.277s\n",
      "[174/3000] 1 tree, 31 leaves, max depth = 12, train loss: 0.15219, val loss: 0.15855, in 0.252s\n",
      "[175/3000] 1 tree, 31 leaves, max depth = 13, train loss: 0.15200, val loss: 0.15842, in 0.249s\n",
      "[176/3000] 1 tree, 31 leaves, max depth = 8, train loss: 0.15175, val loss: 0.15828, in 0.274s\n",
      "[177/3000] 1 tree, 31 leaves, max depth = 9, train loss: 0.15155, val loss: 0.15816, in 0.257s\n",
      "[178/3000] 1 tree, 31 leaves, max depth = 8, train loss: 0.15136, val loss: 0.15804, in 0.455s\n",
      "[179/3000] 1 tree, 31 leaves, max depth = 12, train loss: 0.15116, val loss: 0.15792, in 0.308s\n",
      "[180/3000] 1 tree, 31 leaves, max depth = 11, train loss: 0.15099, val loss: 0.15783, in 0.283s\n",
      "[181/3000] 1 tree, 31 leaves, max depth = 12, train loss: 0.15081, val loss: 0.15770, in 0.305s\n",
      "[182/3000] 1 tree, 31 leaves, max depth = 12, train loss: 0.15057, val loss: 0.15750, in 0.247s\n",
      "[183/3000] 1 tree, 31 leaves, max depth = 9, train loss: 0.15040, val loss: 0.15740, in 0.355s\n",
      "[184/3000] 1 tree, 31 leaves, max depth = 14, train loss: 0.15017, val loss: 0.15721, in 0.267s\n",
      "[185/3000] 1 tree, 31 leaves, max depth = 11, train loss: 0.15002, val loss: 0.15713, in 0.329s\n",
      "[186/3000] 1 tree, 31 leaves, max depth = 9, train loss: 0.14982, val loss: 0.15697, in 0.286s\n",
      "[187/3000] 1 tree, 31 leaves, max depth = 11, train loss: 0.14959, val loss: 0.15680, in 0.315s\n",
      "[188/3000] 1 tree, 31 leaves, max depth = 12, train loss: 0.14938, val loss: 0.15668, in 0.239s\n",
      "[189/3000] 1 tree, 31 leaves, max depth = 10, train loss: 0.14917, val loss: 0.15654, in 0.286s\n",
      "[190/3000] 1 tree, 31 leaves, max depth = 11, train loss: 0.14902, val loss: 0.15647, in 0.266s\n",
      "[191/3000] 1 tree, 31 leaves, max depth = 13, train loss: 0.14883, val loss: 0.15633, in 0.358s\n",
      "[192/3000] 1 tree, 31 leaves, max depth = 10, train loss: 0.14868, val loss: 0.15626, in 0.306s\n",
      "[193/3000] 1 tree, 31 leaves, max depth = 13, train loss: 0.14844, val loss: 0.15611, in 0.294s\n",
      "[194/3000] 1 tree, 31 leaves, max depth = 14, train loss: 0.14828, val loss: 0.15598, in 0.290s\n",
      "[195/3000] 1 tree, 31 leaves, max depth = 10, train loss: 0.14812, val loss: 0.15592, in 0.267s\n",
      "[196/3000] 1 tree, 31 leaves, max depth = 10, train loss: 0.14796, val loss: 0.15584, in 0.235s\n",
      "[197/3000] 1 tree, 31 leaves, max depth = 14, train loss: 0.14780, val loss: 0.15575, in 0.284s\n",
      "[198/3000] 1 tree, 31 leaves, max depth = 13, train loss: 0.14754, val loss: 0.15556, in 0.411s\n",
      "[199/3000] 1 tree, 31 leaves, max depth = 9, train loss: 0.14737, val loss: 0.15543, in 0.316s\n",
      "[200/3000] 1 tree, 31 leaves, max depth = 10, train loss: 0.14716, val loss: 0.15528, in 0.259s\n",
      "[201/3000] 1 tree, 31 leaves, max depth = 10, train loss: 0.14697, val loss: 0.15513, in 0.247s\n",
      "[202/3000] 1 tree, 31 leaves, max depth = 11, train loss: 0.14679, val loss: 0.15504, in 0.273s\n",
      "[203/3000] 1 tree, 31 leaves, max depth = 12, train loss: 0.14653, val loss: 0.15486, in 0.248s\n",
      "[204/3000] 1 tree, 31 leaves, max depth = 13, train loss: 0.14630, val loss: 0.15467, in 0.278s\n",
      "[205/3000] 1 tree, 31 leaves, max depth = 10, train loss: 0.14613, val loss: 0.15455, in 0.225s\n",
      "[206/3000] 1 tree, 31 leaves, max depth = 12, train loss: 0.14599, val loss: 0.15448, in 0.299s\n",
      "[207/3000] 1 tree, 31 leaves, max depth = 9, train loss: 0.14578, val loss: 0.15433, in 0.235s\n",
      "[208/3000] 1 tree, 31 leaves, max depth = 10, train loss: 0.14563, val loss: 0.15422, in 0.280s\n",
      "[209/3000] 1 tree, 31 leaves, max depth = 10, train loss: 0.14547, val loss: 0.15415, in 0.257s\n",
      "[210/3000] 1 tree, 31 leaves, max depth = 13, train loss: 0.14529, val loss: 0.15403, in 0.234s\n",
      "[211/3000] 1 tree, 31 leaves, max depth = 12, train loss: 0.14512, val loss: 0.15392, in 0.275s\n",
      "[212/3000] 1 tree, 31 leaves, max depth = 10, train loss: 0.14496, val loss: 0.15386, in 0.240s\n",
      "[213/3000] 1 tree, 31 leaves, max depth = 10, train loss: 0.14471, val loss: 0.15367, in 0.294s\n",
      "[214/3000] 1 tree, 31 leaves, max depth = 11, train loss: 0.14455, val loss: 0.15363, in 0.224s\n",
      "[215/3000] 1 tree, 31 leaves, max depth = 10, train loss: 0.14439, val loss: 0.15352, in 0.226s\n",
      "[216/3000] 1 tree, 31 leaves, max depth = 11, train loss: 0.14427, val loss: 0.15347, in 0.209s\n",
      "[217/3000] 1 tree, 31 leaves, max depth = 10, train loss: 0.14412, val loss: 0.15334, in 0.249s\n",
      "[218/3000] 1 tree, 31 leaves, max depth = 9, train loss: 0.14395, val loss: 0.15327, in 0.479s\n",
      "[219/3000] 1 tree, 31 leaves, max depth = 13, train loss: 0.14379, val loss: 0.15318, in 0.247s\n",
      "[220/3000] 1 tree, 31 leaves, max depth = 9, train loss: 0.14367, val loss: 0.15315, in 0.212s\n",
      "[221/3000] 1 tree, 31 leaves, max depth = 10, train loss: 0.14355, val loss: 0.15312, in 0.327s\n",
      "[222/3000] 1 tree, 31 leaves, max depth = 10, train loss: 0.14339, val loss: 0.15302, in 0.275s\n",
      "[223/3000] 1 tree, 31 leaves, max depth = 11, train loss: 0.14319, val loss: 0.15288, in 0.263s\n",
      "[224/3000] 1 tree, 31 leaves, max depth = 8, train loss: 0.14305, val loss: 0.15280, in 0.239s\n",
      "[225/3000] 1 tree, 31 leaves, max depth = 9, train loss: 0.14293, val loss: 0.15275, in 0.241s\n",
      "[226/3000] 1 tree, 31 leaves, max depth = 11, train loss: 0.14264, val loss: 0.15256, in 0.313s\n",
      "[227/3000] 1 tree, 31 leaves, max depth = 14, train loss: 0.14236, val loss: 0.15231, in 0.287s\n",
      "[228/3000] 1 tree, 31 leaves, max depth = 9, train loss: 0.14222, val loss: 0.15221, in 0.269s\n",
      "[229/3000] 1 tree, 31 leaves, max depth = 12, train loss: 0.14204, val loss: 0.15204, in 0.293s\n",
      "[230/3000] 1 tree, 31 leaves, max depth = 9, train loss: 0.14191, val loss: 0.15203, in 0.197s\n",
      "[231/3000] 1 tree, 31 leaves, max depth = 10, train loss: 0.14179, val loss: 0.15201, in 0.266s\n",
      "[232/3000] 1 tree, 31 leaves, max depth = 9, train loss: 0.14167, val loss: 0.15195, in 0.211s\n",
      "[233/3000] 1 tree, 31 leaves, max depth = 12, train loss: 0.14149, val loss: 0.15181, in 0.329s\n",
      "[234/3000] 1 tree, 31 leaves, max depth = 14, train loss: 0.14133, val loss: 0.15178, in 0.282s\n",
      "[235/3000] 1 tree, 31 leaves, max depth = 13, train loss: 0.14120, val loss: 0.15170, in 0.254s\n",
      "[236/3000] 1 tree, 31 leaves, max depth = 8, train loss: 0.14106, val loss: 0.15166, in 0.255s\n",
      "[237/3000] 1 tree, 31 leaves, max depth = 10, train loss: 0.14091, val loss: 0.15157, in 0.286s\n",
      "[238/3000] 1 tree, 31 leaves, max depth = 13, train loss: 0.14072, val loss: 0.15143, in 0.411s\n",
      "[239/3000] 1 tree, 31 leaves, max depth = 15, train loss: 0.14061, val loss: 0.15139, in 0.226s\n",
      "[240/3000] 1 tree, 31 leaves, max depth = 9, train loss: 0.14049, val loss: 0.15133, in 0.246s\n",
      "[241/3000] 1 tree, 31 leaves, max depth = 7, train loss: 0.14036, val loss: 0.15133, in 0.298s\n",
      "[242/3000] 1 tree, 31 leaves, max depth = 10, train loss: 0.14021, val loss: 0.15120, in 0.227s\n",
      "[243/3000] 1 tree, 31 leaves, max depth = 10, train loss: 0.14005, val loss: 0.15115, in 0.344s\n",
      "[244/3000] 1 tree, 31 leaves, max depth = 9, train loss: 0.13994, val loss: 0.15112, in 0.267s\n",
      "[245/3000] 1 tree, 31 leaves, max depth = 11, train loss: 0.13968, val loss: 0.15097, in 0.319s\n",
      "[246/3000] 1 tree, 31 leaves, max depth = 11, train loss: 0.13953, val loss: 0.15085, in 0.267s\n",
      "[247/3000] 1 tree, 31 leaves, max depth = 11, train loss: 0.13937, val loss: 0.15070, in 0.265s\n",
      "[248/3000] 1 tree, 31 leaves, max depth = 9, train loss: 0.13925, val loss: 0.15063, in 0.332s\n",
      "[249/3000] 1 tree, 31 leaves, max depth = 10, train loss: 0.13910, val loss: 0.15055, in 0.250s\n",
      "[250/3000] 1 tree, 31 leaves, max depth = 10, train loss: 0.13897, val loss: 0.15048, in 0.303s\n",
      "[251/3000] 1 tree, 31 leaves, max depth = 10, train loss: 0.13880, val loss: 0.15038, in 0.278s\n",
      "[252/3000] 1 tree, 31 leaves, max depth = 9, train loss: 0.13869, val loss: 0.15036, in 0.230s\n",
      "[253/3000] 1 tree, 31 leaves, max depth = 10, train loss: 0.13859, val loss: 0.15033, in 0.290s\n",
      "[254/3000] 1 tree, 31 leaves, max depth = 8, train loss: 0.13849, val loss: 0.15033, in 0.241s\n",
      "[255/3000] 1 tree, 31 leaves, max depth = 12, train loss: 0.13837, val loss: 0.15031, in 0.244s\n",
      "[256/3000] 1 tree, 31 leaves, max depth = 10, train loss: 0.13820, val loss: 0.15021, in 0.251s\n",
      "[257/3000] 1 tree, 31 leaves, max depth = 10, train loss: 0.13810, val loss: 0.15020, in 0.211s\n",
      "[258/3000] 1 tree, 31 leaves, max depth = 9, train loss: 0.13796, val loss: 0.15014, in 0.453s\n",
      "[259/3000] 1 tree, 31 leaves, max depth = 13, train loss: 0.13774, val loss: 0.14998, in 0.277s\n",
      "[260/3000] 1 tree, 31 leaves, max depth = 11, train loss: 0.13760, val loss: 0.14993, in 0.233s\n",
      "[261/3000] 1 tree, 31 leaves, max depth = 10, train loss: 0.13751, val loss: 0.14991, in 0.196s\n",
      "[262/3000] 1 tree, 31 leaves, max depth = 9, train loss: 0.13738, val loss: 0.14986, in 0.295s\n",
      "[263/3000] 1 tree, 31 leaves, max depth = 10, train loss: 0.13725, val loss: 0.14978, in 0.284s\n",
      "[264/3000] 1 tree, 31 leaves, max depth = 10, train loss: 0.13704, val loss: 0.14963, in 0.239s\n",
      "[265/3000] 1 tree, 31 leaves, max depth = 11, train loss: 0.13690, val loss: 0.14957, in 0.264s\n",
      "[266/3000] 1 tree, 31 leaves, max depth = 15, train loss: 0.13680, val loss: 0.14955, in 0.211s\n",
      "[267/3000] 1 tree, 31 leaves, max depth = 12, train loss: 0.13667, val loss: 0.14953, in 0.231s\n",
      "[268/3000] 1 tree, 31 leaves, max depth = 10, train loss: 0.13653, val loss: 0.14940, in 0.291s\n",
      "[269/3000] 1 tree, 31 leaves, max depth = 8, train loss: 0.13641, val loss: 0.14931, in 0.239s\n",
      "[270/3000] 1 tree, 31 leaves, max depth = 15, train loss: 0.13627, val loss: 0.14925, in 0.208s\n",
      "[271/3000] 1 tree, 31 leaves, max depth = 9, train loss: 0.13618, val loss: 0.14924, in 0.225s\n",
      "[272/3000] 1 tree, 31 leaves, max depth = 12, train loss: 0.13606, val loss: 0.14918, in 0.278s\n",
      "[273/3000] 1 tree, 31 leaves, max depth = 9, train loss: 0.13594, val loss: 0.14917, in 0.220s\n",
      "[274/3000] 1 tree, 31 leaves, max depth = 10, train loss: 0.13579, val loss: 0.14912, in 0.221s\n",
      "[275/3000] 1 tree, 31 leaves, max depth = 13, train loss: 0.13565, val loss: 0.14904, in 0.311s\n",
      "[276/3000] 1 tree, 31 leaves, max depth = 9, train loss: 0.13552, val loss: 0.14899, in 0.251s\n",
      "[277/3000] 1 tree, 31 leaves, max depth = 13, train loss: 0.13541, val loss: 0.14895, in 0.243s\n",
      "[278/3000] 1 tree, 31 leaves, max depth = 9, train loss: 0.13522, val loss: 0.14880, in 0.408s\n",
      "[279/3000] 1 tree, 31 leaves, max depth = 15, train loss: 0.13503, val loss: 0.14865, in 0.342s\n",
      "[280/3000] 1 tree, 31 leaves, max depth = 10, train loss: 0.13490, val loss: 0.14865, in 0.275s\n",
      "[281/3000] 1 tree, 31 leaves, max depth = 10, train loss: 0.13481, val loss: 0.14861, in 0.186s\n",
      "[282/3000] 1 tree, 31 leaves, max depth = 14, train loss: 0.13471, val loss: 0.14859, in 0.234s\n",
      "[283/3000] 1 tree, 31 leaves, max depth = 9, train loss: 0.13450, val loss: 0.14848, in 0.294s\n",
      "[284/3000] 1 tree, 31 leaves, max depth = 9, train loss: 0.13438, val loss: 0.14841, in 0.255s\n",
      "[285/3000] 1 tree, 31 leaves, max depth = 12, train loss: 0.13430, val loss: 0.14841, in 0.194s\n",
      "[286/3000] 1 tree, 31 leaves, max depth = 10, train loss: 0.13419, val loss: 0.14836, in 0.220s\n",
      "[287/3000] 1 tree, 31 leaves, max depth = 12, train loss: 0.13411, val loss: 0.14833, in 0.166s\n",
      "[288/3000] 1 tree, 31 leaves, max depth = 10, train loss: 0.13401, val loss: 0.14830, in 0.184s\n",
      "[289/3000] 1 tree, 31 leaves, max depth = 10, train loss: 0.13391, val loss: 0.14826, in 0.205s\n",
      "[290/3000] 1 tree, 31 leaves, max depth = 14, train loss: 0.13370, val loss: 0.14807, in 0.219s\n",
      "[291/3000] 1 tree, 31 leaves, max depth = 11, train loss: 0.13356, val loss: 0.14802, in 0.256s\n",
      "[292/3000] 1 tree, 31 leaves, max depth = 7, train loss: 0.13347, val loss: 0.14801, in 0.219s\n",
      "[293/3000] 1 tree, 31 leaves, max depth = 11, train loss: 0.13336, val loss: 0.14793, in 0.207s\n",
      "[294/3000] 1 tree, 31 leaves, max depth = 11, train loss: 0.13322, val loss: 0.14783, in 0.237s\n",
      "[295/3000] 1 tree, 31 leaves, max depth = 9, train loss: 0.13309, val loss: 0.14775, in 0.189s\n",
      "[296/3000] 1 tree, 31 leaves, max depth = 13, train loss: 0.13299, val loss: 0.14773, in 0.194s\n",
      "[297/3000] 1 tree, 31 leaves, max depth = 10, train loss: 0.13284, val loss: 0.14765, in 0.252s\n",
      "[298/3000] 1 tree, 31 leaves, max depth = 10, train loss: 0.13270, val loss: 0.14753, in 0.229s\n",
      "[299/3000] 1 tree, 31 leaves, max depth = 12, train loss: 0.13261, val loss: 0.14750, in 0.303s\n",
      "[300/3000] 1 tree, 31 leaves, max depth = 9, train loss: 0.13248, val loss: 0.14745, in 0.264s\n",
      "[301/3000] 1 tree, 31 leaves, max depth = 10, train loss: 0.13241, val loss: 0.14742, in 0.237s\n",
      "[302/3000] 1 tree, 31 leaves, max depth = 12, train loss: 0.13232, val loss: 0.14744, in 0.264s\n",
      "[303/3000] 1 tree, 31 leaves, max depth = 11, train loss: 0.13222, val loss: 0.14740, in 0.261s\n",
      "[304/3000] 1 tree, 31 leaves, max depth = 10, train loss: 0.13208, val loss: 0.14733, in 0.251s\n",
      "[305/3000] 1 tree, 31 leaves, max depth = 11, train loss: 0.13190, val loss: 0.14720, in 0.275s\n",
      "[306/3000] 1 tree, 31 leaves, max depth = 14, train loss: 0.13179, val loss: 0.14715, in 0.240s\n",
      "[307/3000] 1 tree, 31 leaves, max depth = 14, train loss: 0.13162, val loss: 0.14707, in 0.273s\n",
      "[308/3000] 1 tree, 31 leaves, max depth = 10, train loss: 0.13150, val loss: 0.14704, in 0.290s\n",
      "[309/3000] 1 tree, 31 leaves, max depth = 7, train loss: 0.13137, val loss: 0.14701, in 0.261s\n",
      "[310/3000] 1 tree, 31 leaves, max depth = 10, train loss: 0.13128, val loss: 0.14700, in 0.207s\n",
      "[311/3000] 1 tree, 31 leaves, max depth = 8, train loss: 0.13118, val loss: 0.14696, in 0.226s\n",
      "[312/3000] 1 tree, 31 leaves, max depth = 13, train loss: 0.13105, val loss: 0.14692, in 0.318s\n",
      "[313/3000] 1 tree, 31 leaves, max depth = 9, train loss: 0.13095, val loss: 0.14689, in 0.229s\n",
      "[314/3000] 1 tree, 31 leaves, max depth = 14, train loss: 0.13075, val loss: 0.14676, in 0.302s\n",
      "[315/3000] 1 tree, 31 leaves, max depth = 9, train loss: 0.13064, val loss: 0.14670, in 0.256s\n",
      "[316/3000] 1 tree, 31 leaves, max depth = 8, train loss: 0.13050, val loss: 0.14666, in 0.201s\n",
      "[317/3000] 1 tree, 31 leaves, max depth = 11, train loss: 0.13039, val loss: 0.14656, in 0.230s\n",
      "[318/3000] 1 tree, 31 leaves, max depth = 10, train loss: 0.13030, val loss: 0.14655, in 0.210s\n",
      "[319/3000] 1 tree, 31 leaves, max depth = 12, train loss: 0.13020, val loss: 0.14654, in 0.187s\n",
      "[320/3000] 1 tree, 31 leaves, max depth = 14, train loss: 0.13009, val loss: 0.14652, in 0.416s\n",
      "[321/3000] 1 tree, 31 leaves, max depth = 10, train loss: 0.12993, val loss: 0.14644, in 0.289s\n",
      "[322/3000] 1 tree, 31 leaves, max depth = 10, train loss: 0.12976, val loss: 0.14632, in 0.298s\n",
      "[323/3000] 1 tree, 31 leaves, max depth = 11, train loss: 0.12956, val loss: 0.14612, in 0.272s\n",
      "[324/3000] 1 tree, 31 leaves, max depth = 9, train loss: 0.12949, val loss: 0.14614, in 0.189s\n",
      "[325/3000] 1 tree, 31 leaves, max depth = 12, train loss: 0.12937, val loss: 0.14606, in 0.252s\n",
      "[326/3000] 1 tree, 31 leaves, max depth = 9, train loss: 0.12927, val loss: 0.14606, in 0.211s\n",
      "[327/3000] 1 tree, 31 leaves, max depth = 12, train loss: 0.12917, val loss: 0.14601, in 0.269s\n",
      "[328/3000] 1 tree, 31 leaves, max depth = 10, train loss: 0.12908, val loss: 0.14599, in 0.184s\n",
      "[329/3000] 1 tree, 31 leaves, max depth = 11, train loss: 0.12898, val loss: 0.14599, in 0.233s\n",
      "[330/3000] 1 tree, 31 leaves, max depth = 10, train loss: 0.12885, val loss: 0.14594, in 0.233s\n",
      "[331/3000] 1 tree, 31 leaves, max depth = 9, train loss: 0.12878, val loss: 0.14592, in 0.196s\n",
      "[332/3000] 1 tree, 31 leaves, max depth = 12, train loss: 0.12865, val loss: 0.14585, in 0.245s\n",
      "[333/3000] 1 tree, 31 leaves, max depth = 10, train loss: 0.12854, val loss: 0.14582, in 0.337s\n",
      "[334/3000] 1 tree, 31 leaves, max depth = 12, train loss: 0.12838, val loss: 0.14575, in 0.283s\n",
      "[335/3000] 1 tree, 31 leaves, max depth = 13, train loss: 0.12828, val loss: 0.14570, in 0.208s\n",
      "[336/3000] 1 tree, 31 leaves, max depth = 8, train loss: 0.12820, val loss: 0.14572, in 0.206s\n",
      "[337/3000] 1 tree, 31 leaves, max depth = 9, train loss: 0.12806, val loss: 0.14563, in 0.281s\n",
      "[338/3000] 1 tree, 31 leaves, max depth = 8, train loss: 0.12797, val loss: 0.14558, in 0.210s\n",
      "[339/3000] 1 tree, 31 leaves, max depth = 11, train loss: 0.12789, val loss: 0.14557, in 0.196s\n",
      "[340/3000] 1 tree, 31 leaves, max depth = 14, train loss: 0.12781, val loss: 0.14556, in 0.211s\n",
      "[341/3000] 1 tree, 31 leaves, max depth = 8, train loss: 0.12767, val loss: 0.14549, in 0.370s\n",
      "[342/3000] 1 tree, 31 leaves, max depth = 11, train loss: 0.12755, val loss: 0.14543, in 0.224s\n",
      "[343/3000] 1 tree, 31 leaves, max depth = 10, train loss: 0.12744, val loss: 0.14538, in 0.282s\n",
      "[344/3000] 1 tree, 31 leaves, max depth = 14, train loss: 0.12731, val loss: 0.14535, in 0.280s\n",
      "[345/3000] 1 tree, 31 leaves, max depth = 11, train loss: 0.12720, val loss: 0.14534, in 0.238s\n",
      "[346/3000] 1 tree, 31 leaves, max depth = 10, train loss: 0.12707, val loss: 0.14524, in 0.264s\n",
      "[347/3000] 1 tree, 31 leaves, max depth = 14, train loss: 0.12695, val loss: 0.14521, in 0.271s\n",
      "[348/3000] 1 tree, 31 leaves, max depth = 12, train loss: 0.12686, val loss: 0.14519, in 0.187s\n",
      "[349/3000] 1 tree, 31 leaves, max depth = 9, train loss: 0.12674, val loss: 0.14513, in 0.257s\n",
      "[350/3000] 1 tree, 31 leaves, max depth = 14, train loss: 0.12662, val loss: 0.14509, in 0.241s\n",
      "[351/3000] 1 tree, 31 leaves, max depth = 13, train loss: 0.12654, val loss: 0.14506, in 0.213s\n",
      "[352/3000] 1 tree, 31 leaves, max depth = 10, train loss: 0.12644, val loss: 0.14504, in 0.214s\n",
      "[353/3000] 1 tree, 31 leaves, max depth = 15, train loss: 0.12628, val loss: 0.14494, in 0.251s\n",
      "[354/3000] 1 tree, 31 leaves, max depth = 11, train loss: 0.12617, val loss: 0.14489, in 0.230s\n",
      "[355/3000] 1 tree, 31 leaves, max depth = 10, train loss: 0.12609, val loss: 0.14490, in 0.191s\n",
      "[356/3000] 1 tree, 31 leaves, max depth = 13, train loss: 0.12601, val loss: 0.14487, in 0.206s\n",
      "[357/3000] 1 tree, 31 leaves, max depth = 10, train loss: 0.12590, val loss: 0.14484, in 0.264s\n",
      "[358/3000] 1 tree, 31 leaves, max depth = 11, train loss: 0.12578, val loss: 0.14475, in 0.260s\n",
      "[359/3000] 1 tree, 31 leaves, max depth = 13, train loss: 0.12563, val loss: 0.14466, in 0.276s\n",
      "[360/3000] 1 tree, 31 leaves, max depth = 10, train loss: 0.12552, val loss: 0.14461, in 0.267s\n",
      "[361/3000] 1 tree, 31 leaves, max depth = 9, train loss: 0.12542, val loss: 0.14458, in 0.456s\n",
      "[362/3000] 1 tree, 31 leaves, max depth = 10, train loss: 0.12534, val loss: 0.14456, in 0.247s\n",
      "[363/3000] 1 tree, 31 leaves, max depth = 11, train loss: 0.12522, val loss: 0.14448, in 0.324s\n",
      "[364/3000] 1 tree, 31 leaves, max depth = 8, train loss: 0.12515, val loss: 0.14443, in 0.230s\n",
      "[365/3000] 1 tree, 31 leaves, max depth = 10, train loss: 0.12502, val loss: 0.14438, in 0.315s\n",
      "[366/3000] 1 tree, 31 leaves, max depth = 13, train loss: 0.12492, val loss: 0.14433, in 0.235s\n",
      "[367/3000] 1 tree, 31 leaves, max depth = 12, train loss: 0.12486, val loss: 0.14433, in 0.198s\n",
      "[368/3000] 1 tree, 31 leaves, max depth = 10, train loss: 0.12474, val loss: 0.14428, in 0.243s\n",
      "[369/3000] 1 tree, 31 leaves, max depth = 13, train loss: 0.12464, val loss: 0.14425, in 0.274s\n",
      "[370/3000] 1 tree, 31 leaves, max depth = 8, train loss: 0.12455, val loss: 0.14421, in 0.206s\n",
      "[371/3000] 1 tree, 31 leaves, max depth = 13, train loss: 0.12440, val loss: 0.14413, in 0.283s\n",
      "[372/3000] 1 tree, 31 leaves, max depth = 13, train loss: 0.12429, val loss: 0.14409, in 0.350s\n",
      "[373/3000] 1 tree, 31 leaves, max depth = 12, train loss: 0.12423, val loss: 0.14408, in 0.193s\n",
      "[374/3000] 1 tree, 31 leaves, max depth = 12, train loss: 0.12410, val loss: 0.14400, in 0.311s\n",
      "[375/3000] 1 tree, 31 leaves, max depth = 11, train loss: 0.12398, val loss: 0.14393, in 0.273s\n",
      "[376/3000] 1 tree, 31 leaves, max depth = 14, train loss: 0.12390, val loss: 0.14387, in 0.236s\n",
      "[377/3000] 1 tree, 31 leaves, max depth = 11, train loss: 0.12376, val loss: 0.14381, in 0.272s\n",
      "[378/3000] 1 tree, 31 leaves, max depth = 7, train loss: 0.12368, val loss: 0.14385, in 0.180s\n",
      "[379/3000] 1 tree, 31 leaves, max depth = 11, train loss: 0.12355, val loss: 0.14379, in 0.272s\n",
      "[380/3000] 1 tree, 31 leaves, max depth = 10, train loss: 0.12343, val loss: 0.14373, in 0.272s\n",
      "[381/3000] 1 tree, 31 leaves, max depth = 11, train loss: 0.12332, val loss: 0.14369, in 0.263s\n",
      "[382/3000] 1 tree, 31 leaves, max depth = 10, train loss: 0.12318, val loss: 0.14362, in 0.399s\n",
      "[383/3000] 1 tree, 31 leaves, max depth = 9, train loss: 0.12302, val loss: 0.14352, in 0.370s\n",
      "[384/3000] 1 tree, 31 leaves, max depth = 9, train loss: 0.12294, val loss: 0.14352, in 0.227s\n",
      "[385/3000] 1 tree, 31 leaves, max depth = 10, train loss: 0.12286, val loss: 0.14352, in 0.215s\n",
      "[386/3000] 1 tree, 31 leaves, max depth = 15, train loss: 0.12281, val loss: 0.14352, in 0.202s\n",
      "[387/3000] 1 tree, 31 leaves, max depth = 8, train loss: 0.12274, val loss: 0.14353, in 0.205s\n",
      "[388/3000] 1 tree, 31 leaves, max depth = 9, train loss: 0.12265, val loss: 0.14350, in 0.208s\n",
      "[389/3000] 1 tree, 31 leaves, max depth = 12, train loss: 0.12248, val loss: 0.14340, in 0.276s\n",
      "[390/3000] 1 tree, 31 leaves, max depth = 14, train loss: 0.12240, val loss: 0.14338, in 0.201s\n",
      "[391/3000] 1 tree, 31 leaves, max depth = 14, train loss: 0.12231, val loss: 0.14338, in 0.257s\n",
      "[392/3000] 1 tree, 31 leaves, max depth = 9, train loss: 0.12221, val loss: 0.14334, in 0.244s\n",
      "[393/3000] 1 tree, 31 leaves, max depth = 9, train loss: 0.12215, val loss: 0.14333, in 0.191s\n",
      "[394/3000] 1 tree, 31 leaves, max depth = 9, train loss: 0.12206, val loss: 0.14333, in 0.207s\n",
      "[395/3000] 1 tree, 31 leaves, max depth = 13, train loss: 0.12200, val loss: 0.14333, in 0.174s\n",
      "[396/3000] 1 tree, 31 leaves, max depth = 10, train loss: 0.12188, val loss: 0.14325, in 0.280s\n",
      "[397/3000] 1 tree, 31 leaves, max depth = 13, train loss: 0.12172, val loss: 0.14311, in 0.230s\n",
      "[398/3000] 1 tree, 31 leaves, max depth = 9, train loss: 0.12161, val loss: 0.14305, in 0.200s\n",
      "[399/3000] 1 tree, 31 leaves, max depth = 10, train loss: 0.12150, val loss: 0.14301, in 0.246s\n",
      "[400/3000] 1 tree, 31 leaves, max depth = 10, train loss: 0.12143, val loss: 0.14303, in 0.206s\n",
      "[401/3000] 1 tree, 31 leaves, max depth = 8, train loss: 0.12134, val loss: 0.14300, in 0.242s\n",
      "[402/3000] 1 tree, 31 leaves, max depth = 11, train loss: 0.12127, val loss: 0.14300, in 0.230s\n",
      "[403/3000] 1 tree, 31 leaves, max depth = 11, train loss: 0.12119, val loss: 0.14299, in 0.330s\n",
      "[404/3000] 1 tree, 31 leaves, max depth = 13, train loss: 0.12112, val loss: 0.14301, in 0.225s\n",
      "[405/3000] 1 tree, 31 leaves, max depth = 17, train loss: 0.12100, val loss: 0.14294, in 0.351s\n",
      "[406/3000] 1 tree, 31 leaves, max depth = 10, train loss: 0.12093, val loss: 0.14293, in 0.218s\n",
      "[407/3000] 1 tree, 31 leaves, max depth = 9, train loss: 0.12086, val loss: 0.14294, in 0.194s\n",
      "[408/3000] 1 tree, 31 leaves, max depth = 10, train loss: 0.12079, val loss: 0.14294, in 0.239s\n",
      "[409/3000] 1 tree, 31 leaves, max depth = 12, train loss: 0.12070, val loss: 0.14294, in 0.199s\n",
      "[410/3000] 1 tree, 31 leaves, max depth = 11, train loss: 0.12058, val loss: 0.14289, in 0.261s\n",
      "[411/3000] 1 tree, 31 leaves, max depth = 10, train loss: 0.12051, val loss: 0.14287, in 0.194s\n",
      "[412/3000] 1 tree, 31 leaves, max depth = 13, train loss: 0.12034, val loss: 0.14267, in 0.314s\n",
      "[413/3000] 1 tree, 31 leaves, max depth = 17, train loss: 0.12027, val loss: 0.14266, in 0.191s\n",
      "[414/3000] 1 tree, 31 leaves, max depth = 8, train loss: 0.12020, val loss: 0.14263, in 0.191s\n",
      "[415/3000] 1 tree, 31 leaves, max depth = 11, train loss: 0.12009, val loss: 0.14260, in 0.246s\n",
      "[416/3000] 1 tree, 31 leaves, max depth = 12, train loss: 0.12004, val loss: 0.14261, in 0.189s\n",
      "[417/3000] 1 tree, 31 leaves, max depth = 12, train loss: 0.11994, val loss: 0.14261, in 0.261s\n",
      "[418/3000] 1 tree, 31 leaves, max depth = 11, train loss: 0.11988, val loss: 0.14259, in 0.190s\n",
      "[419/3000] 1 tree, 31 leaves, max depth = 13, train loss: 0.11976, val loss: 0.14251, in 0.223s\n",
      "[420/3000] 1 tree, 31 leaves, max depth = 10, train loss: 0.11966, val loss: 0.14240, in 0.201s\n",
      "[421/3000] 1 tree, 31 leaves, max depth = 9, train loss: 0.11957, val loss: 0.14237, in 0.228s\n",
      "[422/3000] 1 tree, 31 leaves, max depth = 10, train loss: 0.11951, val loss: 0.14236, in 0.186s\n",
      "[423/3000] 1 tree, 31 leaves, max depth = 12, train loss: 0.11943, val loss: 0.14237, in 0.187s\n",
      "[424/3000] 1 tree, 31 leaves, max depth = 12, train loss: 0.11929, val loss: 0.14230, in 0.391s\n",
      "[425/3000] 1 tree, 31 leaves, max depth = 10, train loss: 0.11915, val loss: 0.14224, in 0.300s\n",
      "[426/3000] 1 tree, 31 leaves, max depth = 11, train loss: 0.11907, val loss: 0.14227, in 0.222s\n",
      "[427/3000] 1 tree, 31 leaves, max depth = 14, train loss: 0.11899, val loss: 0.14228, in 0.196s\n",
      "[428/3000] 1 tree, 31 leaves, max depth = 11, train loss: 0.11890, val loss: 0.14226, in 0.201s\n",
      "[429/3000] 1 tree, 31 leaves, max depth = 10, train loss: 0.11884, val loss: 0.14227, in 0.198s\n",
      "[430/3000] 1 tree, 31 leaves, max depth = 10, train loss: 0.11873, val loss: 0.14223, in 0.237s\n",
      "[431/3000] 1 tree, 31 leaves, max depth = 10, train loss: 0.11866, val loss: 0.14222, in 0.218s\n",
      "[432/3000] 1 tree, 31 leaves, max depth = 11, train loss: 0.11856, val loss: 0.14216, in 0.211s\n",
      "[433/3000] 1 tree, 31 leaves, max depth = 13, train loss: 0.11849, val loss: 0.14217, in 0.215s\n",
      "[434/3000] 1 tree, 31 leaves, max depth = 9, train loss: 0.11842, val loss: 0.14218, in 0.201s\n",
      "[435/3000] 1 tree, 31 leaves, max depth = 12, train loss: 0.11832, val loss: 0.14214, in 0.197s\n",
      "[436/3000] 1 tree, 31 leaves, max depth = 13, train loss: 0.11824, val loss: 0.14211, in 0.208s\n",
      "[437/3000] 1 tree, 31 leaves, max depth = 11, train loss: 0.11815, val loss: 0.14204, in 0.220s\n",
      "[438/3000] 1 tree, 31 leaves, max depth = 11, train loss: 0.11807, val loss: 0.14204, in 0.226s\n",
      "[439/3000] 1 tree, 31 leaves, max depth = 11, train loss: 0.11793, val loss: 0.14195, in 0.301s\n",
      "[440/3000] 1 tree, 31 leaves, max depth = 14, train loss: 0.11781, val loss: 0.14189, in 0.310s\n",
      "[441/3000] 1 tree, 31 leaves, max depth = 13, train loss: 0.11771, val loss: 0.14186, in 0.281s\n",
      "[442/3000] 1 tree, 31 leaves, max depth = 12, train loss: 0.11760, val loss: 0.14180, in 0.261s\n",
      "[443/3000] 1 tree, 31 leaves, max depth = 11, train loss: 0.11749, val loss: 0.14175, in 0.310s\n",
      "[444/3000] 1 tree, 31 leaves, max depth = 10, train loss: 0.11741, val loss: 0.14174, in 0.195s\n",
      "[445/3000] 1 tree, 31 leaves, max depth = 9, train loss: 0.11731, val loss: 0.14172, in 0.368s\n",
      "[446/3000] 1 tree, 31 leaves, max depth = 14, train loss: 0.11720, val loss: 0.14168, in 0.656s\n",
      "[447/3000] 1 tree, 31 leaves, max depth = 10, train loss: 0.11707, val loss: 0.14162, in 0.291s\n",
      "[448/3000] 1 tree, 31 leaves, max depth = 14, train loss: 0.11694, val loss: 0.14155, in 0.286s\n",
      "[449/3000] 1 tree, 31 leaves, max depth = 14, train loss: 0.11686, val loss: 0.14151, in 0.214s\n",
      "[450/3000] 1 tree, 31 leaves, max depth = 12, train loss: 0.11677, val loss: 0.14151, in 0.248s\n",
      "[451/3000] 1 tree, 31 leaves, max depth = 10, train loss: 0.11671, val loss: 0.14150, in 0.228s\n",
      "[452/3000] 1 tree, 31 leaves, max depth = 13, train loss: 0.11661, val loss: 0.14146, in 0.199s\n",
      "[453/3000] 1 tree, 31 leaves, max depth = 12, train loss: 0.11655, val loss: 0.14146, in 0.226s\n",
      "[454/3000] 1 tree, 31 leaves, max depth = 8, train loss: 0.11648, val loss: 0.14147, in 0.199s\n",
      "[455/3000] 1 tree, 31 leaves, max depth = 10, train loss: 0.11636, val loss: 0.14139, in 0.245s\n",
      "[456/3000] 1 tree, 31 leaves, max depth = 10, train loss: 0.11629, val loss: 0.14139, in 0.184s\n",
      "[457/3000] 1 tree, 31 leaves, max depth = 10, train loss: 0.11622, val loss: 0.14141, in 0.171s\n",
      "[458/3000] 1 tree, 31 leaves, max depth = 13, train loss: 0.11617, val loss: 0.14141, in 0.182s\n",
      "[459/3000] 1 tree, 31 leaves, max depth = 13, train loss: 0.11608, val loss: 0.14137, in 0.205s\n",
      "[460/3000] 1 tree, 31 leaves, max depth = 13, train loss: 0.11599, val loss: 0.14135, in 0.273s\n",
      "[461/3000] 1 tree, 31 leaves, max depth = 10, train loss: 0.11588, val loss: 0.14131, in 0.230s\n",
      "[462/3000] 1 tree, 31 leaves, max depth = 11, train loss: 0.11578, val loss: 0.14129, in 0.225s\n",
      "[463/3000] 1 tree, 31 leaves, max depth = 9, train loss: 0.11571, val loss: 0.14128, in 0.182s\n",
      "[464/3000] 1 tree, 31 leaves, max depth = 13, train loss: 0.11565, val loss: 0.14128, in 0.224s\n",
      "[465/3000] 1 tree, 31 leaves, max depth = 11, train loss: 0.11558, val loss: 0.14128, in 0.192s\n",
      "[466/3000] 1 tree, 31 leaves, max depth = 10, train loss: 0.11549, val loss: 0.14124, in 0.216s\n",
      "[467/3000] 1 tree, 31 leaves, max depth = 9, train loss: 0.11541, val loss: 0.14123, in 0.335s\n",
      "[468/3000] 1 tree, 31 leaves, max depth = 11, train loss: 0.11532, val loss: 0.14121, in 0.299s\n",
      "[469/3000] 1 tree, 31 leaves, max depth = 12, train loss: 0.11522, val loss: 0.14114, in 0.298s\n",
      "[470/3000] 1 tree, 31 leaves, max depth = 11, train loss: 0.11512, val loss: 0.14109, in 0.276s\n",
      "[471/3000] 1 tree, 31 leaves, max depth = 12, train loss: 0.11505, val loss: 0.14109, in 0.186s\n",
      "[472/3000] 1 tree, 31 leaves, max depth = 12, train loss: 0.11498, val loss: 0.14110, in 0.196s\n",
      "[473/3000] 1 tree, 31 leaves, max depth = 9, train loss: 0.11491, val loss: 0.14110, in 0.203s\n",
      "[474/3000] 1 tree, 31 leaves, max depth = 9, train loss: 0.11481, val loss: 0.14104, in 0.246s\n",
      "[475/3000] 1 tree, 31 leaves, max depth = 12, train loss: 0.11473, val loss: 0.14099, in 0.223s\n",
      "[476/3000] 1 tree, 31 leaves, max depth = 11, train loss: 0.11466, val loss: 0.14096, in 0.200s\n",
      "[477/3000] 1 tree, 31 leaves, max depth = 15, train loss: 0.11458, val loss: 0.14093, in 0.225s\n",
      "[478/3000] 1 tree, 31 leaves, max depth = 8, train loss: 0.11449, val loss: 0.14088, in 0.223s\n",
      "[479/3000] 1 tree, 31 leaves, max depth = 10, train loss: 0.11436, val loss: 0.14080, in 0.259s\n",
      "[480/3000] 1 tree, 31 leaves, max depth = 14, train loss: 0.11427, val loss: 0.14075, in 0.207s\n",
      "[481/3000] 1 tree, 31 leaves, max depth = 10, train loss: 0.11421, val loss: 0.14075, in 0.206s\n",
      "[482/3000] 1 tree, 31 leaves, max depth = 13, train loss: 0.11411, val loss: 0.14069, in 0.208s\n",
      "[483/3000] 1 tree, 31 leaves, max depth = 9, train loss: 0.11406, val loss: 0.14068, in 0.207s\n",
      "[484/3000] 1 tree, 31 leaves, max depth = 11, train loss: 0.11398, val loss: 0.14066, in 0.202s\n",
      "[485/3000] 1 tree, 31 leaves, max depth = 9, train loss: 0.11390, val loss: 0.14064, in 0.202s\n",
      "[486/3000] 1 tree, 31 leaves, max depth = 13, train loss: 0.11380, val loss: 0.14057, in 0.264s\n",
      "[487/3000] 1 tree, 31 leaves, max depth = 10, train loss: 0.11372, val loss: 0.14053, in 0.198s\n",
      "[488/3000] 1 tree, 31 leaves, max depth = 12, train loss: 0.11366, val loss: 0.14050, in 0.339s\n",
      "[489/3000] 1 tree, 31 leaves, max depth = 10, train loss: 0.11357, val loss: 0.14048, in 0.259s\n",
      "[490/3000] 1 tree, 31 leaves, max depth = 11, train loss: 0.11351, val loss: 0.14047, in 0.212s\n",
      "[491/3000] 1 tree, 31 leaves, max depth = 9, train loss: 0.11342, val loss: 0.14046, in 0.272s\n",
      "[492/3000] 1 tree, 31 leaves, max depth = 9, train loss: 0.11334, val loss: 0.14043, in 0.256s\n",
      "[493/3000] 1 tree, 31 leaves, max depth = 11, train loss: 0.11324, val loss: 0.14037, in 0.256s\n",
      "[494/3000] 1 tree, 31 leaves, max depth = 11, train loss: 0.11319, val loss: 0.14038, in 0.183s\n",
      "[495/3000] 1 tree, 31 leaves, max depth = 12, train loss: 0.11312, val loss: 0.14038, in 0.216s\n",
      "[496/3000] 1 tree, 31 leaves, max depth = 9, train loss: 0.11307, val loss: 0.14033, in 0.188s\n",
      "[497/3000] 1 tree, 31 leaves, max depth = 16, train loss: 0.11301, val loss: 0.14035, in 0.218s\n",
      "[498/3000] 1 tree, 31 leaves, max depth = 10, train loss: 0.11288, val loss: 0.14024, in 0.251s\n",
      "[499/3000] 1 tree, 31 leaves, max depth = 14, train loss: 0.11279, val loss: 0.14024, in 0.260s\n",
      "[500/3000] 1 tree, 31 leaves, max depth = 12, train loss: 0.11272, val loss: 0.14022, in 0.203s\n",
      "[501/3000] 1 tree, 31 leaves, max depth = 9, train loss: 0.11263, val loss: 0.14019, in 0.251s\n",
      "[502/3000] 1 tree, 31 leaves, max depth = 15, train loss: 0.11250, val loss: 0.14011, in 0.253s\n",
      "[503/3000] 1 tree, 31 leaves, max depth = 12, train loss: 0.11240, val loss: 0.14010, in 0.232s\n",
      "[504/3000] 1 tree, 31 leaves, max depth = 11, train loss: 0.11233, val loss: 0.14009, in 0.186s\n",
      "[505/3000] 1 tree, 31 leaves, max depth = 12, train loss: 0.11221, val loss: 0.14005, in 0.307s\n",
      "[506/3000] 1 tree, 31 leaves, max depth = 8, train loss: 0.11212, val loss: 0.14003, in 0.230s\n",
      "[507/3000] 1 tree, 31 leaves, max depth = 11, train loss: 0.11206, val loss: 0.14003, in 0.205s\n",
      "[508/3000] 1 tree, 31 leaves, max depth = 12, train loss: 0.11198, val loss: 0.14003, in 0.207s\n",
      "[509/3000] 1 tree, 31 leaves, max depth = 9, train loss: 0.11188, val loss: 0.13997, in 0.423s\n",
      "[510/3000] 1 tree, 31 leaves, max depth = 12, train loss: 0.11177, val loss: 0.13990, in 0.273s\n",
      "[511/3000] 1 tree, 31 leaves, max depth = 11, train loss: 0.11169, val loss: 0.13989, in 0.253s\n",
      "[512/3000] 1 tree, 31 leaves, max depth = 8, train loss: 0.11163, val loss: 0.13988, in 0.214s\n",
      "[513/3000] 1 tree, 31 leaves, max depth = 11, train loss: 0.11156, val loss: 0.13985, in 0.190s\n",
      "[514/3000] 1 tree, 31 leaves, max depth = 13, train loss: 0.11145, val loss: 0.13980, in 0.251s\n",
      "[515/3000] 1 tree, 31 leaves, max depth = 10, train loss: 0.11136, val loss: 0.13976, in 0.238s\n",
      "[516/3000] 1 tree, 31 leaves, max depth = 11, train loss: 0.11126, val loss: 0.13971, in 0.223s\n",
      "[517/3000] 1 tree, 31 leaves, max depth = 10, train loss: 0.11119, val loss: 0.13970, in 0.205s\n",
      "[518/3000] 1 tree, 31 leaves, max depth = 12, train loss: 0.11111, val loss: 0.13969, in 0.205s\n",
      "[519/3000] 1 tree, 31 leaves, max depth = 12, train loss: 0.11105, val loss: 0.13967, in 0.210s\n",
      "[520/3000] 1 tree, 31 leaves, max depth = 12, train loss: 0.11099, val loss: 0.13968, in 0.190s\n",
      "[521/3000] 1 tree, 31 leaves, max depth = 11, train loss: 0.11086, val loss: 0.13961, in 0.272s\n",
      "[522/3000] 1 tree, 31 leaves, max depth = 13, train loss: 0.11078, val loss: 0.13960, in 0.245s\n",
      "[523/3000] 1 tree, 31 leaves, max depth = 11, train loss: 0.11073, val loss: 0.13958, in 0.180s\n",
      "[524/3000] 1 tree, 31 leaves, max depth = 9, train loss: 0.11066, val loss: 0.13956, in 0.209s\n",
      "[525/3000] 1 tree, 31 leaves, max depth = 8, train loss: 0.11059, val loss: 0.13948, in 0.210s\n",
      "[526/3000] 1 tree, 31 leaves, max depth = 13, train loss: 0.11051, val loss: 0.13946, in 0.255s\n",
      "[527/3000] 1 tree, 31 leaves, max depth = 9, train loss: 0.11047, val loss: 0.13946, in 0.170s\n",
      "[528/3000] 1 tree, 31 leaves, max depth = 10, train loss: 0.11040, val loss: 0.13946, in 0.197s\n",
      "[529/3000] 1 tree, 31 leaves, max depth = 11, train loss: 0.11031, val loss: 0.13943, in 0.393s\n",
      "[530/3000] 1 tree, 31 leaves, max depth = 9, train loss: 0.11023, val loss: 0.13941, in 0.237s\n",
      "[531/3000] 1 tree, 31 leaves, max depth = 11, train loss: 0.11016, val loss: 0.13942, in 0.226s\n",
      "[532/3000] 1 tree, 31 leaves, max depth = 10, train loss: 0.11009, val loss: 0.13939, in 0.201s\n",
      "[533/3000] 1 tree, 31 leaves, max depth = 10, train loss: 0.10999, val loss: 0.13935, in 0.241s\n",
      "[534/3000] 1 tree, 31 leaves, max depth = 10, train loss: 0.10993, val loss: 0.13935, in 0.193s\n",
      "[535/3000] 1 tree, 31 leaves, max depth = 13, train loss: 0.10988, val loss: 0.13935, in 0.200s\n",
      "[536/3000] 1 tree, 31 leaves, max depth = 12, train loss: 0.10977, val loss: 0.13928, in 0.243s\n",
      "[537/3000] 1 tree, 31 leaves, max depth = 13, train loss: 0.10970, val loss: 0.13925, in 0.197s\n",
      "[538/3000] 1 tree, 31 leaves, max depth = 15, train loss: 0.10963, val loss: 0.13925, in 0.193s\n",
      "[539/3000] 1 tree, 31 leaves, max depth = 13, train loss: 0.10955, val loss: 0.13925, in 0.278s\n",
      "[540/3000] 1 tree, 31 leaves, max depth = 11, train loss: 0.10943, val loss: 0.13920, in 0.269s\n",
      "[541/3000] 1 tree, 31 leaves, max depth = 10, train loss: 0.10935, val loss: 0.13916, in 0.238s\n",
      "[542/3000] 1 tree, 31 leaves, max depth = 15, train loss: 0.10929, val loss: 0.13916, in 0.215s\n",
      "[543/3000] 1 tree, 31 leaves, max depth = 10, train loss: 0.10918, val loss: 0.13911, in 0.219s\n",
      "[544/3000] 1 tree, 31 leaves, max depth = 13, train loss: 0.10905, val loss: 0.13905, in 0.255s\n",
      "[545/3000] 1 tree, 31 leaves, max depth = 12, train loss: 0.10891, val loss: 0.13898, in 0.242s\n",
      "[546/3000] 1 tree, 31 leaves, max depth = 11, train loss: 0.10881, val loss: 0.13895, in 0.209s\n",
      "[547/3000] 1 tree, 31 leaves, max depth = 9, train loss: 0.10873, val loss: 0.13893, in 0.250s\n",
      "[548/3000] 1 tree, 31 leaves, max depth = 11, train loss: 0.10866, val loss: 0.13893, in 0.180s\n",
      "[549/3000] 1 tree, 31 leaves, max depth = 14, train loss: 0.10859, val loss: 0.13890, in 0.217s\n",
      "[550/3000] 1 tree, 31 leaves, max depth = 10, train loss: 0.10854, val loss: 0.13889, in 0.214s\n",
      "[551/3000] 1 tree, 31 leaves, max depth = 9, train loss: 0.10846, val loss: 0.13886, in 0.323s\n",
      "[552/3000] 1 tree, 31 leaves, max depth = 9, train loss: 0.10837, val loss: 0.13883, in 0.305s\n",
      "[553/3000] 1 tree, 31 leaves, max depth = 11, train loss: 0.10829, val loss: 0.13883, in 0.259s\n",
      "[554/3000] 1 tree, 31 leaves, max depth = 9, train loss: 0.10820, val loss: 0.13881, in 0.283s\n",
      "[555/3000] 1 tree, 31 leaves, max depth = 10, train loss: 0.10810, val loss: 0.13873, in 0.266s\n",
      "[556/3000] 1 tree, 31 leaves, max depth = 10, train loss: 0.10803, val loss: 0.13873, in 0.221s\n",
      "[557/3000] 1 tree, 31 leaves, max depth = 10, train loss: 0.10790, val loss: 0.13863, in 0.254s\n",
      "[558/3000] 1 tree, 31 leaves, max depth = 11, train loss: 0.10776, val loss: 0.13850, in 0.269s\n",
      "[559/3000] 1 tree, 31 leaves, max depth = 13, train loss: 0.10768, val loss: 0.13843, in 0.222s\n",
      "[560/3000] 1 tree, 31 leaves, max depth = 9, train loss: 0.10760, val loss: 0.13840, in 0.223s\n",
      "[561/3000] 1 tree, 31 leaves, max depth = 13, train loss: 0.10754, val loss: 0.13838, in 0.237s\n",
      "[562/3000] 1 tree, 31 leaves, max depth = 11, train loss: 0.10748, val loss: 0.13839, in 0.182s\n",
      "[563/3000] 1 tree, 31 leaves, max depth = 11, train loss: 0.10738, val loss: 0.13834, in 0.245s\n",
      "[564/3000] 1 tree, 31 leaves, max depth = 9, train loss: 0.10728, val loss: 0.13829, in 0.221s\n",
      "[565/3000] 1 tree, 31 leaves, max depth = 14, train loss: 0.10729, val loss: 0.13829, in 0.233s\n",
      "[566/3000] 1 tree, 31 leaves, max depth = 11, train loss: 0.10722, val loss: 0.13825, in 0.214s\n",
      "[567/3000] 1 tree, 31 leaves, max depth = 18, train loss: 0.10716, val loss: 0.13819, in 0.219s\n",
      "[568/3000] 1 tree, 31 leaves, max depth = 12, train loss: 0.10711, val loss: 0.13818, in 0.198s\n",
      "[569/3000] 1 tree, 31 leaves, max depth = 9, train loss: 0.10705, val loss: 0.13817, in 0.195s\n",
      "[570/3000] 1 tree, 31 leaves, max depth = 14, train loss: 0.10698, val loss: 0.13814, in 0.241s\n",
      "[571/3000] 1 tree, 31 leaves, max depth = 8, train loss: 0.10690, val loss: 0.13812, in 0.349s\n",
      "[572/3000] 1 tree, 31 leaves, max depth = 10, train loss: 0.10683, val loss: 0.13810, in 0.213s\n",
      "[573/3000] 1 tree, 31 leaves, max depth = 9, train loss: 0.10678, val loss: 0.13808, in 0.229s\n",
      "[574/3000] 1 tree, 31 leaves, max depth = 15, train loss: 0.10670, val loss: 0.13810, in 0.283s\n",
      "[575/3000] 1 tree, 31 leaves, max depth = 10, train loss: 0.10662, val loss: 0.13807, in 0.297s\n",
      "[576/3000] 1 tree, 31 leaves, max depth = 12, train loss: 0.10653, val loss: 0.13806, in 0.285s\n",
      "[577/3000] 1 tree, 31 leaves, max depth = 12, train loss: 0.10645, val loss: 0.13805, in 0.266s\n",
      "[578/3000] 1 tree, 31 leaves, max depth = 10, train loss: 0.10638, val loss: 0.13803, in 0.225s\n",
      "[579/3000] 1 tree, 31 leaves, max depth = 10, train loss: 0.10630, val loss: 0.13801, in 0.267s\n",
      "[580/3000] 1 tree, 31 leaves, max depth = 10, train loss: 0.10621, val loss: 0.13800, in 0.195s\n",
      "[581/3000] 1 tree, 31 leaves, max depth = 11, train loss: 0.10611, val loss: 0.13796, in 0.259s\n",
      "[582/3000] 1 tree, 31 leaves, max depth = 13, train loss: 0.10604, val loss: 0.13796, in 0.218s\n",
      "[583/3000] 1 tree, 31 leaves, max depth = 10, train loss: 0.10597, val loss: 0.13795, in 0.202s\n",
      "[584/3000] 1 tree, 31 leaves, max depth = 16, train loss: 0.10589, val loss: 0.13794, in 0.233s\n",
      "[585/3000] 1 tree, 31 leaves, max depth = 9, train loss: 0.10581, val loss: 0.13793, in 0.227s\n",
      "[586/3000] 1 tree, 31 leaves, max depth = 11, train loss: 0.10576, val loss: 0.13795, in 0.196s\n",
      "[587/3000] 1 tree, 31 leaves, max depth = 11, train loss: 0.10570, val loss: 0.13795, in 0.232s\n",
      "[588/3000] 1 tree, 31 leaves, max depth = 12, train loss: 0.10562, val loss: 0.13790, in 0.262s\n",
      "[589/3000] 1 tree, 31 leaves, max depth = 10, train loss: 0.10555, val loss: 0.13785, in 0.238s\n",
      "[590/3000] 1 tree, 31 leaves, max depth = 12, train loss: 0.10549, val loss: 0.13782, in 0.183s\n",
      "[591/3000] 1 tree, 31 leaves, max depth = 12, train loss: 0.10541, val loss: 0.13781, in 0.235s\n",
      "[592/3000] 1 tree, 31 leaves, max depth = 13, train loss: 0.10534, val loss: 0.13783, in 0.363s\n",
      "[593/3000] 1 tree, 31 leaves, max depth = 12, train loss: 0.10527, val loss: 0.13781, in 0.213s\n",
      "[594/3000] 1 tree, 31 leaves, max depth = 8, train loss: 0.10517, val loss: 0.13779, in 0.255s\n",
      "[595/3000] 1 tree, 31 leaves, max depth = 9, train loss: 0.10509, val loss: 0.13778, in 0.198s\n",
      "[596/3000] 1 tree, 31 leaves, max depth = 10, train loss: 0.10500, val loss: 0.13771, in 0.272s\n",
      "[597/3000] 1 tree, 31 leaves, max depth = 11, train loss: 0.10493, val loss: 0.13769, in 0.191s\n",
      "[598/3000] 1 tree, 31 leaves, max depth = 12, train loss: 0.10486, val loss: 0.13768, in 0.204s\n",
      "[599/3000] 1 tree, 31 leaves, max depth = 12, train loss: 0.10479, val loss: 0.13766, in 0.253s\n",
      "[600/3000] 1 tree, 31 leaves, max depth = 12, train loss: 0.10472, val loss: 0.13769, in 0.204s\n",
      "[601/3000] 1 tree, 31 leaves, max depth = 16, train loss: 0.10465, val loss: 0.13771, in 0.244s\n",
      "[602/3000] 1 tree, 31 leaves, max depth = 12, train loss: 0.10455, val loss: 0.13766, in 0.237s\n",
      "[603/3000] 1 tree, 31 leaves, max depth = 16, train loss: 0.10447, val loss: 0.13764, in 0.263s\n",
      "[604/3000] 1 tree, 31 leaves, max depth = 14, train loss: 0.10441, val loss: 0.13765, in 0.200s\n",
      "[605/3000] 1 tree, 31 leaves, max depth = 10, train loss: 0.10428, val loss: 0.13756, in 0.240s\n",
      "[606/3000] 1 tree, 31 leaves, max depth = 13, train loss: 0.10419, val loss: 0.13753, in 0.249s\n",
      "[607/3000] 1 tree, 31 leaves, max depth = 9, train loss: 0.10412, val loss: 0.13752, in 0.185s\n",
      "[608/3000] 1 tree, 31 leaves, max depth = 8, train loss: 0.10405, val loss: 0.13748, in 0.238s\n",
      "[609/3000] 1 tree, 31 leaves, max depth = 10, train loss: 0.10398, val loss: 0.13744, in 0.197s\n",
      "[610/3000] 1 tree, 31 leaves, max depth = 9, train loss: 0.10394, val loss: 0.13745, in 0.172s\n",
      "[611/3000] 1 tree, 31 leaves, max depth = 10, train loss: 0.10388, val loss: 0.13746, in 0.234s\n",
      "[612/3000] 1 tree, 31 leaves, max depth = 13, train loss: 0.10382, val loss: 0.13746, in 0.189s\n",
      "[613/3000] 1 tree, 31 leaves, max depth = 13, train loss: 0.10376, val loss: 0.13746, in 0.329s\n",
      "[614/3000] 1 tree, 31 leaves, max depth = 11, train loss: 0.10369, val loss: 0.13744, in 0.232s\n",
      "[615/3000] 1 tree, 31 leaves, max depth = 11, train loss: 0.10361, val loss: 0.13743, in 0.288s\n",
      "[616/3000] 1 tree, 31 leaves, max depth = 9, train loss: 0.10354, val loss: 0.13738, in 0.233s\n",
      "[617/3000] 1 tree, 31 leaves, max depth = 10, train loss: 0.10349, val loss: 0.13737, in 0.198s\n",
      "[618/3000] 1 tree, 31 leaves, max depth = 10, train loss: 0.10337, val loss: 0.13727, in 0.278s\n",
      "[619/3000] 1 tree, 31 leaves, max depth = 10, train loss: 0.10328, val loss: 0.13722, in 0.280s\n",
      "[620/3000] 1 tree, 31 leaves, max depth = 8, train loss: 0.10320, val loss: 0.13724, in 0.258s\n",
      "[621/3000] 1 tree, 31 leaves, max depth = 14, train loss: 0.10311, val loss: 0.13721, in 0.200s\n",
      "[622/3000] 1 tree, 31 leaves, max depth = 10, train loss: 0.10303, val loss: 0.13719, in 0.234s\n",
      "[623/3000] 1 tree, 31 leaves, max depth = 14, train loss: 0.10296, val loss: 0.13716, in 0.215s\n",
      "[624/3000] 1 tree, 31 leaves, max depth = 13, train loss: 0.10288, val loss: 0.13713, in 0.231s\n",
      "[625/3000] 1 tree, 31 leaves, max depth = 11, train loss: 0.10281, val loss: 0.13712, in 0.221s\n",
      "[626/3000] 1 tree, 31 leaves, max depth = 12, train loss: 0.10272, val loss: 0.13709, in 0.238s\n",
      "[627/3000] 1 tree, 31 leaves, max depth = 13, train loss: 0.10266, val loss: 0.13711, in 0.201s\n",
      "[628/3000] 1 tree, 31 leaves, max depth = 9, train loss: 0.10258, val loss: 0.13711, in 0.254s\n",
      "[629/3000] 1 tree, 31 leaves, max depth = 14, train loss: 0.10252, val loss: 0.13710, in 0.204s\n",
      "[630/3000] 1 tree, 31 leaves, max depth = 12, train loss: 0.10244, val loss: 0.13708, in 0.228s\n",
      "[631/3000] 1 tree, 31 leaves, max depth = 13, train loss: 0.10237, val loss: 0.13708, in 0.220s\n",
      "[632/3000] 1 tree, 31 leaves, max depth = 11, train loss: 0.10230, val loss: 0.13707, in 0.219s\n",
      "[633/3000] 1 tree, 31 leaves, max depth = 12, train loss: 0.10223, val loss: 0.13705, in 0.201s\n",
      "[634/3000] 1 tree, 31 leaves, max depth = 11, train loss: 0.10215, val loss: 0.13700, in 0.364s\n",
      "[635/3000] 1 tree, 31 leaves, max depth = 13, train loss: 0.10207, val loss: 0.13698, in 0.304s\n",
      "[636/3000] 1 tree, 31 leaves, max depth = 8, train loss: 0.10199, val loss: 0.13693, in 0.270s\n",
      "[637/3000] 1 tree, 31 leaves, max depth = 13, train loss: 0.10193, val loss: 0.13690, in 0.249s\n",
      "[638/3000] 1 tree, 31 leaves, max depth = 10, train loss: 0.10185, val loss: 0.13689, in 0.270s\n",
      "[639/3000] 1 tree, 31 leaves, max depth = 10, train loss: 0.10175, val loss: 0.13689, in 0.350s\n",
      "[640/3000] 1 tree, 31 leaves, max depth = 9, train loss: 0.10166, val loss: 0.13684, in 0.229s\n",
      "[641/3000] 1 tree, 31 leaves, max depth = 10, train loss: 0.10161, val loss: 0.13685, in 0.222s\n",
      "[642/3000] 1 tree, 31 leaves, max depth = 8, train loss: 0.10157, val loss: 0.13683, in 0.204s\n",
      "[643/3000] 1 tree, 31 leaves, max depth = 10, train loss: 0.10151, val loss: 0.13682, in 0.291s\n",
      "[644/3000] 1 tree, 31 leaves, max depth = 15, train loss: 0.10144, val loss: 0.13679, in 0.237s\n",
      "[645/3000] 1 tree, 31 leaves, max depth = 11, train loss: 0.10138, val loss: 0.13679, in 0.206s\n",
      "[646/3000] 1 tree, 31 leaves, max depth = 15, train loss: 0.10130, val loss: 0.13674, in 0.256s\n",
      "[647/3000] 1 tree, 31 leaves, max depth = 11, train loss: 0.10123, val loss: 0.13673, in 0.230s\n",
      "[648/3000] 1 tree, 31 leaves, max depth = 11, train loss: 0.10117, val loss: 0.13672, in 0.211s\n",
      "[649/3000] 1 tree, 31 leaves, max depth = 10, train loss: 0.10111, val loss: 0.13672, in 0.200s\n",
      "[650/3000] 1 tree, 31 leaves, max depth = 13, train loss: 0.10106, val loss: 0.13668, in 0.206s\n",
      "[651/3000] 1 tree, 31 leaves, max depth = 12, train loss: 0.10100, val loss: 0.13669, in 0.215s\n",
      "[652/3000] 1 tree, 31 leaves, max depth = 10, train loss: 0.10094, val loss: 0.13670, in 0.190s\n",
      "[653/3000] 1 tree, 31 leaves, max depth = 12, train loss: 0.10089, val loss: 0.13669, in 0.194s\n",
      "[654/3000] 1 tree, 31 leaves, max depth = 10, train loss: 0.10073, val loss: 0.13654, in 0.257s\n",
      "[655/3000] 1 tree, 31 leaves, max depth = 14, train loss: 0.10063, val loss: 0.13644, in 0.410s\n",
      "[656/3000] 1 tree, 31 leaves, max depth = 12, train loss: 0.10056, val loss: 0.13642, in 0.232s\n",
      "[657/3000] 1 tree, 31 leaves, max depth = 12, train loss: 0.10048, val loss: 0.13641, in 0.257s\n",
      "[658/3000] 1 tree, 31 leaves, max depth = 12, train loss: 0.10039, val loss: 0.13638, in 0.228s\n",
      "[659/3000] 1 tree, 31 leaves, max depth = 11, train loss: 0.10030, val loss: 0.13635, in 0.221s\n",
      "[660/3000] 1 tree, 31 leaves, max depth = 10, train loss: 0.10024, val loss: 0.13637, in 0.203s\n",
      "[661/3000] 1 tree, 31 leaves, max depth = 11, train loss: 0.10018, val loss: 0.13635, in 0.184s\n",
      "[662/3000] 1 tree, 31 leaves, max depth = 15, train loss: 0.10010, val loss: 0.13630, in 0.262s\n",
      "[663/3000] 1 tree, 31 leaves, max depth = 17, train loss: 0.10002, val loss: 0.13628, in 0.246s\n",
      "[664/3000] 1 tree, 31 leaves, max depth = 15, train loss: 0.09996, val loss: 0.13629, in 0.175s\n",
      "[665/3000] 1 tree, 31 leaves, max depth = 13, train loss: 0.09988, val loss: 0.13629, in 0.256s\n",
      "[666/3000] 1 tree, 31 leaves, max depth = 11, train loss: 0.09983, val loss: 0.13629, in 0.177s\n",
      "[667/3000] 1 tree, 31 leaves, max depth = 12, train loss: 0.09978, val loss: 0.13630, in 0.196s\n",
      "[668/3000] 1 tree, 31 leaves, max depth = 11, train loss: 0.09971, val loss: 0.13630, in 0.243s\n",
      "[669/3000] 1 tree, 31 leaves, max depth = 10, train loss: 0.09964, val loss: 0.13627, in 0.248s\n",
      "[670/3000] 1 tree, 31 leaves, max depth = 16, train loss: 0.09958, val loss: 0.13622, in 0.206s\n",
      "[671/3000] 1 tree, 31 leaves, max depth = 10, train loss: 0.09949, val loss: 0.13619, in 0.247s\n",
      "[672/3000] 1 tree, 31 leaves, max depth = 11, train loss: 0.09941, val loss: 0.13615, in 0.278s\n",
      "[673/3000] 1 tree, 31 leaves, max depth = 8, train loss: 0.09937, val loss: 0.13617, in 0.180s\n",
      "[674/3000] 1 tree, 31 leaves, max depth = 9, train loss: 0.09932, val loss: 0.13615, in 0.214s\n",
      "[675/3000] 1 tree, 31 leaves, max depth = 13, train loss: 0.09923, val loss: 0.13614, in 0.384s\n",
      "[676/3000] 1 tree, 31 leaves, max depth = 12, train loss: 0.09917, val loss: 0.13613, in 0.196s\n",
      "[677/3000] 1 tree, 31 leaves, max depth = 9, train loss: 0.09910, val loss: 0.13613, in 0.277s\n",
      "[678/3000] 1 tree, 31 leaves, max depth = 12, train loss: 0.09906, val loss: 0.13612, in 0.198s\n",
      "[679/3000] 1 tree, 31 leaves, max depth = 11, train loss: 0.09901, val loss: 0.13613, in 0.220s\n",
      "[680/3000] 1 tree, 31 leaves, max depth = 14, train loss: 0.09895, val loss: 0.13613, in 0.189s\n",
      "[681/3000] 1 tree, 31 leaves, max depth = 13, train loss: 0.09887, val loss: 0.13614, in 0.229s\n",
      "[682/3000] 1 tree, 31 leaves, max depth = 9, train loss: 0.09881, val loss: 0.13616, in 0.189s\n",
      "[683/3000] 1 tree, 31 leaves, max depth = 9, train loss: 0.09874, val loss: 0.13614, in 0.253s\n",
      "[684/3000] 1 tree, 31 leaves, max depth = 12, train loss: 0.09866, val loss: 0.13610, in 0.223s\n",
      "[685/3000] 1 tree, 31 leaves, max depth = 10, train loss: 0.09851, val loss: 0.13597, in 0.257s\n",
      "[686/3000] 1 tree, 31 leaves, max depth = 10, train loss: 0.09842, val loss: 0.13593, in 0.229s\n",
      "[687/3000] 1 tree, 31 leaves, max depth = 11, train loss: 0.09835, val loss: 0.13591, in 0.219s\n",
      "[688/3000] 1 tree, 31 leaves, max depth = 9, train loss: 0.09828, val loss: 0.13587, in 0.188s\n",
      "[689/3000] 1 tree, 31 leaves, max depth = 13, train loss: 0.09821, val loss: 0.13585, in 0.209s\n",
      "[690/3000] 1 tree, 31 leaves, max depth = 11, train loss: 0.09816, val loss: 0.13585, in 0.187s\n",
      "[691/3000] 1 tree, 31 leaves, max depth = 13, train loss: 0.09811, val loss: 0.13581, in 0.239s\n",
      "[692/3000] 1 tree, 31 leaves, max depth = 13, train loss: 0.09806, val loss: 0.13578, in 0.229s\n",
      "[693/3000] 1 tree, 31 leaves, max depth = 12, train loss: 0.09800, val loss: 0.13576, in 0.212s\n",
      "[694/3000] 1 tree, 31 leaves, max depth = 10, train loss: 0.09793, val loss: 0.13571, in 0.191s\n",
      "[695/3000] 1 tree, 31 leaves, max depth = 10, train loss: 0.09788, val loss: 0.13570, in 0.225s\n",
      "[696/3000] 1 tree, 31 leaves, max depth = 14, train loss: 0.09781, val loss: 0.13571, in 0.388s\n",
      "[697/3000] 1 tree, 31 leaves, max depth = 9, train loss: 0.09774, val loss: 0.13568, in 0.269s\n",
      "[698/3000] 1 tree, 31 leaves, max depth = 12, train loss: 0.09767, val loss: 0.13569, in 0.286s\n",
      "[699/3000] 1 tree, 31 leaves, max depth = 13, train loss: 0.09761, val loss: 0.13566, in 0.233s\n",
      "[700/3000] 1 tree, 31 leaves, max depth = 10, train loss: 0.09755, val loss: 0.13562, in 0.270s\n",
      "[701/3000] 1 tree, 31 leaves, max depth = 13, train loss: 0.09750, val loss: 0.13561, in 0.229s\n",
      "[702/3000] 1 tree, 31 leaves, max depth = 14, train loss: 0.09743, val loss: 0.13560, in 0.229s\n",
      "[703/3000] 1 tree, 31 leaves, max depth = 9, train loss: 0.09740, val loss: 0.13559, in 0.213s\n",
      "[704/3000] 1 tree, 31 leaves, max depth = 12, train loss: 0.09732, val loss: 0.13557, in 0.229s\n",
      "[705/3000] 1 tree, 31 leaves, max depth = 14, train loss: 0.09724, val loss: 0.13554, in 0.254s\n",
      "[706/3000] 1 tree, 31 leaves, max depth = 11, train loss: 0.09718, val loss: 0.13554, in 0.244s\n",
      "[707/3000] 1 tree, 31 leaves, max depth = 12, train loss: 0.09711, val loss: 0.13554, in 0.195s\n",
      "[708/3000] 1 tree, 31 leaves, max depth = 10, train loss: 0.09696, val loss: 0.13541, in 0.315s\n",
      "[709/3000] 1 tree, 31 leaves, max depth = 13, train loss: 0.09692, val loss: 0.13537, in 0.232s\n",
      "[710/3000] 1 tree, 31 leaves, max depth = 11, train loss: 0.09684, val loss: 0.13534, in 0.263s\n",
      "[711/3000] 1 tree, 31 leaves, max depth = 12, train loss: 0.09679, val loss: 0.13534, in 0.205s\n",
      "[712/3000] 1 tree, 31 leaves, max depth = 10, train loss: 0.09671, val loss: 0.13535, in 0.341s\n",
      "[713/3000] 1 tree, 31 leaves, max depth = 10, train loss: 0.09662, val loss: 0.13529, in 0.246s\n",
      "[714/3000] 1 tree, 31 leaves, max depth = 11, train loss: 0.09651, val loss: 0.13525, in 0.224s\n",
      "[715/3000] 1 tree, 31 leaves, max depth = 12, train loss: 0.09644, val loss: 0.13521, in 0.238s\n",
      "[716/3000] 1 tree, 31 leaves, max depth = 13, train loss: 0.09638, val loss: 0.13520, in 0.200s\n",
      "[717/3000] 1 tree, 31 leaves, max depth = 13, train loss: 0.09632, val loss: 0.13522, in 0.335s\n",
      "[718/3000] 1 tree, 31 leaves, max depth = 14, train loss: 0.09627, val loss: 0.13524, in 0.206s\n",
      "[719/3000] 1 tree, 31 leaves, max depth = 11, train loss: 0.09624, val loss: 0.13524, in 0.222s\n",
      "[720/3000] 1 tree, 31 leaves, max depth = 9, train loss: 0.09619, val loss: 0.13524, in 0.194s\n",
      "[721/3000] 1 tree, 31 leaves, max depth = 16, train loss: 0.09614, val loss: 0.13522, in 0.247s\n",
      "[722/3000] 1 tree, 31 leaves, max depth = 13, train loss: 0.09606, val loss: 0.13522, in 0.240s\n",
      "[723/3000] 1 tree, 31 leaves, max depth = 9, train loss: 0.09599, val loss: 0.13519, in 0.222s\n",
      "[724/3000] 1 tree, 31 leaves, max depth = 11, train loss: 0.09592, val loss: 0.13515, in 0.221s\n",
      "[725/3000] 1 tree, 31 leaves, max depth = 10, train loss: 0.09585, val loss: 0.13511, in 0.227s\n",
      "[726/3000] 1 tree, 31 leaves, max depth = 10, train loss: 0.09579, val loss: 0.13511, in 0.213s\n",
      "[727/3000] 1 tree, 31 leaves, max depth = 10, train loss: 0.09571, val loss: 0.13507, in 0.264s\n",
      "[728/3000] 1 tree, 31 leaves, max depth = 10, train loss: 0.09564, val loss: 0.13505, in 0.219s\n",
      "[729/3000] 1 tree, 31 leaves, max depth = 12, train loss: 0.09559, val loss: 0.13501, in 0.236s\n",
      "[730/3000] 1 tree, 31 leaves, max depth = 12, train loss: 0.09553, val loss: 0.13499, in 0.228s\n",
      "[731/3000] 1 tree, 31 leaves, max depth = 14, train loss: 0.09547, val loss: 0.13499, in 0.199s\n",
      "[732/3000] 1 tree, 31 leaves, max depth = 12, train loss: 0.09539, val loss: 0.13496, in 0.258s\n",
      "[733/3000] 1 tree, 31 leaves, max depth = 12, train loss: 0.09535, val loss: 0.13494, in 0.186s\n",
      "[734/3000] 1 tree, 31 leaves, max depth = 11, train loss: 0.09522, val loss: 0.13486, in 0.272s\n",
      "[735/3000] 1 tree, 31 leaves, max depth = 9, train loss: 0.09518, val loss: 0.13486, in 0.197s\n",
      "[736/3000] 1 tree, 31 leaves, max depth = 10, train loss: 0.09511, val loss: 0.13484, in 0.243s\n",
      "[737/3000] 1 tree, 31 leaves, max depth = 12, train loss: 0.09507, val loss: 0.13484, in 0.318s\n",
      "[738/3000] 1 tree, 31 leaves, max depth = 10, train loss: 0.09499, val loss: 0.13476, in 0.243s\n",
      "[739/3000] 1 tree, 31 leaves, max depth = 10, train loss: 0.09494, val loss: 0.13474, in 0.238s\n",
      "[740/3000] 1 tree, 31 leaves, max depth = 12, train loss: 0.09487, val loss: 0.13471, in 0.220s\n",
      "[741/3000] 1 tree, 31 leaves, max depth = 10, train loss: 0.09479, val loss: 0.13468, in 0.271s\n",
      "[742/3000] 1 tree, 31 leaves, max depth = 14, train loss: 0.09475, val loss: 0.13467, in 0.180s\n",
      "[743/3000] 1 tree, 31 leaves, max depth = 11, train loss: 0.09470, val loss: 0.13464, in 0.255s\n",
      "[744/3000] 1 tree, 31 leaves, max depth = 11, train loss: 0.09464, val loss: 0.13462, in 0.194s\n",
      "[745/3000] 1 tree, 31 leaves, max depth = 12, train loss: 0.09458, val loss: 0.13461, in 0.219s\n",
      "[746/3000] 1 tree, 31 leaves, max depth = 12, train loss: 0.09452, val loss: 0.13461, in 0.195s\n",
      "[747/3000] 1 tree, 31 leaves, max depth = 9, train loss: 0.09445, val loss: 0.13461, in 0.208s\n",
      "[748/3000] 1 tree, 31 leaves, max depth = 9, train loss: 0.09441, val loss: 0.13462, in 0.186s\n",
      "[749/3000] 1 tree, 31 leaves, max depth = 13, train loss: 0.09435, val loss: 0.13462, in 0.171s\n",
      "[750/3000] 1 tree, 31 leaves, max depth = 17, train loss: 0.09432, val loss: 0.13462, in 0.190s\n",
      "[751/3000] 1 tree, 31 leaves, max depth = 16, train loss: 0.09424, val loss: 0.13457, in 0.250s\n",
      "[752/3000] 1 tree, 31 leaves, max depth = 9, train loss: 0.09418, val loss: 0.13455, in 0.235s\n",
      "[753/3000] 1 tree, 31 leaves, max depth = 14, train loss: 0.09414, val loss: 0.13452, in 0.218s\n",
      "[754/3000] 1 tree, 31 leaves, max depth = 15, train loss: 0.09410, val loss: 0.13453, in 0.201s\n",
      "[755/3000] 1 tree, 31 leaves, max depth = 9, train loss: 0.09404, val loss: 0.13453, in 0.260s\n",
      "[756/3000] 1 tree, 31 leaves, max depth = 12, train loss: 0.09398, val loss: 0.13455, in 0.228s\n",
      "[757/3000] 1 tree, 31 leaves, max depth = 11, train loss: 0.09390, val loss: 0.13454, in 0.219s\n",
      "[758/3000] 1 tree, 31 leaves, max depth = 10, train loss: 0.09383, val loss: 0.13450, in 0.232s\n",
      "[759/3000] 1 tree, 31 leaves, max depth = 10, train loss: 0.09378, val loss: 0.13452, in 0.331s\n",
      "[760/3000] 1 tree, 31 leaves, max depth = 10, train loss: 0.09370, val loss: 0.13451, in 0.276s\n",
      "[761/3000] 1 tree, 31 leaves, max depth = 14, train loss: 0.09365, val loss: 0.13450, in 0.204s\n",
      "[762/3000] 1 tree, 31 leaves, max depth = 11, train loss: 0.09357, val loss: 0.13446, in 0.248s\n",
      "[763/3000] 1 tree, 31 leaves, max depth = 12, train loss: 0.09349, val loss: 0.13445, in 0.266s\n",
      "[764/3000] 1 tree, 31 leaves, max depth = 11, train loss: 0.09345, val loss: 0.13444, in 0.235s\n",
      "[765/3000] 1 tree, 31 leaves, max depth = 10, train loss: 0.09341, val loss: 0.13443, in 0.194s\n",
      "[766/3000] 1 tree, 31 leaves, max depth = 11, train loss: 0.09337, val loss: 0.13442, in 0.206s\n",
      "[767/3000] 1 tree, 31 leaves, max depth = 11, train loss: 0.09331, val loss: 0.13439, in 0.217s\n",
      "[768/3000] 1 tree, 31 leaves, max depth = 11, train loss: 0.09324, val loss: 0.13440, in 0.239s\n",
      "[769/3000] 1 tree, 31 leaves, max depth = 9, train loss: 0.09318, val loss: 0.13438, in 0.228s\n",
      "[770/3000] 1 tree, 31 leaves, max depth = 13, train loss: 0.09310, val loss: 0.13435, in 0.228s\n",
      "[771/3000] 1 tree, 31 leaves, max depth = 13, train loss: 0.09306, val loss: 0.13435, in 0.216s\n",
      "[772/3000] 1 tree, 31 leaves, max depth = 14, train loss: 0.09299, val loss: 0.13433, in 0.234s\n",
      "[773/3000] 1 tree, 31 leaves, max depth = 12, train loss: 0.09292, val loss: 0.13433, in 0.250s\n",
      "[774/3000] 1 tree, 31 leaves, max depth = 10, train loss: 0.09286, val loss: 0.13433, in 0.225s\n",
      "[775/3000] 1 tree, 31 leaves, max depth = 10, train loss: 0.09278, val loss: 0.13430, in 0.221s\n",
      "[776/3000] 1 tree, 31 leaves, max depth = 11, train loss: 0.09272, val loss: 0.13428, in 0.262s\n",
      "[777/3000] 1 tree, 31 leaves, max depth = 16, train loss: 0.09266, val loss: 0.13426, in 0.230s\n",
      "[778/3000] 1 tree, 31 leaves, max depth = 11, train loss: 0.09258, val loss: 0.13423, in 0.243s\n",
      "[779/3000] 1 tree, 31 leaves, max depth = 8, train loss: 0.09250, val loss: 0.13416, in 0.389s\n",
      "[780/3000] 1 tree, 31 leaves, max depth = 12, train loss: 0.09244, val loss: 0.13417, in 0.227s\n",
      "[781/3000] 1 tree, 31 leaves, max depth = 13, train loss: 0.09239, val loss: 0.13416, in 0.219s\n",
      "[782/3000] 1 tree, 31 leaves, max depth = 9, train loss: 0.09234, val loss: 0.13415, in 0.189s\n",
      "[783/3000] 1 tree, 31 leaves, max depth = 12, train loss: 0.09229, val loss: 0.13413, in 0.209s\n",
      "[784/3000] 1 tree, 31 leaves, max depth = 10, train loss: 0.09218, val loss: 0.13403, in 0.258s\n",
      "[785/3000] 1 tree, 31 leaves, max depth = 12, train loss: 0.09212, val loss: 0.13402, in 0.262s\n",
      "[786/3000] 1 tree, 31 leaves, max depth = 11, train loss: 0.09207, val loss: 0.13401, in 0.203s\n",
      "[787/3000] 1 tree, 31 leaves, max depth = 9, train loss: 0.09199, val loss: 0.13400, in 0.244s\n",
      "[788/3000] 1 tree, 31 leaves, max depth = 9, train loss: 0.09192, val loss: 0.13400, in 0.282s\n",
      "[789/3000] 1 tree, 31 leaves, max depth = 12, train loss: 0.09187, val loss: 0.13401, in 0.208s\n",
      "[790/3000] 1 tree, 31 leaves, max depth = 11, train loss: 0.09175, val loss: 0.13390, in 0.276s\n",
      "[791/3000] 1 tree, 31 leaves, max depth = 14, train loss: 0.09170, val loss: 0.13386, in 0.209s\n",
      "[792/3000] 1 tree, 31 leaves, max depth = 14, train loss: 0.09165, val loss: 0.13385, in 0.218s\n",
      "[793/3000] 1 tree, 31 leaves, max depth = 14, train loss: 0.09159, val loss: 0.13384, in 0.251s\n",
      "[794/3000] 1 tree, 31 leaves, max depth = 11, train loss: 0.09153, val loss: 0.13383, in 0.216s\n",
      "[795/3000] 1 tree, 31 leaves, max depth = 9, train loss: 0.09149, val loss: 0.13382, in 0.209s\n",
      "[796/3000] 1 tree, 31 leaves, max depth = 10, train loss: 0.09146, val loss: 0.13381, in 0.204s\n",
      "[797/3000] 1 tree, 31 leaves, max depth = 10, train loss: 0.09140, val loss: 0.13376, in 0.244s\n",
      "[798/3000] 1 tree, 31 leaves, max depth = 12, train loss: 0.09136, val loss: 0.13375, in 0.184s\n",
      "[799/3000] 1 tree, 31 leaves, max depth = 13, train loss: 0.09128, val loss: 0.13375, in 0.262s\n",
      "[800/3000] 1 tree, 31 leaves, max depth = 13, train loss: 0.09124, val loss: 0.13375, in 0.315s\n",
      "[801/3000] 1 tree, 31 leaves, max depth = 11, train loss: 0.09121, val loss: 0.13375, in 0.208s\n",
      "[802/3000] 1 tree, 31 leaves, max depth = 14, train loss: 0.09116, val loss: 0.13373, in 0.210s\n",
      "[803/3000] 1 tree, 31 leaves, max depth = 11, train loss: 0.09112, val loss: 0.13370, in 0.225s\n",
      "[804/3000] 1 tree, 31 leaves, max depth = 12, train loss: 0.09104, val loss: 0.13368, in 0.245s\n",
      "[805/3000] 1 tree, 31 leaves, max depth = 10, train loss: 0.09098, val loss: 0.13366, in 0.257s\n",
      "[806/3000] 1 tree, 31 leaves, max depth = 9, train loss: 0.09091, val loss: 0.13366, in 0.229s\n",
      "[807/3000] 1 tree, 31 leaves, max depth = 11, train loss: 0.09086, val loss: 0.13365, in 0.208s\n",
      "[808/3000] 1 tree, 31 leaves, max depth = 11, train loss: 0.09079, val loss: 0.13361, in 0.236s\n",
      "[809/3000] 1 tree, 31 leaves, max depth = 9, train loss: 0.09074, val loss: 0.13360, in 0.205s\n",
      "[810/3000] 1 tree, 31 leaves, max depth = 9, train loss: 0.09067, val loss: 0.13357, in 0.220s\n",
      "[811/3000] 1 tree, 31 leaves, max depth = 12, train loss: 0.09060, val loss: 0.13357, in 0.259s\n",
      "[812/3000] 1 tree, 31 leaves, max depth = 9, train loss: 0.09055, val loss: 0.13355, in 0.193s\n",
      "[813/3000] 1 tree, 31 leaves, max depth = 10, train loss: 0.09048, val loss: 0.13353, in 0.204s\n",
      "[814/3000] 1 tree, 31 leaves, max depth = 9, train loss: 0.09044, val loss: 0.13352, in 0.203s\n",
      "[815/3000] 1 tree, 31 leaves, max depth = 8, train loss: 0.09040, val loss: 0.13350, in 0.195s\n",
      "[816/3000] 1 tree, 31 leaves, max depth = 13, train loss: 0.09035, val loss: 0.13349, in 0.200s\n",
      "[817/3000] 1 tree, 31 leaves, max depth = 9, train loss: 0.09028, val loss: 0.13347, in 0.253s\n",
      "[818/3000] 1 tree, 31 leaves, max depth = 13, train loss: 0.09023, val loss: 0.13347, in 0.200s\n",
      "[819/3000] 1 tree, 31 leaves, max depth = 9, train loss: 0.09016, val loss: 0.13345, in 0.245s\n",
      "[820/3000] 1 tree, 31 leaves, max depth = 13, train loss: 0.09010, val loss: 0.13343, in 0.184s\n",
      "[821/3000] 1 tree, 31 leaves, max depth = 15, train loss: 0.09006, val loss: 0.13339, in 0.330s\n",
      "[822/3000] 1 tree, 31 leaves, max depth = 12, train loss: 0.09001, val loss: 0.13338, in 0.269s\n",
      "[823/3000] 1 tree, 31 leaves, max depth = 12, train loss: 0.08996, val loss: 0.13340, in 0.228s\n",
      "[824/3000] 1 tree, 31 leaves, max depth = 10, train loss: 0.08989, val loss: 0.13338, in 0.242s\n",
      "[825/3000] 1 tree, 31 leaves, max depth = 11, train loss: 0.08980, val loss: 0.13332, in 0.265s\n",
      "[826/3000] 1 tree, 31 leaves, max depth = 11, train loss: 0.08976, val loss: 0.13333, in 0.209s\n",
      "[827/3000] 1 tree, 31 leaves, max depth = 13, train loss: 0.08970, val loss: 0.13333, in 0.192s\n",
      "[828/3000] 1 tree, 31 leaves, max depth = 13, train loss: 0.08966, val loss: 0.13332, in 0.188s\n",
      "[829/3000] 1 tree, 31 leaves, max depth = 11, train loss: 0.08959, val loss: 0.13328, in 0.209s\n",
      "[830/3000] 1 tree, 31 leaves, max depth = 10, train loss: 0.08954, val loss: 0.13325, in 0.229s\n",
      "[831/3000] 1 tree, 31 leaves, max depth = 11, train loss: 0.08950, val loss: 0.13325, in 0.216s\n",
      "[832/3000] 1 tree, 31 leaves, max depth = 12, train loss: 0.08947, val loss: 0.13324, in 0.224s\n",
      "[833/3000] 1 tree, 31 leaves, max depth = 9, train loss: 0.08942, val loss: 0.13323, in 0.193s\n",
      "[834/3000] 1 tree, 31 leaves, max depth = 8, train loss: 0.08935, val loss: 0.13322, in 0.222s\n",
      "[835/3000] 1 tree, 31 leaves, max depth = 11, train loss: 0.08929, val loss: 0.13322, in 0.261s\n",
      "[836/3000] 1 tree, 31 leaves, max depth = 11, train loss: 0.08924, val loss: 0.13320, in 0.178s\n",
      "[837/3000] 1 tree, 31 leaves, max depth = 9, train loss: 0.08916, val loss: 0.13315, in 0.257s\n",
      "[838/3000] 1 tree, 31 leaves, max depth = 13, train loss: 0.08909, val loss: 0.13310, in 0.213s\n",
      "[839/3000] 1 tree, 31 leaves, max depth = 10, train loss: 0.08901, val loss: 0.13308, in 0.271s\n",
      "[840/3000] 1 tree, 31 leaves, max depth = 14, train loss: 0.08897, val loss: 0.13310, in 0.203s\n",
      "[841/3000] 1 tree, 31 leaves, max depth = 10, train loss: 0.08893, val loss: 0.13308, in 0.190s\n",
      "[842/3000] 1 tree, 31 leaves, max depth = 9, train loss: 0.08888, val loss: 0.13306, in 0.323s\n",
      "[843/3000] 1 tree, 31 leaves, max depth = 10, train loss: 0.08884, val loss: 0.13307, in 0.221s\n",
      "[844/3000] 1 tree, 31 leaves, max depth = 16, train loss: 0.08879, val loss: 0.13307, in 0.239s\n",
      "[845/3000] 1 tree, 31 leaves, max depth = 9, train loss: 0.08871, val loss: 0.13305, in 0.338s\n",
      "[846/3000] 1 tree, 31 leaves, max depth = 9, train loss: 0.08866, val loss: 0.13307, in 0.265s\n",
      "[847/3000] 1 tree, 31 leaves, max depth = 9, train loss: 0.08859, val loss: 0.13306, in 0.300s\n",
      "[848/3000] 1 tree, 31 leaves, max depth = 9, train loss: 0.08851, val loss: 0.13303, in 0.254s\n",
      "[849/3000] 1 tree, 31 leaves, max depth = 10, train loss: 0.08847, val loss: 0.13303, in 0.237s\n",
      "[850/3000] 1 tree, 31 leaves, max depth = 11, train loss: 0.08841, val loss: 0.13300, in 0.196s\n",
      "[851/3000] 1 tree, 31 leaves, max depth = 13, train loss: 0.08835, val loss: 0.13301, in 0.250s\n",
      "[852/3000] 1 tree, 31 leaves, max depth = 14, train loss: 0.08827, val loss: 0.13301, in 0.221s\n",
      "[853/3000] 1 tree, 31 leaves, max depth = 11, train loss: 0.08819, val loss: 0.13298, in 0.261s\n",
      "[854/3000] 1 tree, 31 leaves, max depth = 10, train loss: 0.08813, val loss: 0.13295, in 0.196s\n",
      "[855/3000] 1 tree, 31 leaves, max depth = 13, train loss: 0.08806, val loss: 0.13293, in 0.234s\n",
      "[856/3000] 1 tree, 31 leaves, max depth = 11, train loss: 0.08798, val loss: 0.13290, in 0.276s\n",
      "[857/3000] 1 tree, 31 leaves, max depth = 16, train loss: 0.08792, val loss: 0.13286, in 0.241s\n",
      "[858/3000] 1 tree, 31 leaves, max depth = 16, train loss: 0.08787, val loss: 0.13287, in 0.220s\n",
      "[859/3000] 1 tree, 31 leaves, max depth = 12, train loss: 0.08781, val loss: 0.13287, in 0.198s\n",
      "[860/3000] 1 tree, 31 leaves, max depth = 13, train loss: 0.08776, val loss: 0.13287, in 0.170s\n",
      "[861/3000] 1 tree, 31 leaves, max depth = 12, train loss: 0.08772, val loss: 0.13284, in 0.208s\n",
      "[862/3000] 1 tree, 31 leaves, max depth = 9, train loss: 0.08768, val loss: 0.13285, in 0.211s\n",
      "[863/3000] 1 tree, 31 leaves, max depth = 10, train loss: 0.08764, val loss: 0.13283, in 0.336s\n",
      "[864/3000] 1 tree, 31 leaves, max depth = 10, train loss: 0.08759, val loss: 0.13281, in 0.246s\n",
      "[865/3000] 1 tree, 31 leaves, max depth = 10, train loss: 0.08750, val loss: 0.13276, in 0.310s\n",
      "[866/3000] 1 tree, 31 leaves, max depth = 13, train loss: 0.08745, val loss: 0.13275, in 0.208s\n",
      "[867/3000] 1 tree, 31 leaves, max depth = 11, train loss: 0.08739, val loss: 0.13273, in 0.234s\n",
      "[868/3000] 1 tree, 31 leaves, max depth = 12, train loss: 0.08733, val loss: 0.13273, in 0.205s\n",
      "[869/3000] 1 tree, 31 leaves, max depth = 10, train loss: 0.08730, val loss: 0.13272, in 0.199s\n",
      "[870/3000] 1 tree, 31 leaves, max depth = 10, train loss: 0.08719, val loss: 0.13264, in 0.328s\n",
      "[871/3000] 1 tree, 31 leaves, max depth = 12, train loss: 0.08713, val loss: 0.13265, in 0.248s\n",
      "[872/3000] 1 tree, 31 leaves, max depth = 12, train loss: 0.08706, val loss: 0.13264, in 0.272s\n",
      "[873/3000] 1 tree, 31 leaves, max depth = 10, train loss: 0.08701, val loss: 0.13262, in 0.225s\n",
      "[874/3000] 1 tree, 31 leaves, max depth = 12, train loss: 0.08696, val loss: 0.13261, in 0.188s\n",
      "[875/3000] 1 tree, 31 leaves, max depth = 13, train loss: 0.08689, val loss: 0.13257, in 0.278s\n",
      "[876/3000] 1 tree, 31 leaves, max depth = 10, train loss: 0.08683, val loss: 0.13257, in 0.222s\n",
      "[877/3000] 1 tree, 31 leaves, max depth = 10, train loss: 0.08677, val loss: 0.13255, in 0.214s\n",
      "[878/3000] 1 tree, 31 leaves, max depth = 10, train loss: 0.08671, val loss: 0.13257, in 0.216s\n",
      "[879/3000] 1 tree, 31 leaves, max depth = 13, train loss: 0.08666, val loss: 0.13258, in 0.183s\n",
      "[880/3000] 1 tree, 31 leaves, max depth = 8, train loss: 0.08660, val loss: 0.13256, in 0.207s\n",
      "[881/3000] 1 tree, 31 leaves, max depth = 8, train loss: 0.08656, val loss: 0.13254, in 0.203s\n",
      "[882/3000] 1 tree, 31 leaves, max depth = 9, train loss: 0.08652, val loss: 0.13250, in 0.246s\n",
      "[883/3000] 1 tree, 31 leaves, max depth = 11, train loss: 0.08646, val loss: 0.13248, in 0.228s\n",
      "[884/3000] 1 tree, 31 leaves, max depth = 10, train loss: 0.08642, val loss: 0.13245, in 0.377s\n",
      "[885/3000] 1 tree, 31 leaves, max depth = 13, train loss: 0.08637, val loss: 0.13244, in 0.277s\n",
      "[886/3000] 1 tree, 31 leaves, max depth = 14, train loss: 0.08634, val loss: 0.13244, in 0.198s\n",
      "[887/3000] 1 tree, 31 leaves, max depth = 10, train loss: 0.08628, val loss: 0.13246, in 0.235s\n",
      "[888/3000] 1 tree, 31 leaves, max depth = 11, train loss: 0.08622, val loss: 0.13243, in 0.299s\n",
      "[889/3000] 1 tree, 31 leaves, max depth = 9, train loss: 0.08618, val loss: 0.13242, in 0.203s\n",
      "[890/3000] 1 tree, 31 leaves, max depth = 12, train loss: 0.08612, val loss: 0.13241, in 0.233s\n",
      "[891/3000] 1 tree, 31 leaves, max depth = 15, train loss: 0.08606, val loss: 0.13241, in 0.285s\n",
      "[892/3000] 1 tree, 31 leaves, max depth = 10, train loss: 0.08600, val loss: 0.13241, in 0.214s\n",
      "[893/3000] 1 tree, 31 leaves, max depth = 10, train loss: 0.08591, val loss: 0.13232, in 0.240s\n",
      "[894/3000] 1 tree, 31 leaves, max depth = 14, train loss: 0.08585, val loss: 0.13230, in 0.225s\n",
      "[895/3000] 1 tree, 31 leaves, max depth = 14, train loss: 0.08581, val loss: 0.13229, in 0.217s\n",
      "[896/3000] 1 tree, 31 leaves, max depth = 15, train loss: 0.08574, val loss: 0.13226, in 0.275s\n",
      "[897/3000] 1 tree, 31 leaves, max depth = 12, train loss: 0.08564, val loss: 0.13220, in 0.268s\n",
      "[898/3000] 1 tree, 31 leaves, max depth = 11, train loss: 0.08557, val loss: 0.13216, in 0.263s\n",
      "[899/3000] 1 tree, 31 leaves, max depth = 13, train loss: 0.08552, val loss: 0.13214, in 0.209s\n",
      "[900/3000] 1 tree, 31 leaves, max depth = 10, train loss: 0.08547, val loss: 0.13215, in 0.178s\n",
      "[901/3000] 1 tree, 31 leaves, max depth = 11, train loss: 0.08541, val loss: 0.13215, in 0.237s\n",
      "[902/3000] 1 tree, 31 leaves, max depth = 14, train loss: 0.08537, val loss: 0.13216, in 0.186s\n",
      "[903/3000] 1 tree, 31 leaves, max depth = 12, train loss: 0.08532, val loss: 0.13214, in 0.210s\n",
      "[904/3000] 1 tree, 31 leaves, max depth = 11, train loss: 0.08528, val loss: 0.13213, in 0.196s\n",
      "[905/3000] 1 tree, 31 leaves, max depth = 8, train loss: 0.08522, val loss: 0.13212, in 0.346s\n",
      "[906/3000] 1 tree, 31 leaves, max depth = 12, train loss: 0.08517, val loss: 0.13214, in 0.229s\n",
      "[907/3000] 1 tree, 31 leaves, max depth = 12, train loss: 0.08514, val loss: 0.13212, in 0.254s\n",
      "[908/3000] 1 tree, 31 leaves, max depth = 15, train loss: 0.08508, val loss: 0.13212, in 0.240s\n",
      "[909/3000] 1 tree, 31 leaves, max depth = 19, train loss: 0.08502, val loss: 0.13212, in 0.211s\n",
      "[910/3000] 1 tree, 31 leaves, max depth = 11, train loss: 0.08495, val loss: 0.13211, in 0.264s\n",
      "[911/3000] 1 tree, 31 leaves, max depth = 9, train loss: 0.08490, val loss: 0.13210, in 0.222s\n",
      "[912/3000] 1 tree, 31 leaves, max depth = 8, train loss: 0.08483, val loss: 0.13208, in 0.264s\n",
      "[913/3000] 1 tree, 31 leaves, max depth = 14, train loss: 0.08476, val loss: 0.13207, in 0.202s\n",
      "[914/3000] 1 tree, 31 leaves, max depth = 10, train loss: 0.08471, val loss: 0.13208, in 0.280s\n",
      "[915/3000] 1 tree, 31 leaves, max depth = 11, train loss: 0.08464, val loss: 0.13205, in 0.263s\n",
      "[916/3000] 1 tree, 31 leaves, max depth = 14, train loss: 0.08459, val loss: 0.13205, in 0.239s\n",
      "[917/3000] 1 tree, 31 leaves, max depth = 8, train loss: 0.08454, val loss: 0.13206, in 0.217s\n",
      "[918/3000] 1 tree, 31 leaves, max depth = 14, train loss: 0.08449, val loss: 0.13207, in 0.203s\n",
      "[919/3000] 1 tree, 31 leaves, max depth = 11, train loss: 0.08443, val loss: 0.13205, in 0.252s\n",
      "[920/3000] 1 tree, 31 leaves, max depth = 11, train loss: 0.08439, val loss: 0.13206, in 0.173s\n",
      "[921/3000] 1 tree, 31 leaves, max depth = 9, train loss: 0.08437, val loss: 0.13204, in 0.190s\n",
      "[922/3000] 1 tree, 31 leaves, max depth = 13, train loss: 0.08430, val loss: 0.13204, in 0.256s\n",
      "[923/3000] 1 tree, 31 leaves, max depth = 12, train loss: 0.08421, val loss: 0.13199, in 0.247s\n",
      "[924/3000] 1 tree, 31 leaves, max depth = 11, train loss: 0.08416, val loss: 0.13200, in 0.205s\n",
      "[925/3000] 1 tree, 31 leaves, max depth = 11, train loss: 0.08409, val loss: 0.13193, in 0.244s\n",
      "[926/3000] 1 tree, 31 leaves, max depth = 9, train loss: 0.08403, val loss: 0.13193, in 0.419s\n",
      "[927/3000] 1 tree, 31 leaves, max depth = 16, train loss: 0.08399, val loss: 0.13192, in 0.575s\n",
      "[928/3000] 1 tree, 31 leaves, max depth = 10, train loss: 0.08393, val loss: 0.13189, in 0.247s\n",
      "[929/3000] 1 tree, 31 leaves, max depth = 12, train loss: 0.08389, val loss: 0.13187, in 0.259s\n",
      "[930/3000] 1 tree, 31 leaves, max depth = 11, train loss: 0.08386, val loss: 0.13188, in 0.200s\n",
      "[931/3000] 1 tree, 31 leaves, max depth = 13, train loss: 0.08383, val loss: 0.13188, in 0.236s\n",
      "[932/3000] 1 tree, 31 leaves, max depth = 17, train loss: 0.08379, val loss: 0.13187, in 0.217s\n",
      "[933/3000] 1 tree, 31 leaves, max depth = 10, train loss: 0.08371, val loss: 0.13182, in 0.254s\n",
      "[934/3000] 1 tree, 31 leaves, max depth = 10, train loss: 0.08366, val loss: 0.13182, in 0.227s\n",
      "[935/3000] 1 tree, 31 leaves, max depth = 10, train loss: 0.08360, val loss: 0.13180, in 0.234s\n",
      "[936/3000] 1 tree, 31 leaves, max depth = 10, train loss: 0.08354, val loss: 0.13179, in 0.200s\n",
      "[937/3000] 1 tree, 31 leaves, max depth = 13, train loss: 0.08349, val loss: 0.13180, in 0.221s\n",
      "[938/3000] 1 tree, 31 leaves, max depth = 9, train loss: 0.08345, val loss: 0.13181, in 0.166s\n",
      "[939/3000] 1 tree, 31 leaves, max depth = 13, train loss: 0.08340, val loss: 0.13183, in 0.207s\n",
      "[940/3000] 1 tree, 31 leaves, max depth = 10, train loss: 0.08335, val loss: 0.13183, in 0.210s\n",
      "[941/3000] 1 tree, 31 leaves, max depth = 11, train loss: 0.08331, val loss: 0.13182, in 0.195s\n",
      "[942/3000] 1 tree, 31 leaves, max depth = 10, train loss: 0.08325, val loss: 0.13183, in 0.214s\n",
      "[943/3000] 1 tree, 31 leaves, max depth = 12, train loss: 0.08319, val loss: 0.13180, in 0.224s\n",
      "[944/3000] 1 tree, 31 leaves, max depth = 16, train loss: 0.08315, val loss: 0.13181, in 0.196s\n",
      "[945/3000] 1 tree, 31 leaves, max depth = 12, train loss: 0.08309, val loss: 0.13181, in 0.228s\n",
      "[946/3000] 1 tree, 31 leaves, max depth = 11, train loss: 0.08303, val loss: 0.13180, in 0.202s\n",
      "[947/3000] 1 tree, 31 leaves, max depth = 10, train loss: 0.08297, val loss: 0.13180, in 0.236s\n",
      "[948/3000] 1 tree, 31 leaves, max depth = 9, train loss: 0.08292, val loss: 0.13180, in 0.344s\n",
      "[949/3000] 1 tree, 31 leaves, max depth = 13, train loss: 0.08285, val loss: 0.13176, in 0.300s\n",
      "[950/3000] 1 tree, 31 leaves, max depth = 9, train loss: 0.08281, val loss: 0.13175, in 0.225s\n",
      "[951/3000] 1 tree, 31 leaves, max depth = 10, train loss: 0.08276, val loss: 0.13173, in 0.240s\n",
      "[952/3000] 1 tree, 31 leaves, max depth = 10, train loss: 0.08272, val loss: 0.13172, in 0.229s\n",
      "[953/3000] 1 tree, 31 leaves, max depth = 9, train loss: 0.08266, val loss: 0.13172, in 0.251s\n",
      "[954/3000] 1 tree, 31 leaves, max depth = 11, train loss: 0.08262, val loss: 0.13172, in 0.202s\n",
      "[955/3000] 1 tree, 31 leaves, max depth = 10, train loss: 0.08255, val loss: 0.13164, in 0.276s\n",
      "[956/3000] 1 tree, 31 leaves, max depth = 12, train loss: 0.08250, val loss: 0.13162, in 0.218s\n",
      "[957/3000] 1 tree, 31 leaves, max depth = 11, train loss: 0.08244, val loss: 0.13161, in 0.226s\n",
      "[958/3000] 1 tree, 31 leaves, max depth = 13, train loss: 0.08238, val loss: 0.13160, in 0.217s\n",
      "[959/3000] 1 tree, 31 leaves, max depth = 13, train loss: 0.08234, val loss: 0.13161, in 0.191s\n",
      "[960/3000] 1 tree, 31 leaves, max depth = 12, train loss: 0.08229, val loss: 0.13161, in 0.225s\n",
      "[961/3000] 1 tree, 31 leaves, max depth = 16, train loss: 0.08224, val loss: 0.13161, in 0.200s\n",
      "[962/3000] 1 tree, 31 leaves, max depth = 8, train loss: 0.08221, val loss: 0.13159, in 0.200s\n",
      "[963/3000] 1 tree, 31 leaves, max depth = 14, train loss: 0.08215, val loss: 0.13159, in 0.212s\n",
      "[964/3000] 1 tree, 31 leaves, max depth = 12, train loss: 0.08211, val loss: 0.13159, in 0.202s\n",
      "[965/3000] 1 tree, 31 leaves, max depth = 12, train loss: 0.08208, val loss: 0.13158, in 0.171s\n",
      "[966/3000] 1 tree, 31 leaves, max depth = 14, train loss: 0.08201, val loss: 0.13156, in 0.285s\n",
      "[967/3000] 1 tree, 31 leaves, max depth = 12, train loss: 0.08195, val loss: 0.13154, in 0.205s\n",
      "[968/3000] 1 tree, 31 leaves, max depth = 12, train loss: 0.08189, val loss: 0.13154, in 0.228s\n",
      "[969/3000] 1 tree, 31 leaves, max depth = 8, train loss: 0.08184, val loss: 0.13151, in 0.333s\n",
      "[970/3000] 1 tree, 31 leaves, max depth = 14, train loss: 0.08180, val loss: 0.13153, in 0.238s\n",
      "[971/3000] 1 tree, 31 leaves, max depth = 15, train loss: 0.08175, val loss: 0.13152, in 0.206s\n",
      "[972/3000] 1 tree, 31 leaves, max depth = 12, train loss: 0.08171, val loss: 0.13151, in 0.191s\n",
      "[973/3000] 1 tree, 31 leaves, max depth = 11, train loss: 0.08165, val loss: 0.13151, in 0.221s\n",
      "[974/3000] 1 tree, 31 leaves, max depth = 12, train loss: 0.08158, val loss: 0.13150, in 0.230s\n",
      "[975/3000] 1 tree, 31 leaves, max depth = 10, train loss: 0.08152, val loss: 0.13152, in 0.214s\n",
      "[976/3000] 1 tree, 31 leaves, max depth = 11, train loss: 0.08146, val loss: 0.13152, in 0.250s\n",
      "[977/3000] 1 tree, 31 leaves, max depth = 10, train loss: 0.08142, val loss: 0.13153, in 0.190s\n",
      "[978/3000] 1 tree, 31 leaves, max depth = 13, train loss: 0.08136, val loss: 0.13151, in 0.263s\n",
      "[979/3000] 1 tree, 31 leaves, max depth = 12, train loss: 0.08133, val loss: 0.13151, in 0.199s\n",
      "[980/3000] 1 tree, 31 leaves, max depth = 13, train loss: 0.08128, val loss: 0.13154, in 0.285s\n",
      "[981/3000] 1 tree, 31 leaves, max depth = 13, train loss: 0.08121, val loss: 0.13149, in 0.269s\n",
      "[982/3000] 1 tree, 31 leaves, max depth = 13, train loss: 0.08116, val loss: 0.13150, in 0.267s\n",
      "[983/3000] 1 tree, 31 leaves, max depth = 14, train loss: 0.08112, val loss: 0.13150, in 0.198s\n",
      "[984/3000] 1 tree, 31 leaves, max depth = 12, train loss: 0.08108, val loss: 0.13149, in 0.215s\n",
      "[985/3000] 1 tree, 31 leaves, max depth = 11, train loss: 0.08105, val loss: 0.13148, in 0.247s\n",
      "[986/3000] 1 tree, 31 leaves, max depth = 14, train loss: 0.08099, val loss: 0.13147, in 0.200s\n",
      "[987/3000] 1 tree, 31 leaves, max depth = 8, train loss: 0.08095, val loss: 0.13147, in 0.194s\n",
      "[988/3000] 1 tree, 31 leaves, max depth = 12, train loss: 0.08089, val loss: 0.13146, in 0.216s\n",
      "[989/3000] 1 tree, 31 leaves, max depth = 12, train loss: 0.08085, val loss: 0.13146, in 0.186s\n",
      "[990/3000] 1 tree, 31 leaves, max depth = 12, train loss: 0.08079, val loss: 0.13143, in 0.383s\n",
      "[991/3000] 1 tree, 31 leaves, max depth = 13, train loss: 0.08074, val loss: 0.13143, in 0.220s\n",
      "[992/3000] 1 tree, 31 leaves, max depth = 10, train loss: 0.08068, val loss: 0.13142, in 0.294s\n",
      "[993/3000] 1 tree, 31 leaves, max depth = 13, train loss: 0.08062, val loss: 0.13141, in 0.276s\n",
      "[994/3000] 1 tree, 31 leaves, max depth = 11, train loss: 0.08057, val loss: 0.13143, in 0.219s\n",
      "[995/3000] 1 tree, 31 leaves, max depth = 14, train loss: 0.08053, val loss: 0.13145, in 0.183s\n",
      "[996/3000] 1 tree, 31 leaves, max depth = 10, train loss: 0.08049, val loss: 0.13144, in 0.187s\n",
      "[997/3000] 1 tree, 31 leaves, max depth = 10, train loss: 0.08046, val loss: 0.13144, in 0.201s\n",
      "[998/3000] 1 tree, 31 leaves, max depth = 10, train loss: 0.08040, val loss: 0.13142, in 0.246s\n",
      "[999/3000] 1 tree, 31 leaves, max depth = 9, train loss: 0.08037, val loss: 0.13142, in 0.240s\n",
      "[1000/3000] 1 tree, 31 leaves, max depth = 12, train loss: 0.08032, val loss: 0.13139, in 0.210s\n",
      "[1001/3000] 1 tree, 31 leaves, max depth = 14, train loss: 0.08026, val loss: 0.13137, in 0.238s\n",
      "[1002/3000] 1 tree, 31 leaves, max depth = 13, train loss: 0.08019, val loss: 0.13134, in 0.241s\n",
      "[1003/3000] 1 tree, 31 leaves, max depth = 11, train loss: 0.08013, val loss: 0.13133, in 0.232s\n",
      "[1004/3000] 1 tree, 31 leaves, max depth = 11, train loss: 0.08008, val loss: 0.13131, in 0.204s\n",
      "[1005/3000] 1 tree, 31 leaves, max depth = 13, train loss: 0.08004, val loss: 0.13131, in 0.208s\n",
      "[1006/3000] 1 tree, 31 leaves, max depth = 14, train loss: 0.08000, val loss: 0.13130, in 0.202s\n",
      "[1007/3000] 1 tree, 31 leaves, max depth = 14, train loss: 0.07995, val loss: 0.13128, in 0.213s\n",
      "[1008/3000] 1 tree, 31 leaves, max depth = 11, train loss: 0.07989, val loss: 0.13125, in 0.239s\n",
      "[1009/3000] 1 tree, 31 leaves, max depth = 10, train loss: 0.07983, val loss: 0.13121, in 0.197s\n",
      "[1010/3000] 1 tree, 31 leaves, max depth = 9, train loss: 0.07977, val loss: 0.13120, in 0.250s\n",
      "[1011/3000] 1 tree, 31 leaves, max depth = 14, train loss: 0.07974, val loss: 0.13121, in 0.313s\n",
      "[1012/3000] 1 tree, 31 leaves, max depth = 14, train loss: 0.07971, val loss: 0.13121, in 0.220s\n",
      "[1013/3000] 1 tree, 31 leaves, max depth = 13, train loss: 0.07966, val loss: 0.13120, in 0.252s\n",
      "[1014/3000] 1 tree, 31 leaves, max depth = 12, train loss: 0.07960, val loss: 0.13120, in 0.234s\n",
      "[1015/3000] 1 tree, 31 leaves, max depth = 13, train loss: 0.07957, val loss: 0.13119, in 0.193s\n",
      "[1016/3000] 1 tree, 31 leaves, max depth = 14, train loss: 0.07954, val loss: 0.13119, in 0.205s\n",
      "[1017/3000] 1 tree, 31 leaves, max depth = 13, train loss: 0.07950, val loss: 0.13118, in 0.201s\n",
      "[1018/3000] 1 tree, 31 leaves, max depth = 11, train loss: 0.07945, val loss: 0.13117, in 0.258s\n",
      "[1019/3000] 1 tree, 31 leaves, max depth = 16, train loss: 0.07938, val loss: 0.13112, in 0.287s\n",
      "[1020/3000] 1 tree, 31 leaves, max depth = 12, train loss: 0.07930, val loss: 0.13105, in 0.244s\n",
      "[1021/3000] 1 tree, 31 leaves, max depth = 10, train loss: 0.07921, val loss: 0.13098, in 0.233s\n",
      "[1022/3000] 1 tree, 31 leaves, max depth = 10, train loss: 0.07915, val loss: 0.13094, in 0.267s\n",
      "[1023/3000] 1 tree, 31 leaves, max depth = 11, train loss: 0.07909, val loss: 0.13093, in 0.233s\n",
      "[1024/3000] 1 tree, 31 leaves, max depth = 15, train loss: 0.07905, val loss: 0.13093, in 0.209s\n",
      "[1025/3000] 1 tree, 31 leaves, max depth = 13, train loss: 0.07901, val loss: 0.13093, in 0.184s\n",
      "[1026/3000] 1 tree, 31 leaves, max depth = 12, train loss: 0.07895, val loss: 0.13091, in 0.221s\n",
      "[1027/3000] 1 tree, 31 leaves, max depth = 12, train loss: 0.07890, val loss: 0.13091, in 0.272s\n",
      "[1028/3000] 1 tree, 31 leaves, max depth = 13, train loss: 0.07886, val loss: 0.13091, in 0.199s\n",
      "[1029/3000] 1 tree, 31 leaves, max depth = 9, train loss: 0.07880, val loss: 0.13090, in 0.252s\n",
      "[1030/3000] 1 tree, 31 leaves, max depth = 9, train loss: 0.07874, val loss: 0.13090, in 0.234s\n",
      "[1031/3000] 1 tree, 31 leaves, max depth = 9, train loss: 0.07870, val loss: 0.13089, in 0.261s\n",
      "[1032/3000] 1 tree, 31 leaves, max depth = 11, train loss: 0.07865, val loss: 0.13091, in 0.340s\n",
      "[1033/3000] 1 tree, 31 leaves, max depth = 10, train loss: 0.07859, val loss: 0.13090, in 0.235s\n",
      "[1034/3000] 1 tree, 31 leaves, max depth = 10, train loss: 0.07855, val loss: 0.13092, in 0.244s\n",
      "[1035/3000] 1 tree, 31 leaves, max depth = 14, train loss: 0.07848, val loss: 0.13088, in 0.261s\n",
      "[1036/3000] 1 tree, 31 leaves, max depth = 13, train loss: 0.07843, val loss: 0.13089, in 0.226s\n",
      "[1037/3000] 1 tree, 31 leaves, max depth = 12, train loss: 0.07839, val loss: 0.13089, in 0.192s\n",
      "[1038/3000] 1 tree, 31 leaves, max depth = 14, train loss: 0.07835, val loss: 0.13089, in 0.210s\n",
      "[1039/3000] 1 tree, 31 leaves, max depth = 10, train loss: 0.07832, val loss: 0.13089, in 0.181s\n",
      "[1040/3000] 1 tree, 31 leaves, max depth = 9, train loss: 0.07830, val loss: 0.13090, in 0.235s\n",
      "[1041/3000] 1 tree, 31 leaves, max depth = 14, train loss: 0.07826, val loss: 0.13091, in 0.189s\n",
      "[1042/3000] 1 tree, 31 leaves, max depth = 12, train loss: 0.07821, val loss: 0.13089, in 0.211s\n",
      "[1043/3000] 1 tree, 31 leaves, max depth = 10, train loss: 0.07815, val loss: 0.13086, in 0.242s\n",
      "[1044/3000] 1 tree, 31 leaves, max depth = 10, train loss: 0.07809, val loss: 0.13084, in 0.267s\n",
      "[1045/3000] 1 tree, 31 leaves, max depth = 9, train loss: 0.07805, val loss: 0.13086, in 0.214s\n",
      "[1046/3000] 1 tree, 31 leaves, max depth = 13, train loss: 0.07800, val loss: 0.13088, in 0.248s\n",
      "[1047/3000] 1 tree, 31 leaves, max depth = 11, train loss: 0.07794, val loss: 0.13087, in 0.233s\n",
      "[1048/3000] 1 tree, 31 leaves, max depth = 13, train loss: 0.07789, val loss: 0.13088, in 0.204s\n",
      "[1049/3000] 1 tree, 31 leaves, max depth = 11, train loss: 0.07784, val loss: 0.13085, in 0.229s\n",
      "[1050/3000] 1 tree, 31 leaves, max depth = 10, train loss: 0.07781, val loss: 0.13085, in 0.294s\n",
      "[1051/3000] 1 tree, 31 leaves, max depth = 12, train loss: 0.07777, val loss: 0.13085, in 0.230s\n",
      "[1052/3000] 1 tree, 31 leaves, max depth = 9, train loss: 0.07772, val loss: 0.13084, in 0.242s\n",
      "[1053/3000] 1 tree, 31 leaves, max depth = 12, train loss: 0.07765, val loss: 0.13081, in 0.425s\n",
      "[1054/3000] 1 tree, 31 leaves, max depth = 11, train loss: 0.07758, val loss: 0.13082, in 0.349s\n",
      "[1055/3000] 1 tree, 31 leaves, max depth = 9, train loss: 0.07755, val loss: 0.13080, in 0.215s\n",
      "[1056/3000] 1 tree, 31 leaves, max depth = 12, train loss: 0.07748, val loss: 0.13077, in 0.250s\n",
      "[1057/3000] 1 tree, 31 leaves, max depth = 12, train loss: 0.07743, val loss: 0.13075, in 0.258s\n",
      "[1058/3000] 1 tree, 31 leaves, max depth = 10, train loss: 0.07738, val loss: 0.13074, in 0.262s\n",
      "[1059/3000] 1 tree, 31 leaves, max depth = 11, train loss: 0.07733, val loss: 0.13074, in 0.202s\n",
      "[1060/3000] 1 tree, 31 leaves, max depth = 12, train loss: 0.07729, val loss: 0.13075, in 0.190s\n",
      "[1061/3000] 1 tree, 31 leaves, max depth = 9, train loss: 0.07722, val loss: 0.13072, in 0.207s\n",
      "[1062/3000] 1 tree, 31 leaves, max depth = 13, train loss: 0.07719, val loss: 0.13071, in 0.187s\n",
      "[1063/3000] 1 tree, 31 leaves, max depth = 13, train loss: 0.07715, val loss: 0.13071, in 0.178s\n",
      "[1064/3000] 1 tree, 31 leaves, max depth = 9, train loss: 0.07711, val loss: 0.13071, in 0.188s\n",
      "[1065/3000] 1 tree, 31 leaves, max depth = 8, train loss: 0.07708, val loss: 0.13071, in 0.170s\n",
      "[1066/3000] 1 tree, 31 leaves, max depth = 10, train loss: 0.07703, val loss: 0.13072, in 0.183s\n",
      "[1067/3000] 1 tree, 31 leaves, max depth = 13, train loss: 0.07699, val loss: 0.13071, in 0.220s\n",
      "[1068/3000] 1 tree, 31 leaves, max depth = 15, train loss: 0.07694, val loss: 0.13071, in 0.193s\n",
      "[1069/3000] 1 tree, 31 leaves, max depth = 10, train loss: 0.07689, val loss: 0.13071, in 0.213s\n",
      "[1070/3000] 1 tree, 31 leaves, max depth = 11, train loss: 0.07684, val loss: 0.13069, in 0.198s\n",
      "[1071/3000] 1 tree, 31 leaves, max depth = 13, train loss: 0.07678, val loss: 0.13067, in 0.194s\n",
      "[1072/3000] 1 tree, 31 leaves, max depth = 9, train loss: 0.07675, val loss: 0.13067, in 0.203s\n",
      "[1073/3000] 1 tree, 31 leaves, max depth = 9, train loss: 0.07667, val loss: 0.13066, in 0.234s\n",
      "[1074/3000] 1 tree, 31 leaves, max depth = 15, train loss: 0.07662, val loss: 0.13065, in 0.343s\n",
      "[1075/3000] 1 tree, 31 leaves, max depth = 11, train loss: 0.07657, val loss: 0.13065, in 0.250s\n",
      "[1076/3000] 1 tree, 31 leaves, max depth = 12, train loss: 0.07652, val loss: 0.13063, in 0.192s\n",
      "[1077/3000] 1 tree, 31 leaves, max depth = 12, train loss: 0.07648, val loss: 0.13063, in 0.212s\n",
      "[1078/3000] 1 tree, 31 leaves, max depth = 17, train loss: 0.07642, val loss: 0.13061, in 0.227s\n",
      "[1079/3000] 1 tree, 31 leaves, max depth = 12, train loss: 0.07638, val loss: 0.13061, in 0.185s\n",
      "[1080/3000] 1 tree, 31 leaves, max depth = 11, train loss: 0.07632, val loss: 0.13058, in 0.209s\n",
      "[1081/3000] 1 tree, 31 leaves, max depth = 10, train loss: 0.07626, val loss: 0.13053, in 0.239s\n",
      "[1082/3000] 1 tree, 31 leaves, max depth = 11, train loss: 0.07621, val loss: 0.13054, in 0.199s\n",
      "[1083/3000] 1 tree, 31 leaves, max depth = 11, train loss: 0.07617, val loss: 0.13057, in 0.190s\n",
      "[1084/3000] 1 tree, 31 leaves, max depth = 12, train loss: 0.07612, val loss: 0.13056, in 0.182s\n",
      "[1085/3000] 1 tree, 31 leaves, max depth = 14, train loss: 0.07607, val loss: 0.13054, in 0.200s\n",
      "[1086/3000] 1 tree, 31 leaves, max depth = 10, train loss: 0.07603, val loss: 0.13055, in 0.198s\n",
      "[1087/3000] 1 tree, 31 leaves, max depth = 11, train loss: 0.07599, val loss: 0.13054, in 0.231s\n",
      "[1088/3000] 1 tree, 31 leaves, max depth = 13, train loss: 0.07594, val loss: 0.13057, in 0.207s\n",
      "[1089/3000] 1 tree, 31 leaves, max depth = 12, train loss: 0.07591, val loss: 0.13056, in 0.167s\n",
      "[1090/3000] 1 tree, 31 leaves, max depth = 11, train loss: 0.07589, val loss: 0.13056, in 0.165s\n",
      "[1091/3000] 1 tree, 31 leaves, max depth = 11, train loss: 0.07587, val loss: 0.13057, in 0.181s\n",
      "[1092/3000] 1 tree, 31 leaves, max depth = 10, train loss: 0.07583, val loss: 0.13053, in 0.170s\n",
      "[1093/3000] 1 tree, 31 leaves, max depth = 14, train loss: 0.07577, val loss: 0.13050, in 0.203s\n",
      "[1094/3000] 1 tree, 31 leaves, max depth = 14, train loss: 0.07571, val loss: 0.13047, in 0.199s\n",
      "[1095/3000] 1 tree, 31 leaves, max depth = 12, train loss: 0.07565, val loss: 0.13043, in 0.352s\n",
      "[1096/3000] 1 tree, 31 leaves, max depth = 12, train loss: 0.07562, val loss: 0.13043, in 0.204s\n",
      "[1097/3000] 1 tree, 31 leaves, max depth = 12, train loss: 0.07558, val loss: 0.13040, in 0.202s\n",
      "[1098/3000] 1 tree, 31 leaves, max depth = 13, train loss: 0.07553, val loss: 0.13040, in 0.221s\n",
      "[1099/3000] 1 tree, 31 leaves, max depth = 12, train loss: 0.07547, val loss: 0.13040, in 0.281s\n",
      "[1100/3000] 1 tree, 31 leaves, max depth = 13, train loss: 0.07540, val loss: 0.13040, in 0.229s\n",
      "[1101/3000] 1 tree, 31 leaves, max depth = 15, train loss: 0.07535, val loss: 0.13039, in 0.209s\n",
      "[1102/3000] 1 tree, 31 leaves, max depth = 10, train loss: 0.07528, val loss: 0.13034, in 0.204s\n",
      "[1103/3000] 1 tree, 31 leaves, max depth = 12, train loss: 0.07523, val loss: 0.13034, in 0.247s\n",
      "[1104/3000] 1 tree, 31 leaves, max depth = 10, train loss: 0.07520, val loss: 0.13034, in 0.161s\n",
      "[1105/3000] 1 tree, 31 leaves, max depth = 10, train loss: 0.07517, val loss: 0.13035, in 0.173s\n",
      "[1106/3000] 1 tree, 31 leaves, max depth = 9, train loss: 0.07512, val loss: 0.13033, in 0.179s\n",
      "[1107/3000] 1 tree, 31 leaves, max depth = 11, train loss: 0.07508, val loss: 0.13033, in 0.175s\n",
      "[1108/3000] 1 tree, 31 leaves, max depth = 14, train loss: 0.07505, val loss: 0.13033, in 0.158s\n",
      "[1109/3000] 1 tree, 31 leaves, max depth = 13, train loss: 0.07499, val loss: 0.13030, in 0.225s\n",
      "[1110/3000] 1 tree, 31 leaves, max depth = 11, train loss: 0.07494, val loss: 0.13031, in 0.194s\n",
      "[1111/3000] 1 tree, 31 leaves, max depth = 11, train loss: 0.07488, val loss: 0.13031, in 0.215s\n",
      "[1112/3000] 1 tree, 31 leaves, max depth = 10, train loss: 0.07484, val loss: 0.13032, in 0.168s\n",
      "[1113/3000] 1 tree, 31 leaves, max depth = 13, train loss: 0.07480, val loss: 0.13033, in 0.164s\n",
      "[1114/3000] 1 tree, 31 leaves, max depth = 10, train loss: 0.07475, val loss: 0.13032, in 0.202s\n",
      "[1115/3000] 1 tree, 31 leaves, max depth = 14, train loss: 0.07471, val loss: 0.13032, in 0.174s\n",
      "[1116/3000] 1 tree, 31 leaves, max depth = 8, train loss: 0.07466, val loss: 0.13031, in 0.359s\n",
      "[1117/3000] 1 tree, 31 leaves, max depth = 13, train loss: 0.07461, val loss: 0.13031, in 0.238s\n",
      "[1118/3000] 1 tree, 31 leaves, max depth = 9, train loss: 0.07455, val loss: 0.13031, in 0.237s\n",
      "[1119/3000] 1 tree, 31 leaves, max depth = 13, train loss: 0.07451, val loss: 0.13030, in 0.214s\n",
      "[1120/3000] 1 tree, 31 leaves, max depth = 11, train loss: 0.07447, val loss: 0.13031, in 0.214s\n",
      "[1121/3000] 1 tree, 31 leaves, max depth = 17, train loss: 0.07440, val loss: 0.13028, in 0.260s\n",
      "[1122/3000] 1 tree, 31 leaves, max depth = 13, train loss: 0.07433, val loss: 0.13023, in 0.211s\n",
      "[1123/3000] 1 tree, 31 leaves, max depth = 10, train loss: 0.07428, val loss: 0.13022, in 0.268s\n",
      "[1124/3000] 1 tree, 31 leaves, max depth = 13, train loss: 0.07423, val loss: 0.13021, in 0.255s\n",
      "[1125/3000] 1 tree, 31 leaves, max depth = 10, train loss: 0.07419, val loss: 0.13020, in 0.180s\n",
      "[1126/3000] 1 tree, 31 leaves, max depth = 12, train loss: 0.07414, val loss: 0.13020, in 0.206s\n",
      "[1127/3000] 1 tree, 31 leaves, max depth = 9, train loss: 0.07409, val loss: 0.13022, in 0.242s\n",
      "[1128/3000] 1 tree, 31 leaves, max depth = 11, train loss: 0.07407, val loss: 0.13023, in 0.193s\n",
      "[1129/3000] 1 tree, 31 leaves, max depth = 13, train loss: 0.07402, val loss: 0.13024, in 0.183s\n",
      "[1130/3000] 1 tree, 31 leaves, max depth = 10, train loss: 0.07396, val loss: 0.13022, in 0.205s\n",
      "[1131/3000] 1 tree, 31 leaves, max depth = 9, train loss: 0.07392, val loss: 0.13020, in 0.205s\n",
      "[1132/3000] 1 tree, 31 leaves, max depth = 8, train loss: 0.07385, val loss: 0.13018, in 0.228s\n",
      "[1133/3000] 1 tree, 31 leaves, max depth = 10, train loss: 0.07381, val loss: 0.13016, in 0.202s\n",
      "[1134/3000] 1 tree, 31 leaves, max depth = 14, train loss: 0.07375, val loss: 0.13014, in 0.236s\n",
      "[1135/3000] 1 tree, 31 leaves, max depth = 15, train loss: 0.07370, val loss: 0.13013, in 0.240s\n",
      "[1136/3000] 1 tree, 31 leaves, max depth = 12, train loss: 0.07364, val loss: 0.13014, in 0.327s\n",
      "[1137/3000] 1 tree, 31 leaves, max depth = 10, train loss: 0.07361, val loss: 0.13014, in 0.222s\n",
      "[1138/3000] 1 tree, 31 leaves, max depth = 17, train loss: 0.07355, val loss: 0.13013, in 0.248s\n",
      "[1139/3000] 1 tree, 31 leaves, max depth = 10, train loss: 0.07353, val loss: 0.13012, in 0.184s\n",
      "[1140/3000] 1 tree, 31 leaves, max depth = 8, train loss: 0.07350, val loss: 0.13012, in 0.172s\n",
      "[1141/3000] 1 tree, 31 leaves, max depth = 14, train loss: 0.07347, val loss: 0.13011, in 0.212s\n",
      "[1142/3000] 1 tree, 31 leaves, max depth = 11, train loss: 0.07344, val loss: 0.13011, in 0.215s\n",
      "[1143/3000] 1 tree, 31 leaves, max depth = 13, train loss: 0.07339, val loss: 0.13008, in 0.209s\n",
      "[1144/3000] 1 tree, 31 leaves, max depth = 16, train loss: 0.07336, val loss: 0.13010, in 0.186s\n",
      "[1145/3000] 1 tree, 31 leaves, max depth = 10, train loss: 0.07331, val loss: 0.13010, in 0.181s\n",
      "[1146/3000] 1 tree, 31 leaves, max depth = 13, train loss: 0.07326, val loss: 0.13009, in 0.190s\n",
      "[1147/3000] 1 tree, 31 leaves, max depth = 10, train loss: 0.07319, val loss: 0.13005, in 0.235s\n",
      "[1148/3000] 1 tree, 31 leaves, max depth = 9, train loss: 0.07314, val loss: 0.13003, in 0.240s\n",
      "[1149/3000] 1 tree, 31 leaves, max depth = 9, train loss: 0.07309, val loss: 0.13000, in 0.240s\n",
      "[1150/3000] 1 tree, 31 leaves, max depth = 13, train loss: 0.07305, val loss: 0.13000, in 0.223s\n",
      "[1151/3000] 1 tree, 31 leaves, max depth = 11, train loss: 0.07299, val loss: 0.12999, in 0.271s\n",
      "[1152/3000] 1 tree, 31 leaves, max depth = 11, train loss: 0.07295, val loss: 0.12997, in 0.167s\n",
      "[1153/3000] 1 tree, 31 leaves, max depth = 9, train loss: 0.07291, val loss: 0.12998, in 0.183s\n",
      "[1154/3000] 1 tree, 31 leaves, max depth = 11, train loss: 0.07285, val loss: 0.12997, in 0.226s\n",
      "[1155/3000] 1 tree, 31 leaves, max depth = 12, train loss: 0.07281, val loss: 0.12997, in 0.190s\n",
      "[1156/3000] 1 tree, 31 leaves, max depth = 10, train loss: 0.07276, val loss: 0.12998, in 0.230s\n",
      "[1157/3000] 1 tree, 31 leaves, max depth = 10, train loss: 0.07271, val loss: 0.13001, in 0.211s\n",
      "[1158/3000] 1 tree, 31 leaves, max depth = 14, train loss: 0.07266, val loss: 0.13001, in 0.352s\n",
      "[1159/3000] 1 tree, 31 leaves, max depth = 13, train loss: 0.07261, val loss: 0.13000, in 0.249s\n",
      "[1160/3000] 1 tree, 31 leaves, max depth = 11, train loss: 0.07256, val loss: 0.13000, in 0.263s\n",
      "[1161/3000] 1 tree, 31 leaves, max depth = 13, train loss: 0.07251, val loss: 0.13001, in 0.255s\n",
      "[1162/3000] 1 tree, 31 leaves, max depth = 15, train loss: 0.07248, val loss: 0.13000, in 0.183s\n",
      "[1163/3000] 1 tree, 31 leaves, max depth = 9, train loss: 0.07244, val loss: 0.12999, in 0.179s\n",
      "[1164/3000] 1 tree, 31 leaves, max depth = 10, train loss: 0.07242, val loss: 0.12998, in 0.195s\n",
      "[1165/3000] 1 tree, 31 leaves, max depth = 15, train loss: 0.07237, val loss: 0.12999, in 0.255s\n",
      "[1166/3000] 1 tree, 31 leaves, max depth = 9, train loss: 0.07231, val loss: 0.12998, in 0.229s\n",
      "[1167/3000] 1 tree, 31 leaves, max depth = 12, train loss: 0.07226, val loss: 0.12998, in 0.221s\n",
      "[1168/3000] 1 tree, 31 leaves, max depth = 14, train loss: 0.07221, val loss: 0.12999, in 0.192s\n",
      "[1169/3000] 1 tree, 31 leaves, max depth = 10, train loss: 0.07216, val loss: 0.12997, in 0.217s\n",
      "[1170/3000] 1 tree, 31 leaves, max depth = 10, train loss: 0.07213, val loss: 0.12995, in 0.217s\n",
      "[1171/3000] 1 tree, 31 leaves, max depth = 12, train loss: 0.07207, val loss: 0.12992, in 0.238s\n",
      "[1172/3000] 1 tree, 31 leaves, max depth = 11, train loss: 0.07201, val loss: 0.12988, in 0.199s\n",
      "[1173/3000] 1 tree, 31 leaves, max depth = 11, train loss: 0.07196, val loss: 0.12990, in 0.183s\n",
      "[1174/3000] 1 tree, 31 leaves, max depth = 9, train loss: 0.07194, val loss: 0.12990, in 0.190s\n",
      "[1175/3000] 1 tree, 31 leaves, max depth = 16, train loss: 0.07189, val loss: 0.12987, in 0.204s\n",
      "[1176/3000] 1 tree, 31 leaves, max depth = 11, train loss: 0.07184, val loss: 0.12986, in 0.229s\n",
      "[1177/3000] 1 tree, 31 leaves, max depth = 11, train loss: 0.07179, val loss: 0.12985, in 0.196s\n",
      "[1178/3000] 1 tree, 31 leaves, max depth = 12, train loss: 0.07172, val loss: 0.12980, in 0.189s\n",
      "[1179/3000] 1 tree, 31 leaves, max depth = 12, train loss: 0.07167, val loss: 0.12982, in 0.364s\n",
      "[1180/3000] 1 tree, 31 leaves, max depth = 10, train loss: 0.07162, val loss: 0.12979, in 0.220s\n",
      "[1181/3000] 1 tree, 31 leaves, max depth = 12, train loss: 0.07158, val loss: 0.12980, in 0.245s\n",
      "[1182/3000] 1 tree, 31 leaves, max depth = 10, train loss: 0.07152, val loss: 0.12977, in 0.187s\n",
      "[1183/3000] 1 tree, 31 leaves, max depth = 14, train loss: 0.07147, val loss: 0.12975, in 0.186s\n",
      "[1184/3000] 1 tree, 31 leaves, max depth = 12, train loss: 0.07143, val loss: 0.12974, in 0.207s\n",
      "[1185/3000] 1 tree, 31 leaves, max depth = 13, train loss: 0.07141, val loss: 0.12975, in 0.163s\n",
      "[1186/3000] 1 tree, 31 leaves, max depth = 11, train loss: 0.07138, val loss: 0.12974, in 0.185s\n",
      "[1187/3000] 1 tree, 31 leaves, max depth = 13, train loss: 0.07135, val loss: 0.12975, in 0.171s\n",
      "[1188/3000] 1 tree, 31 leaves, max depth = 11, train loss: 0.07132, val loss: 0.12974, in 0.204s\n",
      "[1189/3000] 1 tree, 31 leaves, max depth = 12, train loss: 0.07129, val loss: 0.12974, in 0.187s\n",
      "[1190/3000] 1 tree, 31 leaves, max depth = 14, train loss: 0.07124, val loss: 0.12973, in 0.211s\n",
      "[1191/3000] 1 tree, 31 leaves, max depth = 11, train loss: 0.07119, val loss: 0.12973, in 0.231s\n",
      "[1192/3000] 1 tree, 31 leaves, max depth = 10, train loss: 0.07116, val loss: 0.12971, in 0.164s\n",
      "[1193/3000] 1 tree, 31 leaves, max depth = 10, train loss: 0.07110, val loss: 0.12971, in 0.242s\n",
      "[1194/3000] 1 tree, 31 leaves, max depth = 14, train loss: 0.07106, val loss: 0.12970, in 0.178s\n",
      "[1195/3000] 1 tree, 31 leaves, max depth = 10, train loss: 0.07104, val loss: 0.12969, in 0.233s\n",
      "[1196/3000] 1 tree, 31 leaves, max depth = 14, train loss: 0.07101, val loss: 0.12969, in 0.199s\n",
      "[1197/3000] 1 tree, 31 leaves, max depth = 12, train loss: 0.07095, val loss: 0.12968, in 0.248s\n",
      "[1198/3000] 1 tree, 31 leaves, max depth = 13, train loss: 0.07091, val loss: 0.12967, in 0.267s\n",
      "[1199/3000] 1 tree, 31 leaves, max depth = 13, train loss: 0.07087, val loss: 0.12964, in 0.217s\n",
      "[1200/3000] 1 tree, 31 leaves, max depth = 12, train loss: 0.07082, val loss: 0.12962, in 0.366s\n",
      "[1201/3000] 1 tree, 31 leaves, max depth = 8, train loss: 0.07076, val loss: 0.12957, in 0.266s\n",
      "[1202/3000] 1 tree, 31 leaves, max depth = 11, train loss: 0.07071, val loss: 0.12959, in 0.216s\n",
      "[1203/3000] 1 tree, 31 leaves, max depth = 10, train loss: 0.07066, val loss: 0.12960, in 0.227s\n",
      "[1204/3000] 1 tree, 31 leaves, max depth = 14, train loss: 0.07064, val loss: 0.12960, in 0.184s\n",
      "[1205/3000] 1 tree, 31 leaves, max depth = 8, train loss: 0.07060, val loss: 0.12960, in 0.221s\n",
      "[1206/3000] 1 tree, 31 leaves, max depth = 13, train loss: 0.07054, val loss: 0.12960, in 0.224s\n",
      "[1207/3000] 1 tree, 31 leaves, max depth = 11, train loss: 0.07051, val loss: 0.12960, in 0.176s\n",
      "[1208/3000] 1 tree, 31 leaves, max depth = 13, train loss: 0.07047, val loss: 0.12960, in 0.187s\n",
      "[1209/3000] 1 tree, 31 leaves, max depth = 13, train loss: 0.07043, val loss: 0.12959, in 0.203s\n",
      "[1210/3000] 1 tree, 31 leaves, max depth = 12, train loss: 0.07038, val loss: 0.12960, in 0.234s\n",
      "[1211/3000] 1 tree, 31 leaves, max depth = 9, train loss: 0.07033, val loss: 0.12961, in 0.185s\n",
      "[1212/3000] 1 tree, 31 leaves, max depth = 12, train loss: 0.07028, val loss: 0.12961, in 0.211s\n",
      "[1213/3000] 1 tree, 31 leaves, max depth = 15, train loss: 0.07026, val loss: 0.12960, in 0.175s\n",
      "[1214/3000] 1 tree, 31 leaves, max depth = 9, train loss: 0.07023, val loss: 0.12958, in 0.165s\n",
      "[1215/3000] 1 tree, 31 leaves, max depth = 12, train loss: 0.07019, val loss: 0.12957, in 0.235s\n",
      "[1216/3000] 1 tree, 31 leaves, max depth = 13, train loss: 0.07013, val loss: 0.12956, in 0.201s\n",
      "[1217/3000] 1 tree, 31 leaves, max depth = 8, train loss: 0.07009, val loss: 0.12953, in 0.174s\n",
      "[1218/3000] 1 tree, 31 leaves, max depth = 10, train loss: 0.07004, val loss: 0.12951, in 0.212s\n",
      "[1219/3000] 1 tree, 31 leaves, max depth = 13, train loss: 0.07000, val loss: 0.12950, in 0.220s\n",
      "[1220/3000] 1 tree, 31 leaves, max depth = 10, train loss: 0.06997, val loss: 0.12948, in 0.155s\n",
      "[1221/3000] 1 tree, 31 leaves, max depth = 11, train loss: 0.06993, val loss: 0.12950, in 0.347s\n",
      "[1222/3000] 1 tree, 31 leaves, max depth = 11, train loss: 0.06989, val loss: 0.12950, in 0.215s\n",
      "[1223/3000] 1 tree, 31 leaves, max depth = 12, train loss: 0.06984, val loss: 0.12949, in 0.223s\n",
      "[1224/3000] 1 tree, 31 leaves, max depth = 16, train loss: 0.06979, val loss: 0.12948, in 0.217s\n",
      "[1225/3000] 1 tree, 31 leaves, max depth = 9, train loss: 0.06975, val loss: 0.12948, in 0.215s\n",
      "[1226/3000] 1 tree, 31 leaves, max depth = 12, train loss: 0.06971, val loss: 0.12949, in 0.213s\n",
      "[1227/3000] 1 tree, 31 leaves, max depth = 11, train loss: 0.06967, val loss: 0.12949, in 0.185s\n",
      "[1228/3000] 1 tree, 31 leaves, max depth = 13, train loss: 0.06962, val loss: 0.12950, in 0.212s\n",
      "[1229/3000] 1 tree, 31 leaves, max depth = 11, train loss: 0.06957, val loss: 0.12948, in 0.183s\n",
      "[1230/3000] 1 tree, 31 leaves, max depth = 10, train loss: 0.06953, val loss: 0.12948, in 0.223s\n",
      "[1231/3000] 1 tree, 31 leaves, max depth = 9, train loss: 0.06948, val loss: 0.12950, in 0.185s\n",
      "[1232/3000] 1 tree, 31 leaves, max depth = 13, train loss: 0.06945, val loss: 0.12951, in 0.166s\n",
      "[1233/3000] 1 tree, 31 leaves, max depth = 14, train loss: 0.06942, val loss: 0.12950, in 0.176s\n",
      "[1234/3000] 1 tree, 31 leaves, max depth = 10, train loss: 0.06937, val loss: 0.12949, in 0.213s\n",
      "[1235/3000] 1 tree, 31 leaves, max depth = 10, train loss: 0.06932, val loss: 0.12951, in 0.206s\n",
      "[1236/3000] 1 tree, 31 leaves, max depth = 11, train loss: 0.06927, val loss: 0.12950, in 0.235s\n",
      "[1237/3000] 1 tree, 31 leaves, max depth = 13, train loss: 0.06923, val loss: 0.12949, in 0.206s\n",
      "[1238/3000] 1 tree, 31 leaves, max depth = 10, train loss: 0.06919, val loss: 0.12949, in 0.207s\n",
      "[1239/3000] 1 tree, 31 leaves, max depth = 11, train loss: 0.06915, val loss: 0.12947, in 0.187s\n",
      "[1240/3000] 1 tree, 31 leaves, max depth = 13, train loss: 0.06911, val loss: 0.12947, in 0.225s\n",
      "[1241/3000] 1 tree, 31 leaves, max depth = 11, train loss: 0.06907, val loss: 0.12948, in 0.174s\n",
      "[1242/3000] 1 tree, 31 leaves, max depth = 14, train loss: 0.06903, val loss: 0.12944, in 0.304s\n",
      "[1243/3000] 1 tree, 31 leaves, max depth = 10, train loss: 0.06899, val loss: 0.12942, in 0.194s\n",
      "[1244/3000] 1 tree, 31 leaves, max depth = 13, train loss: 0.06896, val loss: 0.12942, in 0.202s\n",
      "[1245/3000] 1 tree, 31 leaves, max depth = 16, train loss: 0.06890, val loss: 0.12940, in 0.229s\n",
      "[1246/3000] 1 tree, 31 leaves, max depth = 15, train loss: 0.06886, val loss: 0.12940, in 0.172s\n",
      "[1247/3000] 1 tree, 31 leaves, max depth = 13, train loss: 0.06883, val loss: 0.12941, in 0.193s\n",
      "[1248/3000] 1 tree, 31 leaves, max depth = 11, train loss: 0.06880, val loss: 0.12940, in 0.195s\n",
      "[1249/3000] 1 tree, 31 leaves, max depth = 11, train loss: 0.06875, val loss: 0.12939, in 0.238s\n",
      "[1250/3000] 1 tree, 31 leaves, max depth = 9, train loss: 0.06874, val loss: 0.12939, in 0.188s\n",
      "[1251/3000] 1 tree, 31 leaves, max depth = 11, train loss: 0.06869, val loss: 0.12942, in 0.206s\n",
      "[1252/3000] 1 tree, 31 leaves, max depth = 10, train loss: 0.06865, val loss: 0.12944, in 0.217s\n",
      "[1253/3000] 1 tree, 31 leaves, max depth = 9, train loss: 0.06861, val loss: 0.12942, in 0.171s\n",
      "[1254/3000] 1 tree, 31 leaves, max depth = 10, train loss: 0.06856, val loss: 0.12937, in 0.218s\n",
      "[1255/3000] 1 tree, 31 leaves, max depth = 15, train loss: 0.06850, val loss: 0.12934, in 0.216s\n",
      "[1256/3000] 1 tree, 31 leaves, max depth = 10, train loss: 0.06847, val loss: 0.12931, in 0.190s\n",
      "[1257/3000] 1 tree, 31 leaves, max depth = 10, train loss: 0.06844, val loss: 0.12931, in 0.188s\n",
      "[1258/3000] 1 tree, 31 leaves, max depth = 15, train loss: 0.06840, val loss: 0.12930, in 0.188s\n",
      "[1259/3000] 1 tree, 31 leaves, max depth = 14, train loss: 0.06835, val loss: 0.12930, in 0.218s\n",
      "[1260/3000] 1 tree, 31 leaves, max depth = 9, train loss: 0.06830, val loss: 0.12927, in 0.209s\n",
      "[1261/3000] 1 tree, 31 leaves, max depth = 11, train loss: 0.06825, val loss: 0.12927, in 0.189s\n",
      "[1262/3000] 1 tree, 31 leaves, max depth = 10, train loss: 0.06822, val loss: 0.12927, in 0.355s\n",
      "[1263/3000] 1 tree, 31 leaves, max depth = 10, train loss: 0.06817, val loss: 0.12924, in 0.254s\n",
      "[1264/3000] 1 tree, 31 leaves, max depth = 10, train loss: 0.06813, val loss: 0.12923, in 0.232s\n",
      "[1265/3000] 1 tree, 31 leaves, max depth = 15, train loss: 0.06809, val loss: 0.12924, in 0.197s\n",
      "[1266/3000] 1 tree, 31 leaves, max depth = 10, train loss: 0.06806, val loss: 0.12923, in 0.206s\n",
      "[1267/3000] 1 tree, 31 leaves, max depth = 11, train loss: 0.06803, val loss: 0.12921, in 0.188s\n",
      "[1268/3000] 1 tree, 31 leaves, max depth = 13, train loss: 0.06800, val loss: 0.12924, in 0.173s\n",
      "[1269/3000] 1 tree, 31 leaves, max depth = 11, train loss: 0.06796, val loss: 0.12923, in 0.237s\n",
      "[1270/3000] 1 tree, 31 leaves, max depth = 15, train loss: 0.06791, val loss: 0.12924, in 0.179s\n",
      "[1271/3000] 1 tree, 31 leaves, max depth = 15, train loss: 0.06787, val loss: 0.12924, in 0.216s\n",
      "[1272/3000] 1 tree, 31 leaves, max depth = 13, train loss: 0.06785, val loss: 0.12924, in 0.177s\n",
      "[1273/3000] 1 tree, 31 leaves, max depth = 11, train loss: 0.06780, val loss: 0.12923, in 0.261s\n",
      "[1274/3000] 1 tree, 31 leaves, max depth = 14, train loss: 0.06777, val loss: 0.12926, in 0.177s\n",
      "[1275/3000] 1 tree, 31 leaves, max depth = 11, train loss: 0.06772, val loss: 0.12926, in 0.243s\n",
      "[1276/3000] 1 tree, 31 leaves, max depth = 10, train loss: 0.06767, val loss: 0.12924, in 0.218s\n",
      "[1277/3000] 1 tree, 31 leaves, max depth = 17, train loss: 0.06762, val loss: 0.12925, in 0.219s\n",
      "[1278/3000] 1 tree, 31 leaves, max depth = 12, train loss: 0.06761, val loss: 0.12924, in 0.156s\n",
      "[1279/3000] 1 tree, 31 leaves, max depth = 11, train loss: 0.06756, val loss: 0.12925, in 0.209s\n",
      "[1280/3000] 1 tree, 31 leaves, max depth = 11, train loss: 0.06752, val loss: 0.12926, in 0.165s\n",
      "[1281/3000] 1 tree, 31 leaves, max depth = 15, train loss: 0.06749, val loss: 0.12924, in 0.179s\n",
      "[1282/3000] 1 tree, 31 leaves, max depth = 11, train loss: 0.06745, val loss: 0.12923, in 0.198s\n",
      "[1283/3000] 1 tree, 31 leaves, max depth = 13, train loss: 0.06740, val loss: 0.12922, in 0.348s\n",
      "[1284/3000] 1 tree, 31 leaves, max depth = 16, train loss: 0.06736, val loss: 0.12921, in 0.231s\n",
      "[1285/3000] 1 tree, 31 leaves, max depth = 13, train loss: 0.06731, val loss: 0.12922, in 0.254s\n",
      "[1286/3000] 1 tree, 31 leaves, max depth = 12, train loss: 0.06728, val loss: 0.12922, in 0.180s\n",
      "[1287/3000] 1 tree, 31 leaves, max depth = 12, train loss: 0.06723, val loss: 0.12920, in 0.217s\n",
      "[1288/3000] 1 tree, 31 leaves, max depth = 10, train loss: 0.06718, val loss: 0.12914, in 0.209s\n",
      "[1289/3000] 1 tree, 31 leaves, max depth = 13, train loss: 0.06714, val loss: 0.12915, in 0.202s\n",
      "[1290/3000] 1 tree, 31 leaves, max depth = 13, train loss: 0.06709, val loss: 0.12914, in 0.235s\n",
      "[1291/3000] 1 tree, 31 leaves, max depth = 10, train loss: 0.06706, val loss: 0.12912, in 0.192s\n",
      "[1292/3000] 1 tree, 31 leaves, max depth = 12, train loss: 0.06701, val loss: 0.12914, in 0.238s\n",
      "[1293/3000] 1 tree, 31 leaves, max depth = 13, train loss: 0.06695, val loss: 0.12909, in 0.176s\n",
      "[1294/3000] 1 tree, 31 leaves, max depth = 14, train loss: 0.06690, val loss: 0.12907, in 0.192s\n",
      "[1295/3000] 1 tree, 31 leaves, max depth = 11, train loss: 0.06688, val loss: 0.12907, in 0.175s\n",
      "[1296/3000] 1 tree, 31 leaves, max depth = 11, train loss: 0.06685, val loss: 0.12907, in 0.181s\n",
      "[1297/3000] 1 tree, 31 leaves, max depth = 11, train loss: 0.06682, val loss: 0.12908, in 0.180s\n",
      "[1298/3000] 1 tree, 31 leaves, max depth = 11, train loss: 0.06677, val loss: 0.12910, in 0.199s\n",
      "[1299/3000] 1 tree, 31 leaves, max depth = 12, train loss: 0.06674, val loss: 0.12910, in 0.180s\n",
      "[1300/3000] 1 tree, 31 leaves, max depth = 10, train loss: 0.06669, val loss: 0.12907, in 0.199s\n",
      "[1301/3000] 1 tree, 31 leaves, max depth = 13, train loss: 0.06665, val loss: 0.12907, in 0.212s\n",
      "[1302/3000] 1 tree, 31 leaves, max depth = 13, train loss: 0.06660, val loss: 0.12905, in 0.200s\n",
      "[1303/3000] 1 tree, 31 leaves, max depth = 10, train loss: 0.06656, val loss: 0.12906, in 0.219s\n",
      "[1304/3000] 1 tree, 31 leaves, max depth = 14, train loss: 0.06652, val loss: 0.12907, in 0.324s\n",
      "[1305/3000] 1 tree, 31 leaves, max depth = 8, train loss: 0.06650, val loss: 0.12907, in 0.167s\n",
      "[1306/3000] 1 tree, 31 leaves, max depth = 10, train loss: 0.06648, val loss: 0.12907, in 0.215s\n",
      "[1307/3000] 1 tree, 31 leaves, max depth = 10, train loss: 0.06643, val loss: 0.12904, in 0.242s\n",
      "[1308/3000] 1 tree, 31 leaves, max depth = 13, train loss: 0.06639, val loss: 0.12904, in 0.206s\n",
      "[1309/3000] 1 tree, 31 leaves, max depth = 12, train loss: 0.06630, val loss: 0.12899, in 0.235s\n",
      "[1310/3000] 1 tree, 31 leaves, max depth = 9, train loss: 0.06626, val loss: 0.12900, in 0.212s\n",
      "[1311/3000] 1 tree, 31 leaves, max depth = 10, train loss: 0.06621, val loss: 0.12899, in 0.209s\n",
      "[1312/3000] 1 tree, 31 leaves, max depth = 12, train loss: 0.06616, val loss: 0.12897, in 0.197s\n",
      "[1313/3000] 1 tree, 31 leaves, max depth = 11, train loss: 0.06611, val loss: 0.12897, in 0.204s\n",
      "[1314/3000] 1 tree, 31 leaves, max depth = 10, train loss: 0.06607, val loss: 0.12898, in 0.185s\n",
      "[1315/3000] 1 tree, 31 leaves, max depth = 16, train loss: 0.06604, val loss: 0.12899, in 0.174s\n",
      "[1316/3000] 1 tree, 31 leaves, max depth = 12, train loss: 0.06598, val loss: 0.12896, in 0.251s\n",
      "[1317/3000] 1 tree, 31 leaves, max depth = 11, train loss: 0.06595, val loss: 0.12893, in 0.208s\n",
      "[1318/3000] 1 tree, 31 leaves, max depth = 9, train loss: 0.06589, val loss: 0.12890, in 0.232s\n",
      "[1319/3000] 1 tree, 31 leaves, max depth = 13, train loss: 0.06580, val loss: 0.12882, in 0.246s\n",
      "[1320/3000] 1 tree, 31 leaves, max depth = 13, train loss: 0.06576, val loss: 0.12882, in 0.197s\n",
      "[1321/3000] 1 tree, 31 leaves, max depth = 11, train loss: 0.06571, val loss: 0.12880, in 0.225s\n",
      "[1322/3000] 1 tree, 31 leaves, max depth = 14, train loss: 0.06567, val loss: 0.12879, in 0.171s\n",
      "[1323/3000] 1 tree, 31 leaves, max depth = 9, train loss: 0.06562, val loss: 0.12878, in 0.238s\n",
      "[1324/3000] 1 tree, 31 leaves, max depth = 12, train loss: 0.06555, val loss: 0.12875, in 0.345s\n",
      "[1325/3000] 1 tree, 31 leaves, max depth = 13, train loss: 0.06552, val loss: 0.12873, in 0.190s\n",
      "[1326/3000] 1 tree, 31 leaves, max depth = 16, train loss: 0.06549, val loss: 0.12874, in 0.210s\n",
      "[1327/3000] 1 tree, 31 leaves, max depth = 17, train loss: 0.06545, val loss: 0.12873, in 0.224s\n",
      "[1328/3000] 1 tree, 31 leaves, max depth = 17, train loss: 0.06541, val loss: 0.12874, in 0.186s\n",
      "[1329/3000] 1 tree, 31 leaves, max depth = 13, train loss: 0.06536, val loss: 0.12873, in 0.232s\n",
      "[1330/3000] 1 tree, 31 leaves, max depth = 11, train loss: 0.06534, val loss: 0.12871, in 0.200s\n",
      "[1331/3000] 1 tree, 31 leaves, max depth = 13, train loss: 0.06531, val loss: 0.12871, in 0.202s\n",
      "[1332/3000] 1 tree, 31 leaves, max depth = 12, train loss: 0.06527, val loss: 0.12868, in 0.206s\n",
      "[1333/3000] 1 tree, 31 leaves, max depth = 13, train loss: 0.06522, val loss: 0.12866, in 0.265s\n",
      "[1334/3000] 1 tree, 31 leaves, max depth = 15, train loss: 0.06518, val loss: 0.12865, in 0.157s\n",
      "[1335/3000] 1 tree, 31 leaves, max depth = 12, train loss: 0.06513, val loss: 0.12865, in 0.258s\n",
      "[1336/3000] 1 tree, 31 leaves, max depth = 12, train loss: 0.06507, val loss: 0.12860, in 0.208s\n",
      "[1337/3000] 1 tree, 31 leaves, max depth = 12, train loss: 0.06502, val loss: 0.12863, in 0.201s\n",
      "[1338/3000] 1 tree, 31 leaves, max depth = 13, train loss: 0.06497, val loss: 0.12864, in 0.208s\n",
      "[1339/3000] 1 tree, 31 leaves, max depth = 13, train loss: 0.06493, val loss: 0.12865, in 0.195s\n",
      "[1340/3000] 1 tree, 31 leaves, max depth = 12, train loss: 0.06490, val loss: 0.12865, in 0.208s\n",
      "[1341/3000] 1 tree, 31 leaves, max depth = 10, train loss: 0.06486, val loss: 0.12866, in 0.203s\n",
      "[1342/3000] 1 tree, 31 leaves, max depth = 11, train loss: 0.06482, val loss: 0.12865, in 0.222s\n",
      "[1343/3000] 1 tree, 31 leaves, max depth = 12, train loss: 0.06480, val loss: 0.12866, in 0.173s\n",
      "[1344/3000] 1 tree, 31 leaves, max depth = 9, train loss: 0.06476, val loss: 0.12866, in 0.199s\n",
      "[1345/3000] 1 tree, 31 leaves, max depth = 15, train loss: 0.06472, val loss: 0.12866, in 0.303s\n",
      "[1346/3000] 1 tree, 31 leaves, max depth = 12, train loss: 0.06469, val loss: 0.12866, in 0.213s\n",
      "[1347/3000] 1 tree, 31 leaves, max depth = 8, train loss: 0.06467, val loss: 0.12865, in 0.214s\n",
      "[1348/3000] 1 tree, 31 leaves, max depth = 11, train loss: 0.06465, val loss: 0.12865, in 0.226s\n",
      "[1349/3000] 1 tree, 31 leaves, max depth = 14, train loss: 0.06462, val loss: 0.12866, in 0.168s\n",
      "[1350/3000] 1 tree, 31 leaves, max depth = 12, train loss: 0.06458, val loss: 0.12865, in 0.248s\n",
      "[1351/3000] 1 tree, 31 leaves, max depth = 15, train loss: 0.06453, val loss: 0.12864, in 0.222s\n",
      "[1352/3000] 1 tree, 31 leaves, max depth = 13, train loss: 0.06449, val loss: 0.12862, in 0.199s\n",
      "[1353/3000] 1 tree, 31 leaves, max depth = 11, train loss: 0.06444, val loss: 0.12861, in 0.215s\n",
      "[1354/3000] 1 tree, 31 leaves, max depth = 15, train loss: 0.06439, val loss: 0.12859, in 0.214s\n",
      "[1355/3000] 1 tree, 31 leaves, max depth = 12, train loss: 0.06435, val loss: 0.12860, in 0.208s\n",
      "[1356/3000] 1 tree, 31 leaves, max depth = 11, train loss: 0.06431, val loss: 0.12860, in 0.223s\n",
      "[1357/3000] 1 tree, 31 leaves, max depth = 12, train loss: 0.06427, val loss: 0.12860, in 0.196s\n",
      "[1358/3000] 1 tree, 31 leaves, max depth = 14, train loss: 0.06423, val loss: 0.12858, in 0.171s\n",
      "[1359/3000] 1 tree, 31 leaves, max depth = 12, train loss: 0.06420, val loss: 0.12855, in 0.191s\n",
      "[1360/3000] 1 tree, 31 leaves, max depth = 9, train loss: 0.06415, val loss: 0.12851, in 0.224s\n",
      "[1361/3000] 1 tree, 31 leaves, max depth = 14, train loss: 0.06411, val loss: 0.12852, in 0.173s\n",
      "[1362/3000] 1 tree, 31 leaves, max depth = 13, train loss: 0.06408, val loss: 0.12852, in 0.176s\n",
      "[1363/3000] 1 tree, 31 leaves, max depth = 9, train loss: 0.06403, val loss: 0.12849, in 0.192s\n",
      "[1364/3000] 1 tree, 31 leaves, max depth = 10, train loss: 0.06401, val loss: 0.12849, in 0.176s\n",
      "[1365/3000] 1 tree, 31 leaves, max depth = 15, train loss: 0.06396, val loss: 0.12849, in 0.327s\n",
      "[1366/3000] 1 tree, 31 leaves, max depth = 13, train loss: 0.06393, val loss: 0.12847, in 0.230s\n",
      "[1367/3000] 1 tree, 31 leaves, max depth = 13, train loss: 0.06386, val loss: 0.12841, in 0.240s\n",
      "[1368/3000] 1 tree, 31 leaves, max depth = 11, train loss: 0.06384, val loss: 0.12842, in 0.229s\n",
      "[1369/3000] 1 tree, 31 leaves, max depth = 9, train loss: 0.06380, val loss: 0.12839, in 0.214s\n",
      "[1370/3000] 1 tree, 31 leaves, max depth = 14, train loss: 0.06378, val loss: 0.12837, in 0.242s\n",
      "[1371/3000] 1 tree, 31 leaves, max depth = 11, train loss: 0.06373, val loss: 0.12836, in 0.195s\n",
      "[1372/3000] 1 tree, 31 leaves, max depth = 14, train loss: 0.06370, val loss: 0.12837, in 0.173s\n",
      "[1373/3000] 1 tree, 31 leaves, max depth = 10, train loss: 0.06366, val loss: 0.12838, in 0.221s\n",
      "[1374/3000] 1 tree, 31 leaves, max depth = 9, train loss: 0.06363, val loss: 0.12837, in 0.154s\n",
      "[1375/3000] 1 tree, 31 leaves, max depth = 10, train loss: 0.06361, val loss: 0.12836, in 0.171s\n",
      "[1376/3000] 1 tree, 31 leaves, max depth = 10, train loss: 0.06356, val loss: 0.12837, in 0.206s\n",
      "[1377/3000] 1 tree, 31 leaves, max depth = 10, train loss: 0.06352, val loss: 0.12838, in 0.202s\n",
      "[1378/3000] 1 tree, 31 leaves, max depth = 9, train loss: 0.06346, val loss: 0.12835, in 0.239s\n",
      "[1379/3000] 1 tree, 31 leaves, max depth = 10, train loss: 0.06342, val loss: 0.12836, in 0.228s\n",
      "[1380/3000] 1 tree, 31 leaves, max depth = 13, train loss: 0.06339, val loss: 0.12834, in 0.182s\n",
      "[1381/3000] 1 tree, 31 leaves, max depth = 16, train loss: 0.06335, val loss: 0.12834, in 0.209s\n",
      "[1382/3000] 1 tree, 31 leaves, max depth = 12, train loss: 0.06331, val loss: 0.12834, in 0.242s\n",
      "[1383/3000] 1 tree, 31 leaves, max depth = 10, train loss: 0.06327, val loss: 0.12834, in 0.228s\n",
      "[1384/3000] 1 tree, 31 leaves, max depth = 10, train loss: 0.06324, val loss: 0.12833, in 0.225s\n",
      "[1385/3000] 1 tree, 31 leaves, max depth = 12, train loss: 0.06319, val loss: 0.12829, in 0.365s\n",
      "[1386/3000] 1 tree, 31 leaves, max depth = 14, train loss: 0.06316, val loss: 0.12829, in 0.213s\n",
      "[1387/3000] 1 tree, 31 leaves, max depth = 10, train loss: 0.06312, val loss: 0.12828, in 0.561s\n",
      "[1388/3000] 1 tree, 31 leaves, max depth = 10, train loss: 0.06308, val loss: 0.12829, in 0.273s\n",
      "[1389/3000] 1 tree, 31 leaves, max depth = 12, train loss: 0.06304, val loss: 0.12828, in 0.279s\n",
      "[1390/3000] 1 tree, 31 leaves, max depth = 13, train loss: 0.06302, val loss: 0.12829, in 0.219s\n",
      "[1391/3000] 1 tree, 31 leaves, max depth = 12, train loss: 0.06300, val loss: 0.12829, in 0.203s\n",
      "[1392/3000] 1 tree, 31 leaves, max depth = 10, train loss: 0.06296, val loss: 0.12827, in 0.254s\n",
      "[1393/3000] 1 tree, 31 leaves, max depth = 12, train loss: 0.06293, val loss: 0.12828, in 0.163s\n",
      "[1394/3000] 1 tree, 31 leaves, max depth = 15, train loss: 0.06291, val loss: 0.12828, in 0.188s\n",
      "[1395/3000] 1 tree, 31 leaves, max depth = 12, train loss: 0.06286, val loss: 0.12830, in 0.244s\n",
      "[1396/3000] 1 tree, 31 leaves, max depth = 11, train loss: 0.06282, val loss: 0.12829, in 0.259s\n",
      "[1397/3000] 1 tree, 31 leaves, max depth = 15, train loss: 0.06278, val loss: 0.12829, in 0.183s\n",
      "[1398/3000] 1 tree, 31 leaves, max depth = 9, train loss: 0.06274, val loss: 0.12828, in 0.252s\n",
      "[1399/3000] 1 tree, 31 leaves, max depth = 9, train loss: 0.06272, val loss: 0.12828, in 0.171s\n",
      "[1400/3000] 1 tree, 31 leaves, max depth = 10, train loss: 0.06267, val loss: 0.12827, in 0.223s\n",
      "[1401/3000] 1 tree, 31 leaves, max depth = 12, train loss: 0.06261, val loss: 0.12823, in 0.200s\n",
      "[1402/3000] 1 tree, 31 leaves, max depth = 9, train loss: 0.06259, val loss: 0.12822, in 0.210s\n",
      "[1403/3000] 1 tree, 31 leaves, max depth = 11, train loss: 0.06253, val loss: 0.12818, in 0.276s\n",
      "[1404/3000] 1 tree, 31 leaves, max depth = 11, train loss: 0.06249, val loss: 0.12818, in 0.178s\n",
      "[1405/3000] 1 tree, 31 leaves, max depth = 13, train loss: 0.06246, val loss: 0.12817, in 0.200s\n",
      "[1406/3000] 1 tree, 31 leaves, max depth = 11, train loss: 0.06244, val loss: 0.12818, in 0.185s\n",
      "[1407/3000] 1 tree, 31 leaves, max depth = 10, train loss: 0.06238, val loss: 0.12816, in 0.213s\n",
      "[1408/3000] 1 tree, 31 leaves, max depth = 11, train loss: 0.06236, val loss: 0.12816, in 0.364s\n",
      "[1409/3000] 1 tree, 31 leaves, max depth = 10, train loss: 0.06233, val loss: 0.12816, in 0.238s\n",
      "[1410/3000] 1 tree, 31 leaves, max depth = 12, train loss: 0.06227, val loss: 0.12813, in 0.207s\n",
      "[1411/3000] 1 tree, 31 leaves, max depth = 10, train loss: 0.06224, val loss: 0.12814, in 0.199s\n",
      "[1412/3000] 1 tree, 31 leaves, max depth = 13, train loss: 0.06221, val loss: 0.12815, in 0.209s\n",
      "[1413/3000] 1 tree, 31 leaves, max depth = 14, train loss: 0.06218, val loss: 0.12814, in 0.208s\n",
      "[1414/3000] 1 tree, 31 leaves, max depth = 14, train loss: 0.06215, val loss: 0.12815, in 0.173s\n",
      "[1415/3000] 1 tree, 31 leaves, max depth = 12, train loss: 0.06211, val loss: 0.12815, in 0.208s\n",
      "[1416/3000] 1 tree, 31 leaves, max depth = 10, train loss: 0.06207, val loss: 0.12815, in 0.179s\n",
      "[1417/3000] 1 tree, 31 leaves, max depth = 10, train loss: 0.06203, val loss: 0.12816, in 0.261s\n",
      "[1418/3000] 1 tree, 31 leaves, max depth = 11, train loss: 0.06198, val loss: 0.12815, in 0.209s\n",
      "[1419/3000] 1 tree, 31 leaves, max depth = 10, train loss: 0.06194, val loss: 0.12814, in 0.209s\n",
      "[1420/3000] 1 tree, 31 leaves, max depth = 11, train loss: 0.06189, val loss: 0.12810, in 0.256s\n",
      "[1421/3000] 1 tree, 31 leaves, max depth = 11, train loss: 0.06183, val loss: 0.12808, in 0.283s\n",
      "[1422/3000] 1 tree, 31 leaves, max depth = 12, train loss: 0.06178, val loss: 0.12808, in 0.230s\n",
      "[1423/3000] 1 tree, 31 leaves, max depth = 9, train loss: 0.06174, val loss: 0.12806, in 0.258s\n",
      "[1424/3000] 1 tree, 31 leaves, max depth = 13, train loss: 0.06170, val loss: 0.12807, in 0.202s\n",
      "[1425/3000] 1 tree, 31 leaves, max depth = 12, train loss: 0.06168, val loss: 0.12806, in 0.190s\n",
      "[1426/3000] 1 tree, 31 leaves, max depth = 8, train loss: 0.06166, val loss: 0.12806, in 0.180s\n",
      "[1427/3000] 1 tree, 31 leaves, max depth = 8, train loss: 0.06164, val loss: 0.12806, in 0.214s\n",
      "[1428/3000] 1 tree, 31 leaves, max depth = 14, train loss: 0.06159, val loss: 0.12805, in 0.432s\n",
      "[1429/3000] 1 tree, 31 leaves, max depth = 12, train loss: 0.06155, val loss: 0.12802, in 0.235s\n",
      "[1430/3000] 1 tree, 31 leaves, max depth = 10, train loss: 0.06151, val loss: 0.12805, in 0.252s\n",
      "[1431/3000] 1 tree, 31 leaves, max depth = 10, train loss: 0.06148, val loss: 0.12805, in 0.213s\n",
      "[1432/3000] 1 tree, 31 leaves, max depth = 9, train loss: 0.06145, val loss: 0.12803, in 0.251s\n",
      "[1433/3000] 1 tree, 31 leaves, max depth = 12, train loss: 0.06141, val loss: 0.12802, in 0.303s\n",
      "[1434/3000] 1 tree, 31 leaves, max depth = 13, train loss: 0.06138, val loss: 0.12804, in 0.270s\n",
      "[1435/3000] 1 tree, 31 leaves, max depth = 13, train loss: 0.06136, val loss: 0.12805, in 0.195s\n",
      "[1436/3000] 1 tree, 31 leaves, max depth = 14, train loss: 0.06134, val loss: 0.12805, in 0.179s\n",
      "[1437/3000] 1 tree, 31 leaves, max depth = 14, train loss: 0.06131, val loss: 0.12805, in 0.206s\n",
      "[1438/3000] 1 tree, 31 leaves, max depth = 11, train loss: 0.06125, val loss: 0.12800, in 0.282s\n",
      "[1439/3000] 1 tree, 31 leaves, max depth = 11, train loss: 0.06122, val loss: 0.12796, in 0.263s\n",
      "[1440/3000] 1 tree, 31 leaves, max depth = 10, train loss: 0.06119, val loss: 0.12795, in 0.217s\n",
      "[1441/3000] 1 tree, 31 leaves, max depth = 8, train loss: 0.06116, val loss: 0.12794, in 0.197s\n",
      "[1442/3000] 1 tree, 31 leaves, max depth = 12, train loss: 0.06113, val loss: 0.12795, in 0.202s\n",
      "[1443/3000] 1 tree, 31 leaves, max depth = 14, train loss: 0.06110, val loss: 0.12794, in 0.225s\n",
      "[1444/3000] 1 tree, 31 leaves, max depth = 8, train loss: 0.06106, val loss: 0.12795, in 0.207s\n",
      "[1445/3000] 1 tree, 31 leaves, max depth = 12, train loss: 0.06102, val loss: 0.12795, in 0.194s\n",
      "[1446/3000] 1 tree, 31 leaves, max depth = 14, train loss: 0.06098, val loss: 0.12792, in 0.255s\n",
      "[1447/3000] 1 tree, 31 leaves, max depth = 12, train loss: 0.06093, val loss: 0.12791, in 0.237s\n",
      "[1448/3000] 1 tree, 31 leaves, max depth = 14, train loss: 0.06088, val loss: 0.12790, in 0.214s\n",
      "[1449/3000] 1 tree, 31 leaves, max depth = 11, train loss: 0.06080, val loss: 0.12780, in 0.438s\n",
      "[1450/3000] 1 tree, 31 leaves, max depth = 15, train loss: 0.06076, val loss: 0.12777, in 0.259s\n",
      "[1451/3000] 1 tree, 31 leaves, max depth = 11, train loss: 0.06072, val loss: 0.12777, in 0.239s\n",
      "[1452/3000] 1 tree, 31 leaves, max depth = 11, train loss: 0.06068, val loss: 0.12777, in 0.241s\n",
      "[1453/3000] 1 tree, 31 leaves, max depth = 13, train loss: 0.06064, val loss: 0.12777, in 0.219s\n",
      "[1454/3000] 1 tree, 31 leaves, max depth = 8, train loss: 0.06060, val loss: 0.12775, in 0.240s\n",
      "[1455/3000] 1 tree, 31 leaves, max depth = 10, train loss: 0.06056, val loss: 0.12775, in 0.247s\n",
      "[1456/3000] 1 tree, 31 leaves, max depth = 11, train loss: 0.06053, val loss: 0.12773, in 0.244s\n",
      "[1457/3000] 1 tree, 31 leaves, max depth = 14, train loss: 0.06049, val loss: 0.12773, in 0.227s\n",
      "[1458/3000] 1 tree, 31 leaves, max depth = 13, train loss: 0.06045, val loss: 0.12772, in 0.219s\n",
      "[1459/3000] 1 tree, 31 leaves, max depth = 10, train loss: 0.06043, val loss: 0.12772, in 0.185s\n",
      "[1460/3000] 1 tree, 31 leaves, max depth = 12, train loss: 0.06039, val loss: 0.12770, in 0.168s\n",
      "[1461/3000] 1 tree, 31 leaves, max depth = 13, train loss: 0.06034, val loss: 0.12767, in 0.204s\n",
      "[1462/3000] 1 tree, 31 leaves, max depth = 11, train loss: 0.06030, val loss: 0.12767, in 0.177s\n",
      "[1463/3000] 1 tree, 31 leaves, max depth = 14, train loss: 0.06026, val loss: 0.12767, in 0.200s\n",
      "[1464/3000] 1 tree, 31 leaves, max depth = 8, train loss: 0.06023, val loss: 0.12767, in 0.205s\n",
      "[1465/3000] 1 tree, 31 leaves, max depth = 14, train loss: 0.06019, val loss: 0.12768, in 0.197s\n",
      "[1466/3000] 1 tree, 31 leaves, max depth = 13, train loss: 0.06015, val loss: 0.12766, in 0.215s\n",
      "[1467/3000] 1 tree, 31 leaves, max depth = 10, train loss: 0.06011, val loss: 0.12765, in 0.184s\n",
      "[1468/3000] 1 tree, 31 leaves, max depth = 9, train loss: 0.06007, val loss: 0.12763, in 0.235s\n",
      "[1469/3000] 1 tree, 31 leaves, max depth = 10, train loss: 0.06004, val loss: 0.12762, in 0.165s\n",
      "[1470/3000] 1 tree, 31 leaves, max depth = 8, train loss: 0.06000, val loss: 0.12760, in 0.366s\n",
      "[1471/3000] 1 tree, 31 leaves, max depth = 13, train loss: 0.05996, val loss: 0.12758, in 0.182s\n",
      "[1472/3000] 1 tree, 31 leaves, max depth = 10, train loss: 0.05993, val loss: 0.12757, in 0.177s\n",
      "[1473/3000] 1 tree, 31 leaves, max depth = 14, train loss: 0.05991, val loss: 0.12757, in 0.250s\n",
      "[1474/3000] 1 tree, 31 leaves, max depth = 12, train loss: 0.05987, val loss: 0.12756, in 0.232s\n",
      "[1475/3000] 1 tree, 31 leaves, max depth = 11, train loss: 0.05982, val loss: 0.12752, in 0.231s\n",
      "[1476/3000] 1 tree, 31 leaves, max depth = 11, train loss: 0.05977, val loss: 0.12750, in 0.224s\n",
      "[1477/3000] 1 tree, 31 leaves, max depth = 14, train loss: 0.05973, val loss: 0.12751, in 0.180s\n",
      "[1478/3000] 1 tree, 31 leaves, max depth = 10, train loss: 0.05971, val loss: 0.12749, in 0.177s\n",
      "[1479/3000] 1 tree, 31 leaves, max depth = 9, train loss: 0.05968, val loss: 0.12750, in 0.164s\n",
      "[1480/3000] 1 tree, 31 leaves, max depth = 10, train loss: 0.05964, val loss: 0.12751, in 0.206s\n",
      "[1481/3000] 1 tree, 31 leaves, max depth = 13, train loss: 0.05959, val loss: 0.12749, in 0.216s\n",
      "[1482/3000] 1 tree, 31 leaves, max depth = 11, train loss: 0.05955, val loss: 0.12748, in 0.228s\n",
      "[1483/3000] 1 tree, 31 leaves, max depth = 12, train loss: 0.05950, val loss: 0.12750, in 0.192s\n",
      "[1484/3000] 1 tree, 31 leaves, max depth = 11, train loss: 0.05947, val loss: 0.12749, in 0.235s\n",
      "[1485/3000] 1 tree, 31 leaves, max depth = 14, train loss: 0.05945, val loss: 0.12747, in 0.214s\n",
      "[1486/3000] 1 tree, 31 leaves, max depth = 12, train loss: 0.05941, val loss: 0.12747, in 0.169s\n",
      "[1487/3000] 1 tree, 31 leaves, max depth = 12, train loss: 0.05940, val loss: 0.12747, in 0.177s\n",
      "[1488/3000] 1 tree, 31 leaves, max depth = 9, train loss: 0.05936, val loss: 0.12745, in 0.279s\n",
      "[1489/3000] 1 tree, 31 leaves, max depth = 8, train loss: 0.05932, val loss: 0.12745, in 0.257s\n",
      "[1490/3000] 1 tree, 31 leaves, max depth = 12, train loss: 0.05927, val loss: 0.12745, in 0.389s\n",
      "[1491/3000] 1 tree, 31 leaves, max depth = 10, train loss: 0.05922, val loss: 0.12742, in 0.315s\n",
      "[1492/3000] 1 tree, 31 leaves, max depth = 12, train loss: 0.05917, val loss: 0.12741, in 0.251s\n",
      "[1493/3000] 1 tree, 31 leaves, max depth = 11, train loss: 0.05914, val loss: 0.12739, in 0.187s\n",
      "[1494/3000] 1 tree, 31 leaves, max depth = 13, train loss: 0.05913, val loss: 0.12741, in 0.196s\n",
      "[1495/3000] 1 tree, 31 leaves, max depth = 16, train loss: 0.05910, val loss: 0.12741, in 0.175s\n",
      "[1496/3000] 1 tree, 31 leaves, max depth = 11, train loss: 0.05908, val loss: 0.12741, in 0.170s\n",
      "[1497/3000] 1 tree, 31 leaves, max depth = 15, train loss: 0.05904, val loss: 0.12740, in 0.212s\n",
      "[1498/3000] 1 tree, 31 leaves, max depth = 12, train loss: 0.05901, val loss: 0.12740, in 0.201s\n",
      "[1499/3000] 1 tree, 31 leaves, max depth = 10, train loss: 0.05897, val loss: 0.12739, in 0.203s\n",
      "[1500/3000] 1 tree, 31 leaves, max depth = 12, train loss: 0.05895, val loss: 0.12739, in 0.164s\n",
      "[1501/3000] 1 tree, 31 leaves, max depth = 9, train loss: 0.05891, val loss: 0.12737, in 0.218s\n",
      "[1502/3000] 1 tree, 31 leaves, max depth = 14, train loss: 0.05886, val loss: 0.12736, in 0.179s\n",
      "[1503/3000] 1 tree, 31 leaves, max depth = 8, train loss: 0.05883, val loss: 0.12737, in 0.177s\n",
      "[1504/3000] 1 tree, 31 leaves, max depth = 11, train loss: 0.05880, val loss: 0.12735, in 0.175s\n",
      "[1505/3000] 1 tree, 31 leaves, max depth = 11, train loss: 0.05877, val loss: 0.12736, in 0.172s\n",
      "[1506/3000] 1 tree, 31 leaves, max depth = 11, train loss: 0.05873, val loss: 0.12731, in 0.238s\n",
      "[1507/3000] 1 tree, 31 leaves, max depth = 12, train loss: 0.05868, val loss: 0.12731, in 0.215s\n",
      "[1508/3000] 1 tree, 31 leaves, max depth = 12, train loss: 0.05864, val loss: 0.12728, in 0.210s\n",
      "[1509/3000] 1 tree, 31 leaves, max depth = 9, train loss: 0.05860, val loss: 0.12728, in 0.207s\n",
      "[1510/3000] 1 tree, 31 leaves, max depth = 12, train loss: 0.05857, val loss: 0.12728, in 0.194s\n",
      "[1511/3000] 1 tree, 31 leaves, max depth = 11, train loss: 0.05853, val loss: 0.12725, in 0.321s\n",
      "[1512/3000] 1 tree, 31 leaves, max depth = 14, train loss: 0.05850, val loss: 0.12728, in 0.230s\n",
      "[1513/3000] 1 tree, 31 leaves, max depth = 12, train loss: 0.05847, val loss: 0.12728, in 0.206s\n",
      "[1514/3000] 1 tree, 31 leaves, max depth = 9, train loss: 0.05844, val loss: 0.12730, in 0.223s\n",
      "[1515/3000] 1 tree, 31 leaves, max depth = 11, train loss: 0.05842, val loss: 0.12730, in 0.177s\n",
      "[1516/3000] 1 tree, 31 leaves, max depth = 12, train loss: 0.05839, val loss: 0.12728, in 0.169s\n",
      "[1517/3000] 1 tree, 31 leaves, max depth = 13, train loss: 0.05836, val loss: 0.12727, in 0.250s\n",
      "[1518/3000] 1 tree, 31 leaves, max depth = 8, train loss: 0.05833, val loss: 0.12727, in 0.252s\n",
      "[1519/3000] 1 tree, 31 leaves, max depth = 12, train loss: 0.05829, val loss: 0.12724, in 0.217s\n",
      "[1520/3000] 1 tree, 31 leaves, max depth = 13, train loss: 0.05825, val loss: 0.12721, in 0.229s\n",
      "[1521/3000] 1 tree, 31 leaves, max depth = 9, train loss: 0.05820, val loss: 0.12721, in 0.184s\n",
      "[1522/3000] 1 tree, 31 leaves, max depth = 9, train loss: 0.05816, val loss: 0.12722, in 0.241s\n",
      "[1523/3000] 1 tree, 31 leaves, max depth = 11, train loss: 0.05814, val loss: 0.12722, in 0.160s\n",
      "[1524/3000] 1 tree, 31 leaves, max depth = 11, train loss: 0.05811, val loss: 0.12723, in 0.213s\n",
      "[1525/3000] 1 tree, 31 leaves, max depth = 9, train loss: 0.05806, val loss: 0.12721, in 0.224s\n",
      "[1526/3000] 1 tree, 31 leaves, max depth = 11, train loss: 0.05802, val loss: 0.12720, in 0.201s\n",
      "[1527/3000] 1 tree, 31 leaves, max depth = 10, train loss: 0.05799, val loss: 0.12721, in 0.187s\n",
      "[1528/3000] 1 tree, 31 leaves, max depth = 12, train loss: 0.05796, val loss: 0.12720, in 0.208s\n",
      "[1529/3000] 1 tree, 31 leaves, max depth = 9, train loss: 0.05792, val loss: 0.12717, in 0.196s\n",
      "[1530/3000] 1 tree, 31 leaves, max depth = 12, train loss: 0.05789, val loss: 0.12716, in 0.171s\n",
      "[1531/3000] 1 tree, 31 leaves, max depth = 14, train loss: 0.05787, val loss: 0.12717, in 0.161s\n",
      "[1532/3000] 1 tree, 31 leaves, max depth = 11, train loss: 0.05784, val loss: 0.12717, in 0.321s\n",
      "[1533/3000] 1 tree, 31 leaves, max depth = 12, train loss: 0.05781, val loss: 0.12717, in 0.199s\n",
      "[1534/3000] 1 tree, 31 leaves, max depth = 12, train loss: 0.05778, val loss: 0.12719, in 0.207s\n",
      "[1535/3000] 1 tree, 31 leaves, max depth = 9, train loss: 0.05775, val loss: 0.12718, in 0.172s\n",
      "[1536/3000] 1 tree, 31 leaves, max depth = 9, train loss: 0.05771, val loss: 0.12719, in 0.188s\n",
      "[1537/3000] 1 tree, 31 leaves, max depth = 10, train loss: 0.05769, val loss: 0.12719, in 0.190s\n",
      "[1538/3000] 1 tree, 31 leaves, max depth = 10, train loss: 0.05766, val loss: 0.12717, in 0.234s\n",
      "[1539/3000] 1 tree, 31 leaves, max depth = 13, train loss: 0.05761, val loss: 0.12718, in 0.244s\n",
      "[1540/3000] 1 tree, 31 leaves, max depth = 12, train loss: 0.05759, val loss: 0.12719, in 0.172s\n",
      "[1541/3000] 1 tree, 31 leaves, max depth = 10, train loss: 0.05755, val loss: 0.12720, in 0.195s\n",
      "[1542/3000] 1 tree, 31 leaves, max depth = 15, train loss: 0.05752, val loss: 0.12721, in 0.204s\n",
      "[1543/3000] 1 tree, 31 leaves, max depth = 13, train loss: 0.05748, val loss: 0.12722, in 0.230s\n",
      "[1544/3000] 1 tree, 31 leaves, max depth = 17, train loss: 0.05744, val loss: 0.12721, in 0.230s\n",
      "[1545/3000] 1 tree, 31 leaves, max depth = 8, train loss: 0.05740, val loss: 0.12724, in 0.232s\n",
      "[1546/3000] 1 tree, 31 leaves, max depth = 10, train loss: 0.05737, val loss: 0.12725, in 0.155s\n",
      "[1547/3000] 1 tree, 31 leaves, max depth = 12, train loss: 0.05734, val loss: 0.12723, in 0.173s\n",
      "[1548/3000] 1 tree, 31 leaves, max depth = 11, train loss: 0.05732, val loss: 0.12723, in 0.183s\n",
      "[1549/3000] 1 tree, 31 leaves, max depth = 10, train loss: 0.05728, val loss: 0.12721, in 0.250s\n",
      "[1550/3000] 1 tree, 31 leaves, max depth = 12, train loss: 0.05724, val loss: 0.12720, in 0.182s\n",
      "[1551/3000] 1 tree, 31 leaves, max depth = 7, train loss: 0.05722, val loss: 0.12719, in 0.209s\n",
      "[1552/3000] 1 tree, 31 leaves, max depth = 13, train loss: 0.05718, val loss: 0.12718, in 0.196s\n",
      "[1553/3000] 1 tree, 31 leaves, max depth = 14, train loss: 0.05714, val loss: 0.12719, in 0.204s\n",
      "[1554/3000] 1 tree, 31 leaves, max depth = 10, train loss: 0.05710, val loss: 0.12717, in 0.286s\n",
      "[1555/3000] 1 tree, 31 leaves, max depth = 17, train loss: 0.05707, val loss: 0.12714, in 0.226s\n",
      "[1556/3000] 1 tree, 31 leaves, max depth = 11, train loss: 0.05704, val loss: 0.12714, in 0.193s\n",
      "[1557/3000] 1 tree, 31 leaves, max depth = 10, train loss: 0.05701, val loss: 0.12713, in 0.219s\n",
      "[1558/3000] 1 tree, 31 leaves, max depth = 13, train loss: 0.05697, val loss: 0.12714, in 0.237s\n",
      "[1559/3000] 1 tree, 31 leaves, max depth = 10, train loss: 0.05693, val loss: 0.12715, in 0.174s\n",
      "[1560/3000] 1 tree, 31 leaves, max depth = 13, train loss: 0.05691, val loss: 0.12716, in 0.154s\n",
      "[1561/3000] 1 tree, 31 leaves, max depth = 9, train loss: 0.05687, val loss: 0.12715, in 0.224s\n",
      "[1562/3000] 1 tree, 31 leaves, max depth = 15, train loss: 0.05684, val loss: 0.12715, in 0.171s\n",
      "[1563/3000] 1 tree, 31 leaves, max depth = 14, train loss: 0.05682, val loss: 0.12715, in 0.230s\n",
      "[1564/3000] 1 tree, 31 leaves, max depth = 11, train loss: 0.05678, val loss: 0.12713, in 0.223s\n",
      "[1565/3000] 1 tree, 31 leaves, max depth = 13, train loss: 0.05675, val loss: 0.12712, in 0.222s\n",
      "[1566/3000] 1 tree, 31 leaves, max depth = 13, train loss: 0.05673, val loss: 0.12710, in 0.232s\n",
      "[1567/3000] 1 tree, 31 leaves, max depth = 12, train loss: 0.05671, val loss: 0.12710, in 0.163s\n",
      "[1568/3000] 1 tree, 31 leaves, max depth = 13, train loss: 0.05666, val loss: 0.12709, in 0.201s\n",
      "[1569/3000] 1 tree, 31 leaves, max depth = 10, train loss: 0.05665, val loss: 0.12708, in 0.198s\n",
      "[1570/3000] 1 tree, 31 leaves, max depth = 10, train loss: 0.05662, val loss: 0.12708, in 0.191s\n",
      "[1571/3000] 1 tree, 31 leaves, max depth = 14, train loss: 0.05659, val loss: 0.12709, in 0.213s\n",
      "[1572/3000] 1 tree, 31 leaves, max depth = 11, train loss: 0.05656, val loss: 0.12709, in 0.167s\n",
      "[1573/3000] 1 tree, 31 leaves, max depth = 12, train loss: 0.05653, val loss: 0.12708, in 0.188s\n",
      "[1574/3000] 1 tree, 31 leaves, max depth = 15, train loss: 0.05650, val loss: 0.12707, in 0.377s\n",
      "[1575/3000] 1 tree, 31 leaves, max depth = 11, train loss: 0.05647, val loss: 0.12706, in 0.190s\n",
      "[1576/3000] 1 tree, 31 leaves, max depth = 12, train loss: 0.05644, val loss: 0.12705, in 0.206s\n",
      "[1577/3000] 1 tree, 31 leaves, max depth = 12, train loss: 0.05642, val loss: 0.12705, in 0.221s\n",
      "[1578/3000] 1 tree, 31 leaves, max depth = 13, train loss: 0.05638, val loss: 0.12703, in 0.195s\n",
      "[1579/3000] 1 tree, 31 leaves, max depth = 12, train loss: 0.05635, val loss: 0.12703, in 0.196s\n",
      "[1580/3000] 1 tree, 31 leaves, max depth = 15, train loss: 0.05631, val loss: 0.12704, in 0.203s\n",
      "[1581/3000] 1 tree, 31 leaves, max depth = 11, train loss: 0.05628, val loss: 0.12702, in 0.207s\n",
      "[1582/3000] 1 tree, 31 leaves, max depth = 10, train loss: 0.05624, val loss: 0.12702, in 0.178s\n",
      "[1583/3000] 1 tree, 31 leaves, max depth = 14, train loss: 0.05621, val loss: 0.12703, in 0.194s\n",
      "[1584/3000] 1 tree, 31 leaves, max depth = 11, train loss: 0.05617, val loss: 0.12702, in 0.193s\n",
      "[1585/3000] 1 tree, 31 leaves, max depth = 9, train loss: 0.05614, val loss: 0.12702, in 0.212s\n",
      "[1586/3000] 1 tree, 31 leaves, max depth = 11, train loss: 0.05611, val loss: 0.12701, in 0.201s\n",
      "[1587/3000] 1 tree, 31 leaves, max depth = 12, train loss: 0.05606, val loss: 0.12699, in 0.253s\n",
      "[1588/3000] 1 tree, 31 leaves, max depth = 11, train loss: 0.05603, val loss: 0.12698, in 0.205s\n",
      "[1589/3000] 1 tree, 31 leaves, max depth = 10, train loss: 0.05599, val loss: 0.12697, in 0.232s\n",
      "[1590/3000] 1 tree, 31 leaves, max depth = 11, train loss: 0.05595, val loss: 0.12696, in 0.205s\n",
      "[1591/3000] 1 tree, 31 leaves, max depth = 10, train loss: 0.05594, val loss: 0.12695, in 0.169s\n",
      "[1592/3000] 1 tree, 31 leaves, max depth = 9, train loss: 0.05590, val loss: 0.12696, in 0.201s\n",
      "[1593/3000] 1 tree, 31 leaves, max depth = 11, train loss: 0.05587, val loss: 0.12698, in 0.196s\n",
      "[1594/3000] 1 tree, 31 leaves, max depth = 14, train loss: 0.05583, val loss: 0.12698, in 0.190s\n",
      "[1595/3000] 1 tree, 31 leaves, max depth = 11, train loss: 0.05582, val loss: 0.12700, in 0.317s\n",
      "[1596/3000] 1 tree, 31 leaves, max depth = 8, train loss: 0.05579, val loss: 0.12700, in 0.203s\n",
      "[1597/3000] 1 tree, 31 leaves, max depth = 10, train loss: 0.05577, val loss: 0.12699, in 0.212s\n",
      "[1598/3000] 1 tree, 31 leaves, max depth = 13, train loss: 0.05575, val loss: 0.12699, in 0.186s\n",
      "[1599/3000] 1 tree, 31 leaves, max depth = 11, train loss: 0.05570, val loss: 0.12695, in 0.188s\n",
      "[1600/3000] 1 tree, 31 leaves, max depth = 13, train loss: 0.05566, val loss: 0.12696, in 0.195s\n",
      "[1601/3000] 1 tree, 31 leaves, max depth = 10, train loss: 0.05562, val loss: 0.12694, in 0.226s\n",
      "[1602/3000] 1 tree, 31 leaves, max depth = 11, train loss: 0.05560, val loss: 0.12695, in 0.163s\n",
      "[1603/3000] 1 tree, 31 leaves, max depth = 15, train loss: 0.05558, val loss: 0.12692, in 0.166s\n",
      "[1604/3000] 1 tree, 31 leaves, max depth = 13, train loss: 0.05554, val loss: 0.12692, in 0.177s\n",
      "[1605/3000] 1 tree, 31 leaves, max depth = 10, train loss: 0.05550, val loss: 0.12692, in 0.194s\n",
      "[1606/3000] 1 tree, 31 leaves, max depth = 13, train loss: 0.05547, val loss: 0.12690, in 0.277s\n",
      "[1607/3000] 1 tree, 31 leaves, max depth = 10, train loss: 0.05544, val loss: 0.12692, in 0.225s\n",
      "[1608/3000] 1 tree, 31 leaves, max depth = 13, train loss: 0.05540, val loss: 0.12692, in 0.264s\n",
      "[1609/3000] 1 tree, 31 leaves, max depth = 9, train loss: 0.05537, val loss: 0.12691, in 0.235s\n",
      "[1610/3000] 1 tree, 31 leaves, max depth = 11, train loss: 0.05533, val loss: 0.12688, in 0.207s\n",
      "[1611/3000] 1 tree, 31 leaves, max depth = 13, train loss: 0.05528, val loss: 0.12687, in 0.210s\n",
      "[1612/3000] 1 tree, 31 leaves, max depth = 13, train loss: 0.05526, val loss: 0.12689, in 0.163s\n",
      "[1613/3000] 1 tree, 31 leaves, max depth = 11, train loss: 0.05523, val loss: 0.12689, in 0.216s\n",
      "[1614/3000] 1 tree, 31 leaves, max depth = 15, train loss: 0.05519, val loss: 0.12688, in 0.229s\n",
      "[1615/3000] 1 tree, 31 leaves, max depth = 14, train loss: 0.05515, val loss: 0.12689, in 0.225s\n",
      "[1616/3000] 1 tree, 31 leaves, max depth = 11, train loss: 0.05511, val loss: 0.12687, in 0.313s\n",
      "[1617/3000] 1 tree, 31 leaves, max depth = 10, train loss: 0.05506, val loss: 0.12686, in 0.262s\n",
      "[1618/3000] 1 tree, 31 leaves, max depth = 15, train loss: 0.05504, val loss: 0.12684, in 0.186s\n",
      "[1619/3000] 1 tree, 31 leaves, max depth = 11, train loss: 0.05501, val loss: 0.12683, in 0.171s\n",
      "[1620/3000] 1 tree, 31 leaves, max depth = 12, train loss: 0.05497, val loss: 0.12683, in 0.193s\n",
      "[1621/3000] 1 tree, 31 leaves, max depth = 13, train loss: 0.05494, val loss: 0.12682, in 0.166s\n",
      "[1622/3000] 1 tree, 31 leaves, max depth = 17, train loss: 0.05490, val loss: 0.12681, in 0.200s\n",
      "[1623/3000] 1 tree, 31 leaves, max depth = 8, train loss: 0.05486, val loss: 0.12681, in 0.185s\n",
      "[1624/3000] 1 tree, 31 leaves, max depth = 10, train loss: 0.05482, val loss: 0.12679, in 0.196s\n",
      "[1625/3000] 1 tree, 31 leaves, max depth = 10, train loss: 0.05480, val loss: 0.12679, in 0.178s\n",
      "[1626/3000] 1 tree, 31 leaves, max depth = 11, train loss: 0.05476, val loss: 0.12675, in 0.198s\n",
      "[1627/3000] 1 tree, 31 leaves, max depth = 11, train loss: 0.05470, val loss: 0.12669, in 0.226s\n",
      "[1628/3000] 1 tree, 31 leaves, max depth = 9, train loss: 0.05467, val loss: 0.12672, in 0.179s\n",
      "[1629/3000] 1 tree, 31 leaves, max depth = 13, train loss: 0.05463, val loss: 0.12670, in 0.210s\n",
      "[1630/3000] 1 tree, 31 leaves, max depth = 11, train loss: 0.05460, val loss: 0.12668, in 0.172s\n",
      "[1631/3000] 1 tree, 31 leaves, max depth = 13, train loss: 0.05458, val loss: 0.12667, in 0.173s\n",
      "[1632/3000] 1 tree, 31 leaves, max depth = 14, train loss: 0.05455, val loss: 0.12666, in 0.168s\n",
      "[1633/3000] 1 tree, 31 leaves, max depth = 11, train loss: 0.05451, val loss: 0.12663, in 0.215s\n",
      "[1634/3000] 1 tree, 31 leaves, max depth = 11, train loss: 0.05448, val loss: 0.12662, in 0.211s\n",
      "[1635/3000] 1 tree, 31 leaves, max depth = 9, train loss: 0.05444, val loss: 0.12660, in 0.190s\n",
      "[1636/3000] 1 tree, 31 leaves, max depth = 11, train loss: 0.05439, val loss: 0.12657, in 0.222s\n",
      "[1637/3000] 1 tree, 31 leaves, max depth = 13, train loss: 0.05435, val loss: 0.12658, in 0.327s\n",
      "[1638/3000] 1 tree, 31 leaves, max depth = 12, train loss: 0.05434, val loss: 0.12656, in 0.205s\n",
      "[1639/3000] 1 tree, 31 leaves, max depth = 13, train loss: 0.05431, val loss: 0.12656, in 0.235s\n",
      "[1640/3000] 1 tree, 31 leaves, max depth = 13, train loss: 0.05427, val loss: 0.12657, in 0.230s\n",
      "[1641/3000] 1 tree, 31 leaves, max depth = 12, train loss: 0.05424, val loss: 0.12656, in 0.231s\n",
      "[1642/3000] 1 tree, 31 leaves, max depth = 12, train loss: 0.05420, val loss: 0.12657, in 0.259s\n",
      "[1643/3000] 1 tree, 31 leaves, max depth = 13, train loss: 0.05418, val loss: 0.12656, in 0.242s\n",
      "[1644/3000] 1 tree, 31 leaves, max depth = 14, train loss: 0.05415, val loss: 0.12656, in 0.255s\n",
      "[1645/3000] 1 tree, 31 leaves, max depth = 14, train loss: 0.05412, val loss: 0.12654, in 0.240s\n",
      "[1646/3000] 1 tree, 31 leaves, max depth = 13, train loss: 0.05408, val loss: 0.12655, in 0.237s\n",
      "[1647/3000] 1 tree, 31 leaves, max depth = 15, train loss: 0.05406, val loss: 0.12654, in 0.190s\n",
      "[1648/3000] 1 tree, 31 leaves, max depth = 15, train loss: 0.05403, val loss: 0.12654, in 0.221s\n",
      "[1649/3000] 1 tree, 31 leaves, max depth = 12, train loss: 0.05400, val loss: 0.12655, in 0.212s\n",
      "[1650/3000] 1 tree, 31 leaves, max depth = 10, train loss: 0.05397, val loss: 0.12654, in 0.172s\n",
      "[1651/3000] 1 tree, 31 leaves, max depth = 12, train loss: 0.05395, val loss: 0.12653, in 0.210s\n",
      "[1652/3000] 1 tree, 31 leaves, max depth = 14, train loss: 0.05393, val loss: 0.12654, in 0.189s\n",
      "[1653/3000] 1 tree, 31 leaves, max depth = 9, train loss: 0.05390, val loss: 0.12655, in 0.214s\n",
      "[1654/3000] 1 tree, 31 leaves, max depth = 13, train loss: 0.05387, val loss: 0.12654, in 0.205s\n",
      "[1655/3000] 1 tree, 31 leaves, max depth = 10, train loss: 0.05385, val loss: 0.12655, in 0.186s\n",
      "[1656/3000] 1 tree, 31 leaves, max depth = 12, train loss: 0.05381, val loss: 0.12655, in 0.223s\n",
      "[1657/3000] 1 tree, 31 leaves, max depth = 11, train loss: 0.05378, val loss: 0.12654, in 0.232s\n",
      "[1658/3000] 1 tree, 31 leaves, max depth = 14, train loss: 0.05377, val loss: 0.12653, in 0.356s\n",
      "[1659/3000] 1 tree, 31 leaves, max depth = 10, train loss: 0.05373, val loss: 0.12652, in 0.265s\n",
      "[1660/3000] 1 tree, 31 leaves, max depth = 11, train loss: 0.05370, val loss: 0.12654, in 0.226s\n",
      "[1661/3000] 1 tree, 31 leaves, max depth = 10, train loss: 0.05365, val loss: 0.12654, in 0.220s\n",
      "[1662/3000] 1 tree, 31 leaves, max depth = 10, train loss: 0.05363, val loss: 0.12656, in 0.170s\n",
      "[1663/3000] 1 tree, 31 leaves, max depth = 12, train loss: 0.05361, val loss: 0.12657, in 0.172s\n",
      "[1664/3000] 1 tree, 31 leaves, max depth = 11, train loss: 0.05357, val loss: 0.12654, in 0.194s\n",
      "[1665/3000] 1 tree, 31 leaves, max depth = 14, train loss: 0.05354, val loss: 0.12654, in 0.168s\n",
      "[1666/3000] 1 tree, 31 leaves, max depth = 8, train loss: 0.05351, val loss: 0.12655, in 0.176s\n",
      "[1667/3000] 1 tree, 31 leaves, max depth = 12, train loss: 0.05348, val loss: 0.12652, in 0.202s\n",
      "[1668/3000] 1 tree, 31 leaves, max depth = 12, train loss: 0.05345, val loss: 0.12650, in 0.222s\n",
      "[1669/3000] 1 tree, 31 leaves, max depth = 13, train loss: 0.05342, val loss: 0.12650, in 0.207s\n",
      "[1670/3000] 1 tree, 31 leaves, max depth = 9, train loss: 0.05339, val loss: 0.12650, in 0.219s\n",
      "[1671/3000] 1 tree, 31 leaves, max depth = 11, train loss: 0.05337, val loss: 0.12650, in 0.187s\n",
      "[1672/3000] 1 tree, 31 leaves, max depth = 14, train loss: 0.05332, val loss: 0.12646, in 0.233s\n",
      "[1673/3000] 1 tree, 31 leaves, max depth = 11, train loss: 0.05330, val loss: 0.12645, in 0.203s\n",
      "[1674/3000] 1 tree, 31 leaves, max depth = 10, train loss: 0.05329, val loss: 0.12647, in 0.161s\n",
      "[1675/3000] 1 tree, 31 leaves, max depth = 11, train loss: 0.05326, val loss: 0.12648, in 0.223s\n",
      "[1676/3000] 1 tree, 31 leaves, max depth = 14, train loss: 0.05322, val loss: 0.12646, in 0.220s\n",
      "[1677/3000] 1 tree, 31 leaves, max depth = 11, train loss: 0.05321, val loss: 0.12646, in 0.196s\n",
      "[1678/3000] 1 tree, 31 leaves, max depth = 13, train loss: 0.05317, val loss: 0.12644, in 0.363s\n",
      "[1679/3000] 1 tree, 31 leaves, max depth = 12, train loss: 0.05315, val loss: 0.12643, in 0.197s\n",
      "[1680/3000] 1 tree, 31 leaves, max depth = 12, train loss: 0.05312, val loss: 0.12642, in 0.311s\n",
      "[1681/3000] 1 tree, 31 leaves, max depth = 10, train loss: 0.05311, val loss: 0.12642, in 0.207s\n",
      "[1682/3000] 1 tree, 31 leaves, max depth = 12, train loss: 0.05307, val loss: 0.12644, in 0.214s\n",
      "[1683/3000] 1 tree, 31 leaves, max depth = 12, train loss: 0.05303, val loss: 0.12643, in 0.219s\n",
      "[1684/3000] 1 tree, 31 leaves, max depth = 10, train loss: 0.05300, val loss: 0.12645, in 0.177s\n",
      "[1685/3000] 1 tree, 31 leaves, max depth = 10, train loss: 0.05296, val loss: 0.12642, in 0.200s\n",
      "[1686/3000] 1 tree, 31 leaves, max depth = 11, train loss: 0.05288, val loss: 0.12635, in 0.305s\n",
      "[1687/3000] 1 tree, 31 leaves, max depth = 12, train loss: 0.05285, val loss: 0.12635, in 0.192s\n",
      "[1688/3000] 1 tree, 31 leaves, max depth = 8, train loss: 0.05282, val loss: 0.12634, in 0.188s\n",
      "[1689/3000] 1 tree, 31 leaves, max depth = 10, train loss: 0.05278, val loss: 0.12632, in 0.201s\n",
      "[1690/3000] 1 tree, 31 leaves, max depth = 11, train loss: 0.05275, val loss: 0.12632, in 0.184s\n",
      "[1691/3000] 1 tree, 31 leaves, max depth = 10, train loss: 0.05272, val loss: 0.12630, in 0.190s\n",
      "[1692/3000] 1 tree, 31 leaves, max depth = 12, train loss: 0.05269, val loss: 0.12631, in 0.221s\n",
      "[1693/3000] 1 tree, 31 leaves, max depth = 17, train loss: 0.05267, val loss: 0.12631, in 0.180s\n",
      "[1694/3000] 1 tree, 31 leaves, max depth = 10, train loss: 0.05263, val loss: 0.12629, in 0.189s\n",
      "[1695/3000] 1 tree, 31 leaves, max depth = 10, train loss: 0.05261, val loss: 0.12629, in 0.170s\n",
      "[1696/3000] 1 tree, 31 leaves, max depth = 14, train loss: 0.05259, val loss: 0.12628, in 0.169s\n",
      "[1697/3000] 1 tree, 31 leaves, max depth = 11, train loss: 0.05255, val loss: 0.12626, in 0.242s\n",
      "[1698/3000] 1 tree, 31 leaves, max depth = 13, train loss: 0.05250, val loss: 0.12621, in 0.237s\n",
      "[1699/3000] 1 tree, 31 leaves, max depth = 10, train loss: 0.05247, val loss: 0.12623, in 0.186s\n",
      "[1700/3000] 1 tree, 31 leaves, max depth = 14, train loss: 0.05244, val loss: 0.12624, in 0.320s\n",
      "[1701/3000] 1 tree, 31 leaves, max depth = 11, train loss: 0.05242, val loss: 0.12624, in 0.223s\n",
      "[1702/3000] 1 tree, 31 leaves, max depth = 10, train loss: 0.05239, val loss: 0.12623, in 0.231s\n",
      "[1703/3000] 1 tree, 31 leaves, max depth = 12, train loss: 0.05237, val loss: 0.12623, in 0.167s\n",
      "[1704/3000] 1 tree, 31 leaves, max depth = 12, train loss: 0.05233, val loss: 0.12625, in 0.212s\n",
      "[1705/3000] 1 tree, 31 leaves, max depth = 12, train loss: 0.05229, val loss: 0.12626, in 0.254s\n",
      "[1706/3000] 1 tree, 31 leaves, max depth = 10, train loss: 0.05226, val loss: 0.12627, in 0.179s\n",
      "[1707/3000] 1 tree, 31 leaves, max depth = 14, train loss: 0.05222, val loss: 0.12628, in 0.227s\n",
      "[1708/3000] 1 tree, 31 leaves, max depth = 8, train loss: 0.05219, val loss: 0.12627, in 0.201s\n",
      "[1709/3000] 1 tree, 31 leaves, max depth = 13, train loss: 0.05216, val loss: 0.12626, in 0.211s\n",
      "[1710/3000] 1 tree, 31 leaves, max depth = 10, train loss: 0.05213, val loss: 0.12625, in 0.181s\n",
      "[1711/3000] 1 tree, 31 leaves, max depth = 12, train loss: 0.05210, val loss: 0.12625, in 0.180s\n",
      "[1712/3000] 1 tree, 31 leaves, max depth = 12, train loss: 0.05204, val loss: 0.12619, in 0.257s\n",
      "[1713/3000] 1 tree, 31 leaves, max depth = 12, train loss: 0.05201, val loss: 0.12618, in 0.208s\n",
      "[1714/3000] 1 tree, 31 leaves, max depth = 12, train loss: 0.05198, val loss: 0.12617, in 0.199s\n",
      "[1715/3000] 1 tree, 31 leaves, max depth = 13, train loss: 0.05194, val loss: 0.12616, in 0.238s\n",
      "[1716/3000] 1 tree, 31 leaves, max depth = 13, train loss: 0.05193, val loss: 0.12615, in 0.194s\n",
      "[1717/3000] 1 tree, 31 leaves, max depth = 13, train loss: 0.05189, val loss: 0.12615, in 0.228s\n",
      "[1718/3000] 1 tree, 31 leaves, max depth = 11, train loss: 0.05186, val loss: 0.12615, in 0.247s\n",
      "[1719/3000] 1 tree, 31 leaves, max depth = 16, train loss: 0.05183, val loss: 0.12613, in 0.255s\n",
      "[1720/3000] 1 tree, 31 leaves, max depth = 15, train loss: 0.05180, val loss: 0.12610, in 0.209s\n",
      "[1721/3000] 1 tree, 31 leaves, max depth = 10, train loss: 0.05176, val loss: 0.12609, in 0.350s\n",
      "[1722/3000] 1 tree, 31 leaves, max depth = 12, train loss: 0.05172, val loss: 0.12610, in 0.230s\n",
      "[1723/3000] 1 tree, 31 leaves, max depth = 13, train loss: 0.05170, val loss: 0.12609, in 0.246s\n",
      "[1724/3000] 1 tree, 31 leaves, max depth = 9, train loss: 0.05167, val loss: 0.12610, in 0.186s\n",
      "[1725/3000] 1 tree, 31 leaves, max depth = 9, train loss: 0.05164, val loss: 0.12611, in 0.241s\n",
      "[1726/3000] 1 tree, 31 leaves, max depth = 11, train loss: 0.05160, val loss: 0.12611, in 0.208s\n",
      "[1727/3000] 1 tree, 31 leaves, max depth = 13, train loss: 0.05157, val loss: 0.12611, in 0.202s\n",
      "[1728/3000] 1 tree, 31 leaves, max depth = 11, train loss: 0.05154, val loss: 0.12611, in 0.209s\n",
      "[1729/3000] 1 tree, 31 leaves, max depth = 14, train loss: 0.05151, val loss: 0.12612, in 0.194s\n",
      "[1730/3000] 1 tree, 31 leaves, max depth = 14, train loss: 0.05148, val loss: 0.12612, in 0.174s\n",
      "[1731/3000] 1 tree, 31 leaves, max depth = 11, train loss: 0.05145, val loss: 0.12611, in 0.205s\n",
      "[1732/3000] 1 tree, 31 leaves, max depth = 10, train loss: 0.05143, val loss: 0.12611, in 0.153s\n",
      "[1733/3000] 1 tree, 31 leaves, max depth = 9, train loss: 0.05138, val loss: 0.12606, in 0.249s\n",
      "[1734/3000] 1 tree, 31 leaves, max depth = 11, train loss: 0.05133, val loss: 0.12604, in 0.230s\n",
      "[1735/3000] 1 tree, 31 leaves, max depth = 12, train loss: 0.05130, val loss: 0.12605, in 0.205s\n",
      "[1736/3000] 1 tree, 31 leaves, max depth = 9, train loss: 0.05128, val loss: 0.12604, in 0.189s\n",
      "[1737/3000] 1 tree, 31 leaves, max depth = 14, train loss: 0.05125, val loss: 0.12607, in 0.205s\n",
      "[1738/3000] 1 tree, 31 leaves, max depth = 9, train loss: 0.05121, val loss: 0.12606, in 0.223s\n",
      "[1739/3000] 1 tree, 31 leaves, max depth = 11, train loss: 0.05118, val loss: 0.12607, in 0.232s\n",
      "[1740/3000] 1 tree, 31 leaves, max depth = 11, train loss: 0.05117, val loss: 0.12605, in 0.162s\n",
      "[1741/3000] 1 tree, 31 leaves, max depth = 11, train loss: 0.05115, val loss: 0.12604, in 0.300s\n",
      "[1742/3000] 1 tree, 31 leaves, max depth = 11, train loss: 0.05113, val loss: 0.12604, in 0.205s\n",
      "[1743/3000] 1 tree, 31 leaves, max depth = 13, train loss: 0.05110, val loss: 0.12604, in 0.255s\n",
      "[1744/3000] 1 tree, 31 leaves, max depth = 14, train loss: 0.05107, val loss: 0.12605, in 0.255s\n",
      "[1745/3000] 1 tree, 31 leaves, max depth = 10, train loss: 0.05105, val loss: 0.12606, in 0.165s\n",
      "[1746/3000] 1 tree, 31 leaves, max depth = 15, train loss: 0.05102, val loss: 0.12607, in 0.180s\n",
      "[1747/3000] 1 tree, 31 leaves, max depth = 11, train loss: 0.05098, val loss: 0.12606, in 0.259s\n",
      "[1748/3000] 1 tree, 31 leaves, max depth = 15, train loss: 0.05095, val loss: 0.12606, in 0.201s\n",
      "[1749/3000] 1 tree, 31 leaves, max depth = 11, train loss: 0.05093, val loss: 0.12605, in 0.190s\n",
      "[1750/3000] 1 tree, 31 leaves, max depth = 14, train loss: 0.05090, val loss: 0.12604, in 0.190s\n",
      "[1751/3000] 1 tree, 31 leaves, max depth = 11, train loss: 0.05088, val loss: 0.12603, in 0.208s\n",
      "[1752/3000] 1 tree, 31 leaves, max depth = 11, train loss: 0.05082, val loss: 0.12598, in 0.225s\n",
      "[1753/3000] 1 tree, 31 leaves, max depth = 10, train loss: 0.05079, val loss: 0.12597, in 0.188s\n",
      "[1754/3000] 1 tree, 31 leaves, max depth = 12, train loss: 0.05074, val loss: 0.12593, in 0.246s\n",
      "[1755/3000] 1 tree, 31 leaves, max depth = 9, train loss: 0.05071, val loss: 0.12594, in 0.217s\n",
      "[1756/3000] 1 tree, 31 leaves, max depth = 10, train loss: 0.05068, val loss: 0.12593, in 0.213s\n",
      "[1757/3000] 1 tree, 31 leaves, max depth = 12, train loss: 0.05066, val loss: 0.12592, in 0.194s\n",
      "[1758/3000] 1 tree, 31 leaves, max depth = 13, train loss: 0.05063, val loss: 0.12594, in 0.179s\n",
      "[1759/3000] 1 tree, 31 leaves, max depth = 11, train loss: 0.05060, val loss: 0.12593, in 0.221s\n",
      "[1760/3000] 1 tree, 31 leaves, max depth = 11, train loss: 0.05058, val loss: 0.12593, in 0.194s\n",
      "[1761/3000] 1 tree, 31 leaves, max depth = 11, train loss: 0.05055, val loss: 0.12593, in 0.214s\n",
      "[1762/3000] 1 tree, 31 leaves, max depth = 11, train loss: 0.05052, val loss: 0.12594, in 0.315s\n",
      "[1763/3000] 1 tree, 31 leaves, max depth = 14, train loss: 0.05049, val loss: 0.12594, in 0.192s\n",
      "[1764/3000] 1 tree, 31 leaves, max depth = 15, train loss: 0.05046, val loss: 0.12593, in 0.228s\n",
      "[1765/3000] 1 tree, 31 leaves, max depth = 11, train loss: 0.05044, val loss: 0.12591, in 0.188s\n",
      "[1766/3000] 1 tree, 31 leaves, max depth = 12, train loss: 0.05038, val loss: 0.12586, in 0.252s\n",
      "[1767/3000] 1 tree, 31 leaves, max depth = 11, train loss: 0.05035, val loss: 0.12584, in 0.219s\n",
      "[1768/3000] 1 tree, 31 leaves, max depth = 11, train loss: 0.05032, val loss: 0.12585, in 0.217s\n",
      "[1769/3000] 1 tree, 31 leaves, max depth = 12, train loss: 0.05030, val loss: 0.12587, in 0.175s\n",
      "[1770/3000] 1 tree, 31 leaves, max depth = 12, train loss: 0.05028, val loss: 0.12585, in 0.211s\n",
      "[1771/3000] 1 tree, 31 leaves, max depth = 13, train loss: 0.05024, val loss: 0.12583, in 0.191s\n",
      "[1772/3000] 1 tree, 31 leaves, max depth = 12, train loss: 0.05022, val loss: 0.12583, in 0.175s\n",
      "[1773/3000] 1 tree, 31 leaves, max depth = 13, train loss: 0.05020, val loss: 0.12583, in 0.194s\n",
      "[1774/3000] 1 tree, 31 leaves, max depth = 10, train loss: 0.05017, val loss: 0.12583, in 0.191s\n",
      "[1775/3000] 1 tree, 31 leaves, max depth = 13, train loss: 0.05013, val loss: 0.12581, in 0.187s\n",
      "[1776/3000] 1 tree, 31 leaves, max depth = 13, train loss: 0.05009, val loss: 0.12582, in 0.223s\n",
      "[1777/3000] 1 tree, 31 leaves, max depth = 17, train loss: 0.05006, val loss: 0.12583, in 0.207s\n",
      "[1778/3000] 1 tree, 31 leaves, max depth = 14, train loss: 0.05003, val loss: 0.12584, in 0.210s\n",
      "[1779/3000] 1 tree, 31 leaves, max depth = 10, train loss: 0.05001, val loss: 0.12585, in 0.171s\n",
      "[1780/3000] 1 tree, 31 leaves, max depth = 12, train loss: 0.04997, val loss: 0.12584, in 0.175s\n",
      "[1781/3000] 1 tree, 31 leaves, max depth = 13, train loss: 0.04995, val loss: 0.12583, in 0.223s\n",
      "[1782/3000] 1 tree, 31 leaves, max depth = 9, train loss: 0.04993, val loss: 0.12581, in 0.192s\n",
      "[1783/3000] 1 tree, 31 leaves, max depth = 12, train loss: 0.04990, val loss: 0.12581, in 0.360s\n",
      "[1784/3000] 1 tree, 31 leaves, max depth = 12, train loss: 0.04987, val loss: 0.12580, in 0.250s\n",
      "[1785/3000] 1 tree, 31 leaves, max depth = 11, train loss: 0.04985, val loss: 0.12581, in 0.200s\n",
      "[1786/3000] 1 tree, 31 leaves, max depth = 11, train loss: 0.04982, val loss: 0.12580, in 0.188s\n",
      "[1787/3000] 1 tree, 31 leaves, max depth = 11, train loss: 0.04979, val loss: 0.12580, in 0.279s\n",
      "[1788/3000] 1 tree, 31 leaves, max depth = 14, train loss: 0.04975, val loss: 0.12579, in 0.261s\n",
      "[1789/3000] 1 tree, 31 leaves, max depth = 12, train loss: 0.04973, val loss: 0.12577, in 0.230s\n",
      "[1790/3000] 1 tree, 31 leaves, max depth = 12, train loss: 0.04970, val loss: 0.12578, in 0.197s\n",
      "[1791/3000] 1 tree, 31 leaves, max depth = 13, train loss: 0.04967, val loss: 0.12580, in 0.197s\n",
      "[1792/3000] 1 tree, 31 leaves, max depth = 12, train loss: 0.04964, val loss: 0.12580, in 0.234s\n",
      "[1793/3000] 1 tree, 31 leaves, max depth = 10, train loss: 0.04960, val loss: 0.12578, in 0.242s\n",
      "[1794/3000] 1 tree, 31 leaves, max depth = 14, train loss: 0.04958, val loss: 0.12576, in 0.235s\n",
      "[1795/3000] 1 tree, 31 leaves, max depth = 10, train loss: 0.04955, val loss: 0.12577, in 0.194s\n",
      "[1796/3000] 1 tree, 31 leaves, max depth = 8, train loss: 0.04953, val loss: 0.12576, in 0.215s\n",
      "[1797/3000] 1 tree, 31 leaves, max depth = 12, train loss: 0.04949, val loss: 0.12573, in 0.225s\n",
      "[1798/3000] 1 tree, 31 leaves, max depth = 12, train loss: 0.04946, val loss: 0.12572, in 0.166s\n",
      "[1799/3000] 1 tree, 31 leaves, max depth = 13, train loss: 0.04943, val loss: 0.12574, in 0.172s\n",
      "[1800/3000] 1 tree, 31 leaves, max depth = 11, train loss: 0.04940, val loss: 0.12574, in 0.183s\n",
      "[1801/3000] 1 tree, 31 leaves, max depth = 11, train loss: 0.04935, val loss: 0.12572, in 0.249s\n",
      "[1802/3000] 1 tree, 31 leaves, max depth = 12, train loss: 0.04931, val loss: 0.12572, in 0.224s\n",
      "[1803/3000] 1 tree, 31 leaves, max depth = 14, train loss: 0.04929, val loss: 0.12572, in 0.360s\n",
      "[1804/3000] 1 tree, 31 leaves, max depth = 11, train loss: 0.04925, val loss: 0.12570, in 0.234s\n",
      "[1805/3000] 1 tree, 31 leaves, max depth = 13, train loss: 0.04923, val loss: 0.12569, in 0.178s\n",
      "[1806/3000] 1 tree, 31 leaves, max depth = 9, train loss: 0.04920, val loss: 0.12567, in 0.184s\n",
      "[1807/3000] 1 tree, 31 leaves, max depth = 10, train loss: 0.04918, val loss: 0.12566, in 0.209s\n",
      "[1808/3000] 1 tree, 31 leaves, max depth = 10, train loss: 0.04916, val loss: 0.12566, in 0.186s\n",
      "[1809/3000] 1 tree, 31 leaves, max depth = 10, train loss: 0.04914, val loss: 0.12566, in 0.186s\n",
      "[1810/3000] 1 tree, 31 leaves, max depth = 9, train loss: 0.04911, val loss: 0.12564, in 0.197s\n",
      "[1811/3000] 1 tree, 31 leaves, max depth = 13, train loss: 0.04909, val loss: 0.12565, in 0.198s\n",
      "[1812/3000] 1 tree, 31 leaves, max depth = 13, train loss: 0.04905, val loss: 0.12567, in 0.177s\n",
      "[1813/3000] 1 tree, 31 leaves, max depth = 9, train loss: 0.04903, val loss: 0.12566, in 0.171s\n",
      "[1814/3000] 1 tree, 31 leaves, max depth = 11, train loss: 0.04899, val loss: 0.12565, in 0.243s\n",
      "[1815/3000] 1 tree, 31 leaves, max depth = 12, train loss: 0.04895, val loss: 0.12562, in 0.210s\n",
      "[1816/3000] 1 tree, 31 leaves, max depth = 12, train loss: 0.04892, val loss: 0.12561, in 0.232s\n",
      "[1817/3000] 1 tree, 31 leaves, max depth = 11, train loss: 0.04890, val loss: 0.12563, in 0.162s\n",
      "[1818/3000] 1 tree, 31 leaves, max depth = 10, train loss: 0.04887, val loss: 0.12561, in 0.199s\n",
      "[1819/3000] 1 tree, 31 leaves, max depth = 14, train loss: 0.04885, val loss: 0.12560, in 0.158s\n",
      "[1820/3000] 1 tree, 31 leaves, max depth = 9, train loss: 0.04882, val loss: 0.12559, in 0.195s\n",
      "[1821/3000] 1 tree, 31 leaves, max depth = 11, train loss: 0.04879, val loss: 0.12558, in 0.214s\n",
      "[1822/3000] 1 tree, 31 leaves, max depth = 11, train loss: 0.04877, val loss: 0.12557, in 0.221s\n",
      "[1823/3000] 1 tree, 31 leaves, max depth = 13, train loss: 0.04873, val loss: 0.12555, in 0.198s\n",
      "[1824/3000] 1 tree, 31 leaves, max depth = 9, train loss: 0.04871, val loss: 0.12555, in 0.193s\n",
      "[1825/3000] 1 tree, 31 leaves, max depth = 13, train loss: 0.04869, val loss: 0.12554, in 0.345s\n",
      "[1826/3000] 1 tree, 31 leaves, max depth = 16, train loss: 0.04866, val loss: 0.12553, in 0.179s\n",
      "[1827/3000] 1 tree, 31 leaves, max depth = 13, train loss: 0.04863, val loss: 0.12553, in 0.227s\n",
      "[1828/3000] 1 tree, 31 leaves, max depth = 11, train loss: 0.04862, val loss: 0.12553, in 0.215s\n",
      "[1829/3000] 1 tree, 31 leaves, max depth = 11, train loss: 0.04859, val loss: 0.12553, in 0.256s\n",
      "[1830/3000] 1 tree, 31 leaves, max depth = 11, train loss: 0.04857, val loss: 0.12553, in 0.197s\n",
      "[1831/3000] 1 tree, 31 leaves, max depth = 12, train loss: 0.04854, val loss: 0.12553, in 0.208s\n",
      "[1832/3000] 1 tree, 31 leaves, max depth = 10, train loss: 0.04852, val loss: 0.12551, in 0.204s\n",
      "[1833/3000] 1 tree, 31 leaves, max depth = 12, train loss: 0.04849, val loss: 0.12551, in 0.261s\n",
      "[1834/3000] 1 tree, 31 leaves, max depth = 9, train loss: 0.04847, val loss: 0.12550, in 0.197s\n",
      "[1835/3000] 1 tree, 31 leaves, max depth = 11, train loss: 0.04845, val loss: 0.12551, in 0.202s\n",
      "[1836/3000] 1 tree, 31 leaves, max depth = 11, train loss: 0.04841, val loss: 0.12551, in 0.184s\n",
      "[1837/3000] 1 tree, 31 leaves, max depth = 10, train loss: 0.04838, val loss: 0.12552, in 0.216s\n",
      "[1838/3000] 1 tree, 31 leaves, max depth = 14, train loss: 0.04834, val loss: 0.12552, in 0.208s\n",
      "[1839/3000] 1 tree, 31 leaves, max depth = 13, train loss: 0.04833, val loss: 0.12553, in 0.189s\n",
      "[1840/3000] 1 tree, 31 leaves, max depth = 14, train loss: 0.04830, val loss: 0.12554, in 0.189s\n",
      "[1841/3000] 1 tree, 31 leaves, max depth = 12, train loss: 0.04829, val loss: 0.12555, in 0.170s\n",
      "[1842/3000] 1 tree, 31 leaves, max depth = 14, train loss: 0.04827, val loss: 0.12555, in 0.224s\n",
      "[1843/3000] 1 tree, 31 leaves, max depth = 9, train loss: 0.04825, val loss: 0.12555, in 0.180s\n",
      "[1844/3000] 1 tree, 31 leaves, max depth = 14, train loss: 0.04823, val loss: 0.12556, in 0.220s\n",
      "[1845/3000] 1 tree, 31 leaves, max depth = 10, train loss: 0.04820, val loss: 0.12557, in 0.333s\n",
      "[1846/3000] 1 tree, 31 leaves, max depth = 9, train loss: 0.04818, val loss: 0.12557, in 0.185s\n",
      "[1847/3000] 1 tree, 31 leaves, max depth = 10, train loss: 0.04816, val loss: 0.12558, in 0.232s\n",
      "[1848/3000] 1 tree, 31 leaves, max depth = 11, train loss: 0.04812, val loss: 0.12559, in 0.199s\n",
      "[1849/3000] 1 tree, 31 leaves, max depth = 10, train loss: 0.04809, val loss: 0.12559, in 0.205s\n",
      "[1850/3000] 1 tree, 31 leaves, max depth = 10, train loss: 0.04806, val loss: 0.12559, in 0.201s\n",
      "[1851/3000] 1 tree, 31 leaves, max depth = 11, train loss: 0.04804, val loss: 0.12560, in 0.172s\n",
      "[1852/3000] 1 tree, 31 leaves, max depth = 11, train loss: 0.04801, val loss: 0.12559, in 0.184s\n",
      "[1853/3000] 1 tree, 31 leaves, max depth = 10, train loss: 0.04799, val loss: 0.12560, in 0.184s\n",
      "[1854/3000] 1 tree, 31 leaves, max depth = 9, train loss: 0.04793, val loss: 0.12553, in 0.264s\n",
      "[1855/3000] 1 tree, 31 leaves, max depth = 10, train loss: 0.04791, val loss: 0.12554, in 0.164s\n",
      "[1856/3000] 1 tree, 31 leaves, max depth = 11, train loss: 0.04788, val loss: 0.12554, in 0.198s\n",
      "[1857/3000] 1 tree, 31 leaves, max depth = 10, train loss: 0.04785, val loss: 0.12553, in 0.209s\n",
      "[1858/3000] 1 tree, 31 leaves, max depth = 11, train loss: 0.04781, val loss: 0.12552, in 0.202s\n",
      "[1859/3000] 1 tree, 31 leaves, max depth = 13, train loss: 0.04777, val loss: 0.12550, in 0.207s\n",
      "[1860/3000] 1 tree, 31 leaves, max depth = 9, train loss: 0.04774, val loss: 0.12548, in 0.204s\n",
      "[1861/3000] 1 tree, 31 leaves, max depth = 11, train loss: 0.04770, val loss: 0.12549, in 0.203s\n",
      "[1862/3000] 1 tree, 31 leaves, max depth = 10, train loss: 0.04767, val loss: 0.12548, in 0.203s\n",
      "[1863/3000] 1 tree, 31 leaves, max depth = 11, train loss: 0.04764, val loss: 0.12549, in 0.215s\n",
      "[1864/3000] 1 tree, 31 leaves, max depth = 13, train loss: 0.04761, val loss: 0.12548, in 0.184s\n",
      "[1865/3000] 1 tree, 31 leaves, max depth = 11, train loss: 0.04758, val loss: 0.12550, in 0.260s\n",
      "[1866/3000] 1 tree, 31 leaves, max depth = 10, train loss: 0.04756, val loss: 0.12550, in 0.348s\n",
      "[1867/3000] 1 tree, 31 leaves, max depth = 9, train loss: 0.04753, val loss: 0.12550, in 0.246s\n",
      "[1868/3000] 1 tree, 31 leaves, max depth = 12, train loss: 0.04751, val loss: 0.12549, in 0.601s\n",
      "[1869/3000] 1 tree, 31 leaves, max depth = 10, train loss: 0.04748, val loss: 0.12551, in 0.185s\n",
      "[1870/3000] 1 tree, 31 leaves, max depth = 12, train loss: 0.04745, val loss: 0.12550, in 0.216s\n",
      "[1871/3000] 1 tree, 31 leaves, max depth = 12, train loss: 0.04743, val loss: 0.12548, in 0.182s\n",
      "[1872/3000] 1 tree, 31 leaves, max depth = 10, train loss: 0.04740, val loss: 0.12550, in 0.220s\n",
      "[1873/3000] 1 tree, 31 leaves, max depth = 12, train loss: 0.04737, val loss: 0.12552, in 0.217s\n",
      "[1874/3000] 1 tree, 31 leaves, max depth = 12, train loss: 0.04735, val loss: 0.12552, in 0.222s\n",
      "[1875/3000] 1 tree, 31 leaves, max depth = 9, train loss: 0.04731, val loss: 0.12553, in 0.189s\n",
      "[1876/3000] 1 tree, 31 leaves, max depth = 10, train loss: 0.04727, val loss: 0.12552, in 0.252s\n",
      "[1877/3000] 1 tree, 31 leaves, max depth = 10, train loss: 0.04724, val loss: 0.12551, in 0.257s\n",
      "[1878/3000] 1 tree, 31 leaves, max depth = 10, train loss: 0.04721, val loss: 0.12552, in 0.242s\n",
      "[1879/3000] 1 tree, 31 leaves, max depth = 14, train loss: 0.04718, val loss: 0.12552, in 0.166s\n",
      "[1880/3000] 1 tree, 31 leaves, max depth = 15, train loss: 0.04717, val loss: 0.12551, in 0.193s\n",
      "[1881/3000] 1 tree, 31 leaves, max depth = 12, train loss: 0.04714, val loss: 0.12553, in 0.211s\n",
      "[1882/3000] 1 tree, 31 leaves, max depth = 9, train loss: 0.04711, val loss: 0.12553, in 0.202s\n",
      "[1883/3000] 1 tree, 31 leaves, max depth = 12, train loss: 0.04709, val loss: 0.12553, in 0.211s\n",
      "[1884/3000] 1 tree, 31 leaves, max depth = 12, train loss: 0.04705, val loss: 0.12551, in 0.223s\n",
      "[1885/3000] 1 tree, 31 leaves, max depth = 12, train loss: 0.04704, val loss: 0.12551, in 0.190s\n",
      "[1886/3000] 1 tree, 31 leaves, max depth = 13, train loss: 0.04702, val loss: 0.12553, in 0.212s\n",
      "[1887/3000] 1 tree, 31 leaves, max depth = 12, train loss: 0.04700, val loss: 0.12554, in 0.192s\n",
      "[1888/3000] 1 tree, 31 leaves, max depth = 18, train loss: 0.04698, val loss: 0.12554, in 0.393s\n",
      "[1889/3000] 1 tree, 31 leaves, max depth = 15, train loss: 0.04695, val loss: 0.12552, in 0.243s\n",
      "[1890/3000] 1 tree, 31 leaves, max depth = 10, train loss: 0.04692, val loss: 0.12552, in 0.183s\n",
      "[1891/3000] 1 tree, 31 leaves, max depth = 15, train loss: 0.04690, val loss: 0.12551, in 0.193s\n",
      "[1892/3000] 1 tree, 31 leaves, max depth = 11, train loss: 0.04688, val loss: 0.12551, in 0.178s\n",
      "[1893/3000] 1 tree, 31 leaves, max depth = 12, train loss: 0.04685, val loss: 0.12552, in 0.210s\n",
      "[1894/3000] 1 tree, 31 leaves, max depth = 16, train loss: 0.04682, val loss: 0.12551, in 0.227s\n",
      "[1895/3000] 1 tree, 31 leaves, max depth = 10, train loss: 0.04679, val loss: 0.12551, in 0.189s\n",
      "[1896/3000] 1 tree, 31 leaves, max depth = 9, train loss: 0.04675, val loss: 0.12550, in 0.287s\n",
      "[1897/3000] 1 tree, 31 leaves, max depth = 13, train loss: 0.04672, val loss: 0.12549, in 0.208s\n",
      "[1898/3000] 1 tree, 31 leaves, max depth = 13, train loss: 0.04669, val loss: 0.12548, in 0.208s\n",
      "[1899/3000] 1 tree, 31 leaves, max depth = 14, train loss: 0.04667, val loss: 0.12550, in 0.208s\n",
      "[1900/3000] 1 tree, 31 leaves, max depth = 10, train loss: 0.04664, val loss: 0.12550, in 0.181s\n",
      "[1901/3000] 1 tree, 31 leaves, max depth = 8, train loss: 0.04661, val loss: 0.12550, in 0.187s\n",
      "[1902/3000] 1 tree, 31 leaves, max depth = 12, train loss: 0.04658, val loss: 0.12550, in 0.231s\n",
      "[1903/3000] 1 tree, 31 leaves, max depth = 16, train loss: 0.04655, val loss: 0.12549, in 0.193s\n",
      "[1904/3000] 1 tree, 31 leaves, max depth = 10, train loss: 0.04653, val loss: 0.12550, in 0.198s\n",
      "[1905/3000] 1 tree, 31 leaves, max depth = 12, train loss: 0.04651, val loss: 0.12548, in 0.216s\n",
      "[1906/3000] 1 tree, 31 leaves, max depth = 14, train loss: 0.04649, val loss: 0.12549, in 0.161s\n",
      "[1907/3000] 1 tree, 31 leaves, max depth = 10, train loss: 0.04646, val loss: 0.12547, in 0.205s\n",
      "[1908/3000] 1 tree, 31 leaves, max depth = 9, train loss: 0.04642, val loss: 0.12546, in 0.213s\n",
      "[1909/3000] 1 tree, 31 leaves, max depth = 11, train loss: 0.04639, val loss: 0.12547, in 0.360s\n",
      "[1910/3000] 1 tree, 31 leaves, max depth = 9, train loss: 0.04637, val loss: 0.12550, in 0.232s\n",
      "[1911/3000] 1 tree, 31 leaves, max depth = 10, train loss: 0.04634, val loss: 0.12549, in 0.197s\n",
      "[1912/3000] 1 tree, 31 leaves, max depth = 12, train loss: 0.04632, val loss: 0.12549, in 0.180s\n",
      "[1913/3000] 1 tree, 31 leaves, max depth = 14, train loss: 0.04630, val loss: 0.12549, in 0.156s\n",
      "[1914/3000] 1 tree, 31 leaves, max depth = 10, train loss: 0.04627, val loss: 0.12546, in 0.216s\n",
      "[1915/3000] 1 tree, 31 leaves, max depth = 14, train loss: 0.04626, val loss: 0.12546, in 0.195s\n",
      "[1916/3000] 1 tree, 31 leaves, max depth = 10, train loss: 0.04621, val loss: 0.12546, in 0.318s\n",
      "[1917/3000] 1 tree, 31 leaves, max depth = 10, train loss: 0.04619, val loss: 0.12546, in 0.181s\n",
      "[1918/3000] 1 tree, 31 leaves, max depth = 12, train loss: 0.04614, val loss: 0.12546, in 0.235s\n",
      "[1919/3000] 1 tree, 31 leaves, max depth = 10, train loss: 0.04611, val loss: 0.12549, in 0.199s\n",
      "[1920/3000] 1 tree, 31 leaves, max depth = 9, train loss: 0.04609, val loss: 0.12547, in 0.235s\n",
      "[1921/3000] 1 tree, 31 leaves, max depth = 9, train loss: 0.04607, val loss: 0.12548, in 0.235s\n",
      "[1922/3000] 1 tree, 31 leaves, max depth = 14, train loss: 0.04602, val loss: 0.12545, in 0.240s\n",
      "[1923/3000] 1 tree, 31 leaves, max depth = 13, train loss: 0.04600, val loss: 0.12546, in 0.182s\n",
      "[1924/3000] 1 tree, 31 leaves, max depth = 9, train loss: 0.04598, val loss: 0.12546, in 0.216s\n",
      "[1925/3000] 1 tree, 31 leaves, max depth = 15, train loss: 0.04596, val loss: 0.12547, in 0.195s\n",
      "[1926/3000] 1 tree, 31 leaves, max depth = 11, train loss: 0.04594, val loss: 0.12547, in 0.179s\n",
      "[1927/3000] 1 tree, 31 leaves, max depth = 12, train loss: 0.04589, val loss: 0.12545, in 0.256s\n",
      "[1928/3000] 1 tree, 31 leaves, max depth = 17, train loss: 0.04586, val loss: 0.12544, in 0.224s\n",
      "[1929/3000] 1 tree, 31 leaves, max depth = 12, train loss: 0.04582, val loss: 0.12543, in 0.333s\n",
      "[1930/3000] 1 tree, 31 leaves, max depth = 13, train loss: 0.04579, val loss: 0.12543, in 0.222s\n",
      "[1931/3000] 1 tree, 31 leaves, max depth = 14, train loss: 0.04576, val loss: 0.12540, in 0.268s\n",
      "[1932/3000] 1 tree, 31 leaves, max depth = 12, train loss: 0.04573, val loss: 0.12539, in 0.294s\n",
      "[1933/3000] 1 tree, 31 leaves, max depth = 13, train loss: 0.04570, val loss: 0.12540, in 0.234s\n",
      "[1934/3000] 1 tree, 31 leaves, max depth = 13, train loss: 0.04567, val loss: 0.12537, in 0.234s\n",
      "[1935/3000] 1 tree, 31 leaves, max depth = 10, train loss: 0.04565, val loss: 0.12538, in 0.196s\n",
      "[1936/3000] 1 tree, 31 leaves, max depth = 12, train loss: 0.04564, val loss: 0.12537, in 0.221s\n",
      "[1937/3000] 1 tree, 31 leaves, max depth = 14, train loss: 0.04562, val loss: 0.12539, in 0.204s\n",
      "[1938/3000] 1 tree, 31 leaves, max depth = 12, train loss: 0.04559, val loss: 0.12540, in 0.219s\n",
      "[1939/3000] 1 tree, 31 leaves, max depth = 10, train loss: 0.04557, val loss: 0.12540, in 0.183s\n",
      "[1940/3000] 1 tree, 31 leaves, max depth = 9, train loss: 0.04554, val loss: 0.12540, in 0.232s\n",
      "[1941/3000] 1 tree, 31 leaves, max depth = 14, train loss: 0.04551, val loss: 0.12539, in 0.223s\n",
      "[1942/3000] 1 tree, 31 leaves, max depth = 10, train loss: 0.04549, val loss: 0.12539, in 0.197s\n",
      "[1943/3000] 1 tree, 31 leaves, max depth = 13, train loss: 0.04546, val loss: 0.12540, in 0.187s\n",
      "[1944/3000] 1 tree, 31 leaves, max depth = 12, train loss: 0.04542, val loss: 0.12540, in 0.263s\n",
      "[1945/3000] 1 tree, 31 leaves, max depth = 12, train loss: 0.04540, val loss: 0.12538, in 0.177s\n",
      "[1946/3000] 1 tree, 31 leaves, max depth = 10, train loss: 0.04537, val loss: 0.12540, in 0.200s\n",
      "[1947/3000] 1 tree, 31 leaves, max depth = 10, train loss: 0.04535, val loss: 0.12540, in 0.166s\n",
      "[1948/3000] 1 tree, 31 leaves, max depth = 13, train loss: 0.04532, val loss: 0.12540, in 0.188s\n",
      "[1949/3000] 1 tree, 31 leaves, max depth = 13, train loss: 0.04530, val loss: 0.12539, in 0.190s\n",
      "[1950/3000] 1 tree, 31 leaves, max depth = 14, train loss: 0.04528, val loss: 0.12539, in 0.165s\n",
      "[1951/3000] 1 tree, 31 leaves, max depth = 10, train loss: 0.04526, val loss: 0.12539, in 0.329s\n",
      "[1952/3000] 1 tree, 31 leaves, max depth = 12, train loss: 0.04524, val loss: 0.12539, in 0.184s\n",
      "[1953/3000] 1 tree, 31 leaves, max depth = 11, train loss: 0.04520, val loss: 0.12539, in 0.251s\n",
      "[1954/3000] 1 tree, 31 leaves, max depth = 17, train loss: 0.04517, val loss: 0.12541, in 0.228s\n",
      "[1955/3000] 1 tree, 31 leaves, max depth = 14, train loss: 0.04514, val loss: 0.12539, in 0.175s\n",
      "[1956/3000] 1 tree, 31 leaves, max depth = 12, train loss: 0.04511, val loss: 0.12539, in 0.204s\n",
      "[1957/3000] 1 tree, 31 leaves, max depth = 13, train loss: 0.04510, val loss: 0.12538, in 0.177s\n",
      "[1958/3000] 1 tree, 31 leaves, max depth = 10, train loss: 0.04508, val loss: 0.12539, in 0.221s\n",
      "[1959/3000] 1 tree, 31 leaves, max depth = 11, train loss: 0.04506, val loss: 0.12539, in 0.164s\n",
      "[1960/3000] 1 tree, 31 leaves, max depth = 10, train loss: 0.04503, val loss: 0.12538, in 0.185s\n",
      "[1961/3000] 1 tree, 31 leaves, max depth = 12, train loss: 0.04500, val loss: 0.12537, in 0.230s\n",
      "[1962/3000] 1 tree, 31 leaves, max depth = 12, train loss: 0.04498, val loss: 0.12540, in 0.159s\n",
      "[1963/3000] 1 tree, 31 leaves, max depth = 12, train loss: 0.04497, val loss: 0.12539, in 0.176s\n",
      "[1964/3000] 1 tree, 31 leaves, max depth = 12, train loss: 0.04495, val loss: 0.12539, in 0.190s\n",
      "[1965/3000] 1 tree, 31 leaves, max depth = 16, train loss: 0.04492, val loss: 0.12539, in 0.228s\n",
      "[1966/3000] 1 tree, 31 leaves, max depth = 10, train loss: 0.04489, val loss: 0.12537, in 0.226s\n",
      "[1967/3000] 1 tree, 31 leaves, max depth = 12, train loss: 0.04486, val loss: 0.12536, in 0.181s\n",
      "[1968/3000] 1 tree, 31 leaves, max depth = 10, train loss: 0.04483, val loss: 0.12535, in 0.212s\n",
      "[1969/3000] 1 tree, 31 leaves, max depth = 11, train loss: 0.04478, val loss: 0.12532, in 0.221s\n",
      "[1970/3000] 1 tree, 31 leaves, max depth = 15, train loss: 0.04475, val loss: 0.12533, in 0.224s\n",
      "[1971/3000] 1 tree, 31 leaves, max depth = 15, train loss: 0.04471, val loss: 0.12527, in 0.235s\n",
      "[1972/3000] 1 tree, 31 leaves, max depth = 10, train loss: 0.04469, val loss: 0.12527, in 0.337s\n",
      "[1973/3000] 1 tree, 31 leaves, max depth = 12, train loss: 0.04463, val loss: 0.12522, in 0.252s\n",
      "[1974/3000] 1 tree, 31 leaves, max depth = 9, train loss: 0.04460, val loss: 0.12519, in 0.197s\n",
      "[1975/3000] 1 tree, 31 leaves, max depth = 15, train loss: 0.04457, val loss: 0.12519, in 0.238s\n",
      "[1976/3000] 1 tree, 31 leaves, max depth = 14, train loss: 0.04456, val loss: 0.12517, in 0.194s\n",
      "[1977/3000] 1 tree, 31 leaves, max depth = 12, train loss: 0.04454, val loss: 0.12517, in 0.186s\n",
      "[1978/3000] 1 tree, 31 leaves, max depth = 11, train loss: 0.04451, val loss: 0.12515, in 0.204s\n",
      "[1979/3000] 1 tree, 31 leaves, max depth = 12, train loss: 0.04448, val loss: 0.12514, in 0.200s\n",
      "[1980/3000] 1 tree, 31 leaves, max depth = 13, train loss: 0.04446, val loss: 0.12513, in 0.238s\n",
      "[1981/3000] 1 tree, 31 leaves, max depth = 10, train loss: 0.04444, val loss: 0.12513, in 0.202s\n",
      "[1982/3000] 1 tree, 31 leaves, max depth = 13, train loss: 0.04441, val loss: 0.12513, in 0.229s\n",
      "[1983/3000] 1 tree, 31 leaves, max depth = 13, train loss: 0.04439, val loss: 0.12513, in 0.171s\n",
      "[1984/3000] 1 tree, 31 leaves, max depth = 14, train loss: 0.04437, val loss: 0.12512, in 0.225s\n",
      "[1985/3000] 1 tree, 31 leaves, max depth = 14, train loss: 0.04433, val loss: 0.12511, in 0.220s\n",
      "[1986/3000] 1 tree, 31 leaves, max depth = 13, train loss: 0.04430, val loss: 0.12513, in 0.227s\n",
      "[1987/3000] 1 tree, 31 leaves, max depth = 11, train loss: 0.04428, val loss: 0.12514, in 0.204s\n",
      "[1988/3000] 1 tree, 31 leaves, max depth = 10, train loss: 0.04425, val loss: 0.12512, in 0.181s\n",
      "[1989/3000] 1 tree, 31 leaves, max depth = 11, train loss: 0.04422, val loss: 0.12511, in 0.233s\n",
      "[1990/3000] 1 tree, 31 leaves, max depth = 13, train loss: 0.04420, val loss: 0.12511, in 0.163s\n",
      "[1991/3000] 1 tree, 31 leaves, max depth = 11, train loss: 0.04417, val loss: 0.12513, in 0.192s\n",
      "[1992/3000] 1 tree, 31 leaves, max depth = 12, train loss: 0.04414, val loss: 0.12514, in 0.346s\n",
      "[1993/3000] 1 tree, 31 leaves, max depth = 12, train loss: 0.04412, val loss: 0.12514, in 0.208s\n",
      "[1994/3000] 1 tree, 31 leaves, max depth = 10, train loss: 0.04410, val loss: 0.12515, in 0.194s\n",
      "[1995/3000] 1 tree, 31 leaves, max depth = 14, train loss: 0.04407, val loss: 0.12512, in 0.196s\n",
      "[1996/3000] 1 tree, 31 leaves, max depth = 10, train loss: 0.04405, val loss: 0.12514, in 0.206s\n",
      "[1997/3000] 1 tree, 31 leaves, max depth = 11, train loss: 0.04404, val loss: 0.12513, in 0.188s\n",
      "[1998/3000] 1 tree, 31 leaves, max depth = 16, train loss: 0.04401, val loss: 0.12513, in 0.263s\n",
      "[1999/3000] 1 tree, 31 leaves, max depth = 13, train loss: 0.04400, val loss: 0.12513, in 0.200s\n",
      "[2000/3000] 1 tree, 31 leaves, max depth = 11, train loss: 0.04397, val loss: 0.12513, in 0.263s\n",
      "[2001/3000] 1 tree, 31 leaves, max depth = 9, train loss: 0.04395, val loss: 0.12512, in 0.173s\n",
      "[2002/3000] 1 tree, 31 leaves, max depth = 11, train loss: 0.04393, val loss: 0.12513, in 0.202s\n",
      "[2003/3000] 1 tree, 31 leaves, max depth = 12, train loss: 0.04390, val loss: 0.12510, in 0.212s\n",
      "[2004/3000] 1 tree, 31 leaves, max depth = 11, train loss: 0.04387, val loss: 0.12510, in 0.202s\n",
      "[2005/3000] 1 tree, 31 leaves, max depth = 9, train loss: 0.04385, val loss: 0.12511, in 0.222s\n",
      "[2006/3000] 1 tree, 31 leaves, max depth = 10, train loss: 0.04381, val loss: 0.12510, in 0.213s\n",
      "[2007/3000] 1 tree, 31 leaves, max depth = 10, train loss: 0.04378, val loss: 0.12510, in 0.208s\n",
      "[2008/3000] 1 tree, 31 leaves, max depth = 10, train loss: 0.04375, val loss: 0.12510, in 0.194s\n",
      "[2009/3000] 1 tree, 31 leaves, max depth = 13, train loss: 0.04372, val loss: 0.12511, in 0.198s\n",
      "[2010/3000] 1 tree, 31 leaves, max depth = 10, train loss: 0.04370, val loss: 0.12511, in 0.234s\n",
      "[2011/3000] 1 tree, 31 leaves, max depth = 15, train loss: 0.04368, val loss: 0.12512, in 0.201s\n",
      "[2012/3000] 1 tree, 31 leaves, max depth = 10, train loss: 0.04366, val loss: 0.12512, in 0.223s\n",
      "[2013/3000] 1 tree, 31 leaves, max depth = 10, train loss: 0.04364, val loss: 0.12512, in 0.294s\n",
      "[2014/3000] 1 tree, 31 leaves, max depth = 12, train loss: 0.04361, val loss: 0.12511, in 0.429s\n",
      "[2015/3000] 1 tree, 31 leaves, max depth = 10, train loss: 0.04358, val loss: 0.12511, in 0.234s\n",
      "[2016/3000] 1 tree, 31 leaves, max depth = 12, train loss: 0.04355, val loss: 0.12512, in 0.221s\n",
      "[2017/3000] 1 tree, 31 leaves, max depth = 10, train loss: 0.04353, val loss: 0.12512, in 0.258s\n",
      "[2018/3000] 1 tree, 31 leaves, max depth = 15, train loss: 0.04350, val loss: 0.12511, in 0.231s\n",
      "[2019/3000] 1 tree, 31 leaves, max depth = 10, train loss: 0.04349, val loss: 0.12510, in 0.201s\n",
      "[2020/3000] 1 tree, 31 leaves, max depth = 12, train loss: 0.04348, val loss: 0.12510, in 0.228s\n",
      "[2021/3000] 1 tree, 31 leaves, max depth = 10, train loss: 0.04345, val loss: 0.12510, in 0.219s\n",
      "[2022/3000] 1 tree, 31 leaves, max depth = 15, train loss: 0.04343, val loss: 0.12510, in 0.222s\n",
      "[2023/3000] 1 tree, 31 leaves, max depth = 13, train loss: 0.04340, val loss: 0.12508, in 0.197s\n",
      "[2024/3000] 1 tree, 31 leaves, max depth = 11, train loss: 0.04337, val loss: 0.12508, in 0.216s\n",
      "[2025/3000] 1 tree, 31 leaves, max depth = 12, train loss: 0.04334, val loss: 0.12508, in 0.237s\n",
      "[2026/3000] 1 tree, 31 leaves, max depth = 14, train loss: 0.04332, val loss: 0.12508, in 0.232s\n",
      "[2027/3000] 1 tree, 31 leaves, max depth = 11, train loss: 0.04328, val loss: 0.12507, in 0.211s\n",
      "[2028/3000] 1 tree, 31 leaves, max depth = 10, train loss: 0.04325, val loss: 0.12507, in 0.248s\n",
      "[2029/3000] 1 tree, 31 leaves, max depth = 13, train loss: 0.04324, val loss: 0.12507, in 0.206s\n",
      "[2030/3000] 1 tree, 31 leaves, max depth = 11, train loss: 0.04323, val loss: 0.12507, in 0.164s\n",
      "[2031/3000] 1 tree, 31 leaves, max depth = 9, train loss: 0.04322, val loss: 0.12508, in 0.188s\n",
      "[2032/3000] 1 tree, 31 leaves, max depth = 11, train loss: 0.04319, val loss: 0.12507, in 0.228s\n",
      "[2033/3000] 1 tree, 31 leaves, max depth = 15, train loss: 0.04317, val loss: 0.12507, in 0.154s\n",
      "[2034/3000] 1 tree, 31 leaves, max depth = 13, train loss: 0.04314, val loss: 0.12507, in 0.291s\n",
      "[2035/3000] 1 tree, 31 leaves, max depth = 11, train loss: 0.04312, val loss: 0.12508, in 0.221s\n",
      "[2036/3000] 1 tree, 31 leaves, max depth = 12, train loss: 0.04309, val loss: 0.12509, in 0.188s\n",
      "[2037/3000] 1 tree, 31 leaves, max depth = 9, train loss: 0.04307, val loss: 0.12509, in 0.218s\n",
      "[2038/3000] 1 tree, 31 leaves, max depth = 11, train loss: 0.04305, val loss: 0.12509, in 0.183s\n",
      "[2039/3000] 1 tree, 31 leaves, max depth = 12, train loss: 0.04301, val loss: 0.12506, in 0.269s\n",
      "[2040/3000] 1 tree, 31 leaves, max depth = 9, train loss: 0.04298, val loss: 0.12504, in 0.252s\n",
      "[2041/3000] 1 tree, 31 leaves, max depth = 17, train loss: 0.04295, val loss: 0.12501, in 0.204s\n",
      "[2042/3000] 1 tree, 31 leaves, max depth = 12, train loss: 0.04293, val loss: 0.12501, in 0.202s\n",
      "[2043/3000] 1 tree, 31 leaves, max depth = 10, train loss: 0.04291, val loss: 0.12501, in 0.223s\n",
      "[2044/3000] 1 tree, 31 leaves, max depth = 11, train loss: 0.04289, val loss: 0.12503, in 0.201s\n",
      "[2045/3000] 1 tree, 31 leaves, max depth = 10, train loss: 0.04286, val loss: 0.12503, in 0.217s\n",
      "[2046/3000] 1 tree, 31 leaves, max depth = 15, train loss: 0.04284, val loss: 0.12506, in 0.193s\n",
      "[2047/3000] 1 tree, 31 leaves, max depth = 12, train loss: 0.04280, val loss: 0.12504, in 0.234s\n",
      "[2048/3000] 1 tree, 31 leaves, max depth = 12, train loss: 0.04278, val loss: 0.12507, in 0.204s\n",
      "[2049/3000] 1 tree, 31 leaves, max depth = 11, train loss: 0.04275, val loss: 0.12507, in 0.209s\n",
      "[2050/3000] 1 tree, 31 leaves, max depth = 10, train loss: 0.04274, val loss: 0.12506, in 0.184s\n",
      "[2051/3000] 1 tree, 31 leaves, max depth = 12, train loss: 0.04273, val loss: 0.12507, in 0.192s\n",
      "[2052/3000] 1 tree, 31 leaves, max depth = 11, train loss: 0.04270, val loss: 0.12507, in 0.260s\n",
      "[2053/3000] 1 tree, 31 leaves, max depth = 10, train loss: 0.04268, val loss: 0.12507, in 0.198s\n",
      "[2054/3000] 1 tree, 31 leaves, max depth = 12, train loss: 0.04265, val loss: 0.12507, in 0.218s\n",
      "[2055/3000] 1 tree, 31 leaves, max depth = 14, train loss: 0.04262, val loss: 0.12508, in 0.333s\n",
      "[2056/3000] 1 tree, 31 leaves, max depth = 12, train loss: 0.04259, val loss: 0.12508, in 0.231s\n",
      "[2057/3000] 1 tree, 31 leaves, max depth = 10, train loss: 0.04256, val loss: 0.12508, in 0.231s\n",
      "[2058/3000] 1 tree, 31 leaves, max depth = 8, train loss: 0.04254, val loss: 0.12508, in 0.207s\n",
      "[2059/3000] 1 tree, 31 leaves, max depth = 12, train loss: 0.04251, val loss: 0.12511, in 0.206s\n",
      "[2060/3000] 1 tree, 31 leaves, max depth = 13, train loss: 0.04248, val loss: 0.12509, in 0.251s\n",
      "[2061/3000] 1 tree, 31 leaves, max depth = 15, train loss: 0.04245, val loss: 0.12509, in 0.202s\n",
      "[2062/3000] 1 tree, 31 leaves, max depth = 9, train loss: 0.04243, val loss: 0.12509, in 0.193s\n",
      "[2063/3000] 1 tree, 31 leaves, max depth = 10, train loss: 0.04241, val loss: 0.12507, in 0.219s\n",
      "[2064/3000] 1 tree, 31 leaves, max depth = 12, train loss: 0.04238, val loss: 0.12506, in 0.205s\n",
      "[2065/3000] 1 tree, 31 leaves, max depth = 14, train loss: 0.04236, val loss: 0.12506, in 0.194s\n",
      "[2066/3000] 1 tree, 31 leaves, max depth = 10, train loss: 0.04234, val loss: 0.12507, in 0.188s\n",
      "[2067/3000] 1 tree, 31 leaves, max depth = 13, train loss: 0.04231, val loss: 0.12506, in 0.192s\n",
      "[2068/3000] 1 tree, 31 leaves, max depth = 10, train loss: 0.04230, val loss: 0.12507, in 0.237s\n",
      "[2069/3000] 1 tree, 31 leaves, max depth = 14, train loss: 0.04228, val loss: 0.12506, in 0.157s\n",
      "[2070/3000] 1 tree, 31 leaves, max depth = 12, train loss: 0.04226, val loss: 0.12504, in 0.195s\n",
      "[2071/3000] 1 tree, 31 leaves, max depth = 11, train loss: 0.04223, val loss: 0.12505, in 0.204s\n",
      "[2072/3000] 1 tree, 31 leaves, max depth = 12, train loss: 0.04220, val loss: 0.12504, in 0.237s\n",
      "[2073/3000] 1 tree, 31 leaves, max depth = 12, train loss: 0.04218, val loss: 0.12503, in 0.252s\n",
      "[2074/3000] 1 tree, 31 leaves, max depth = 10, train loss: 0.04215, val loss: 0.12499, in 0.295s\n",
      "[2075/3000] 1 tree, 31 leaves, max depth = 16, train loss: 0.04213, val loss: 0.12498, in 0.346s\n",
      "[2076/3000] 1 tree, 31 leaves, max depth = 10, train loss: 0.04211, val loss: 0.12498, in 0.180s\n",
      "[2077/3000] 1 tree, 31 leaves, max depth = 10, train loss: 0.04208, val loss: 0.12498, in 0.277s\n",
      "[2078/3000] 1 tree, 31 leaves, max depth = 10, train loss: 0.04204, val loss: 0.12497, in 0.264s\n",
      "[2079/3000] 1 tree, 31 leaves, max depth = 14, train loss: 0.04202, val loss: 0.12498, in 0.232s\n",
      "[2080/3000] 1 tree, 31 leaves, max depth = 13, train loss: 0.04199, val loss: 0.12497, in 0.250s\n",
      "[2081/3000] 1 tree, 31 leaves, max depth = 9, train loss: 0.04195, val loss: 0.12494, in 0.245s\n",
      "[2082/3000] 1 tree, 31 leaves, max depth = 12, train loss: 0.04193, val loss: 0.12494, in 0.217s\n",
      "[2083/3000] 1 tree, 31 leaves, max depth = 13, train loss: 0.04190, val loss: 0.12494, in 0.218s\n",
      "[2084/3000] 1 tree, 31 leaves, max depth = 17, train loss: 0.04187, val loss: 0.12493, in 0.263s\n",
      "[2085/3000] 1 tree, 31 leaves, max depth = 9, train loss: 0.04184, val loss: 0.12491, in 0.237s\n",
      "[2086/3000] 1 tree, 31 leaves, max depth = 13, train loss: 0.04181, val loss: 0.12493, in 0.215s\n",
      "[2087/3000] 1 tree, 31 leaves, max depth = 14, train loss: 0.04179, val loss: 0.12492, in 0.181s\n",
      "[2088/3000] 1 tree, 31 leaves, max depth = 15, train loss: 0.04177, val loss: 0.12491, in 0.160s\n",
      "[2089/3000] 1 tree, 31 leaves, max depth = 11, train loss: 0.04175, val loss: 0.12491, in 0.176s\n",
      "[2090/3000] 1 tree, 31 leaves, max depth = 17, train loss: 0.04174, val loss: 0.12491, in 0.167s\n",
      "[2091/3000] 1 tree, 31 leaves, max depth = 14, train loss: 0.04173, val loss: 0.12491, in 0.180s\n",
      "[2092/3000] 1 tree, 31 leaves, max depth = 8, train loss: 0.04172, val loss: 0.12490, in 0.193s\n",
      "[2093/3000] 1 tree, 31 leaves, max depth = 12, train loss: 0.04169, val loss: 0.12492, in 0.186s\n",
      "[2094/3000] 1 tree, 31 leaves, max depth = 13, train loss: 0.04168, val loss: 0.12492, in 0.189s\n",
      "[2095/3000] 1 tree, 31 leaves, max depth = 11, train loss: 0.04165, val loss: 0.12490, in 0.216s\n",
      "[2096/3000] 1 tree, 31 leaves, max depth = 14, train loss: 0.04162, val loss: 0.12491, in 0.378s\n",
      "[2097/3000] 1 tree, 31 leaves, max depth = 13, train loss: 0.04160, val loss: 0.12492, in 0.205s\n",
      "[2098/3000] 1 tree, 31 leaves, max depth = 14, train loss: 0.04157, val loss: 0.12491, in 0.244s\n",
      "[2099/3000] 1 tree, 31 leaves, max depth = 9, train loss: 0.04155, val loss: 0.12490, in 0.296s\n",
      "[2100/3000] 1 tree, 31 leaves, max depth = 14, train loss: 0.04153, val loss: 0.12492, in 0.202s\n",
      "[2101/3000] 1 tree, 31 leaves, max depth = 10, train loss: 0.04150, val loss: 0.12490, in 0.185s\n",
      "[2102/3000] 1 tree, 31 leaves, max depth = 12, train loss: 0.04147, val loss: 0.12491, in 0.217s\n",
      "[2103/3000] 1 tree, 31 leaves, max depth = 14, train loss: 0.04144, val loss: 0.12493, in 0.185s\n",
      "[2104/3000] 1 tree, 31 leaves, max depth = 11, train loss: 0.04143, val loss: 0.12494, in 0.189s\n",
      "[2105/3000] 1 tree, 31 leaves, max depth = 17, train loss: 0.04141, val loss: 0.12496, in 0.219s\n",
      "[2106/3000] 1 tree, 31 leaves, max depth = 10, train loss: 0.04139, val loss: 0.12495, in 0.196s\n",
      "[2107/3000] 1 tree, 31 leaves, max depth = 11, train loss: 0.04136, val loss: 0.12497, in 0.213s\n",
      "[2108/3000] 1 tree, 31 leaves, max depth = 10, train loss: 0.04133, val loss: 0.12498, in 0.207s\n",
      "[2109/3000] 1 tree, 31 leaves, max depth = 11, train loss: 0.04131, val loss: 0.12499, in 0.181s\n",
      "[2110/3000] 1 tree, 31 leaves, max depth = 15, train loss: 0.04127, val loss: 0.12498, in 0.192s\n",
      "[2111/3000] 1 tree, 31 leaves, max depth = 12, train loss: 0.04125, val loss: 0.12496, in 0.257s\n",
      "[2112/3000] 1 tree, 31 leaves, max depth = 11, train loss: 0.04122, val loss: 0.12497, in 0.209s\n",
      "[2113/3000] 1 tree, 31 leaves, max depth = 13, train loss: 0.04120, val loss: 0.12497, in 0.196s\n",
      "[2114/3000] 1 tree, 31 leaves, max depth = 13, train loss: 0.04118, val loss: 0.12498, in 0.243s\n",
      "[2115/3000] 1 tree, 31 leaves, max depth = 13, train loss: 0.04117, val loss: 0.12497, in 0.219s\n",
      "[2116/3000] 1 tree, 31 leaves, max depth = 12, train loss: 0.04114, val loss: 0.12495, in 0.373s\n",
      "[2117/3000] 1 tree, 31 leaves, max depth = 11, train loss: 0.04113, val loss: 0.12495, in 0.213s\n",
      "[2118/3000] 1 tree, 31 leaves, max depth = 10, train loss: 0.04110, val loss: 0.12496, in 0.255s\n",
      "[2119/3000] 1 tree, 31 leaves, max depth = 13, train loss: 0.04107, val loss: 0.12492, in 0.237s\n",
      "[2120/3000] 1 tree, 31 leaves, max depth = 12, train loss: 0.04105, val loss: 0.12490, in 0.191s\n",
      "[2121/3000] 1 tree, 31 leaves, max depth = 10, train loss: 0.04102, val loss: 0.12490, in 0.257s\n",
      "[2122/3000] 1 tree, 31 leaves, max depth = 9, train loss: 0.04099, val loss: 0.12490, in 0.247s\n",
      "[2123/3000] 1 tree, 31 leaves, max depth = 11, train loss: 0.04097, val loss: 0.12492, in 0.212s\n",
      "[2124/3000] 1 tree, 31 leaves, max depth = 14, train loss: 0.04095, val loss: 0.12492, in 0.180s\n",
      "[2125/3000] 1 tree, 31 leaves, max depth = 11, train loss: 0.04094, val loss: 0.12492, in 0.172s\n",
      "[2126/3000] 1 tree, 31 leaves, max depth = 14, train loss: 0.04091, val loss: 0.12491, in 0.184s\n",
      "[2127/3000] 1 tree, 31 leaves, max depth = 18, train loss: 0.04090, val loss: 0.12491, in 0.187s\n",
      "[2128/3000] 1 tree, 31 leaves, max depth = 8, train loss: 0.04087, val loss: 0.12493, in 0.232s\n",
      "[2129/3000] 1 tree, 31 leaves, max depth = 10, train loss: 0.04085, val loss: 0.12491, in 0.187s\n",
      "[2130/3000] 1 tree, 31 leaves, max depth = 8, train loss: 0.04084, val loss: 0.12491, in 0.195s\n",
      "[2131/3000] 1 tree, 31 leaves, max depth = 18, train loss: 0.04081, val loss: 0.12491, in 0.228s\n",
      "[2132/3000] 1 tree, 31 leaves, max depth = 12, train loss: 0.04078, val loss: 0.12490, in 0.231s\n",
      "[2133/3000] 1 tree, 31 leaves, max depth = 12, train loss: 0.04076, val loss: 0.12491, in 0.191s\n",
      "[2134/3000] 1 tree, 31 leaves, max depth = 10, train loss: 0.04073, val loss: 0.12490, in 0.213s\n",
      "[2135/3000] 1 tree, 31 leaves, max depth = 14, train loss: 0.04071, val loss: 0.12491, in 0.201s\n",
      "[2136/3000] 1 tree, 31 leaves, max depth = 17, train loss: 0.04069, val loss: 0.12491, in 0.232s\n",
      "[2137/3000] 1 tree, 31 leaves, max depth = 8, train loss: 0.04067, val loss: 0.12491, in 0.383s\n",
      "[2138/3000] 1 tree, 31 leaves, max depth = 11, train loss: 0.04064, val loss: 0.12492, in 0.250s\n",
      "[2139/3000] 1 tree, 31 leaves, max depth = 12, train loss: 0.04062, val loss: 0.12492, in 0.213s\n",
      "[2140/3000] 1 tree, 31 leaves, max depth = 16, train loss: 0.04060, val loss: 0.12492, in 0.247s\n",
      "[2141/3000] 1 tree, 31 leaves, max depth = 10, train loss: 0.04059, val loss: 0.12491, in 0.173s\n",
      "[2142/3000] 1 tree, 31 leaves, max depth = 13, train loss: 0.04056, val loss: 0.12491, in 0.267s\n",
      "[2143/3000] 1 tree, 31 leaves, max depth = 11, train loss: 0.04054, val loss: 0.12491, in 0.181s\n",
      "[2144/3000] 1 tree, 31 leaves, max depth = 10, train loss: 0.04052, val loss: 0.12490, in 0.284s\n",
      "[2145/3000] 1 tree, 31 leaves, max depth = 15, train loss: 0.04050, val loss: 0.12488, in 0.185s\n",
      "[2146/3000] 1 tree, 31 leaves, max depth = 14, train loss: 0.04048, val loss: 0.12491, in 0.235s\n",
      "[2147/3000] 1 tree, 31 leaves, max depth = 12, train loss: 0.04046, val loss: 0.12489, in 0.215s\n",
      "[2148/3000] 1 tree, 31 leaves, max depth = 11, train loss: 0.04043, val loss: 0.12488, in 0.230s\n",
      "[2149/3000] 1 tree, 31 leaves, max depth = 14, train loss: 0.04040, val loss: 0.12488, in 0.237s\n",
      "[2150/3000] 1 tree, 31 leaves, max depth = 16, train loss: 0.04037, val loss: 0.12486, in 0.179s\n",
      "[2151/3000] 1 tree, 31 leaves, max depth = 12, train loss: 0.04035, val loss: 0.12486, in 0.196s\n",
      "[2152/3000] 1 tree, 31 leaves, max depth = 13, train loss: 0.04034, val loss: 0.12487, in 0.218s\n",
      "[2153/3000] 1 tree, 31 leaves, max depth = 11, train loss: 0.04031, val loss: 0.12487, in 0.230s\n",
      "[2154/3000] 1 tree, 31 leaves, max depth = 12, train loss: 0.04028, val loss: 0.12487, in 0.261s\n",
      "[2155/3000] 1 tree, 31 leaves, max depth = 13, train loss: 0.04027, val loss: 0.12488, in 0.222s\n",
      "[2156/3000] 1 tree, 31 leaves, max depth = 13, train loss: 0.04025, val loss: 0.12486, in 0.224s\n",
      "[2157/3000] 1 tree, 31 leaves, max depth = 16, train loss: 0.04021, val loss: 0.12484, in 0.409s\n",
      "[2158/3000] 1 tree, 31 leaves, max depth = 12, train loss: 0.04018, val loss: 0.12483, in 0.301s\n",
      "[2159/3000] 1 tree, 31 leaves, max depth = 12, train loss: 0.04015, val loss: 0.12482, in 0.303s\n",
      "[2160/3000] 1 tree, 31 leaves, max depth = 13, train loss: 0.04012, val loss: 0.12482, in 0.285s\n",
      "[2161/3000] 1 tree, 31 leaves, max depth = 12, train loss: 0.04009, val loss: 0.12482, in 0.211s\n",
      "[2162/3000] 1 tree, 31 leaves, max depth = 10, train loss: 0.04006, val loss: 0.12479, in 0.207s\n",
      "[2163/3000] 1 tree, 31 leaves, max depth = 13, train loss: 0.04004, val loss: 0.12478, in 0.208s\n",
      "[2164/3000] 1 tree, 31 leaves, max depth = 14, train loss: 0.04001, val loss: 0.12478, in 0.213s\n",
      "[2165/3000] 1 tree, 31 leaves, max depth = 10, train loss: 0.03999, val loss: 0.12481, in 0.234s\n",
      "[2166/3000] 1 tree, 31 leaves, max depth = 12, train loss: 0.03996, val loss: 0.12480, in 0.266s\n",
      "[2167/3000] 1 tree, 31 leaves, max depth = 11, train loss: 0.03993, val loss: 0.12481, in 0.187s\n",
      "[2168/3000] 1 tree, 31 leaves, max depth = 14, train loss: 0.03991, val loss: 0.12482, in 0.224s\n",
      "[2169/3000] 1 tree, 31 leaves, max depth = 10, train loss: 0.03989, val loss: 0.12483, in 0.257s\n",
      "[2170/3000] 1 tree, 31 leaves, max depth = 13, train loss: 0.03986, val loss: 0.12481, in 0.256s\n",
      "[2171/3000] 1 tree, 31 leaves, max depth = 10, train loss: 0.03984, val loss: 0.12480, in 0.204s\n",
      "[2172/3000] 1 tree, 31 leaves, max depth = 13, train loss: 0.03982, val loss: 0.12479, in 0.179s\n",
      "[2173/3000] 1 tree, 31 leaves, max depth = 12, train loss: 0.03978, val loss: 0.12478, in 0.247s\n",
      "[2174/3000] 1 tree, 31 leaves, max depth = 19, train loss: 0.03976, val loss: 0.12481, in 0.229s\n",
      "[2175/3000] 1 tree, 31 leaves, max depth = 13, train loss: 0.03975, val loss: 0.12480, in 0.249s\n",
      "[2176/3000] 1 tree, 31 leaves, max depth = 15, train loss: 0.03973, val loss: 0.12477, in 0.175s\n",
      "[2177/3000] 1 tree, 31 leaves, max depth = 11, train loss: 0.03970, val loss: 0.12476, in 0.280s\n",
      "[2178/3000] 1 tree, 31 leaves, max depth = 10, train loss: 0.03967, val loss: 0.12476, in 0.372s\n",
      "[2179/3000] 1 tree, 31 leaves, max depth = 10, train loss: 0.03965, val loss: 0.12476, in 0.271s\n",
      "[2180/3000] 1 tree, 31 leaves, max depth = 10, train loss: 0.03963, val loss: 0.12476, in 0.246s\n",
      "[2181/3000] 1 tree, 31 leaves, max depth = 10, train loss: 0.03960, val loss: 0.12476, in 0.191s\n",
      "[2182/3000] 1 tree, 31 leaves, max depth = 11, train loss: 0.03958, val loss: 0.12476, in 0.277s\n",
      "[2183/3000] 1 tree, 31 leaves, max depth = 13, train loss: 0.03955, val loss: 0.12477, in 0.250s\n",
      "[2184/3000] 1 tree, 31 leaves, max depth = 10, train loss: 0.03952, val loss: 0.12477, in 0.206s\n",
      "[2185/3000] 1 tree, 31 leaves, max depth = 11, train loss: 0.03950, val loss: 0.12479, in 0.226s\n",
      "[2186/3000] 1 tree, 31 leaves, max depth = 11, train loss: 0.03948, val loss: 0.12477, in 0.177s\n",
      "[2187/3000] 1 tree, 31 leaves, max depth = 12, train loss: 0.03946, val loss: 0.12476, in 0.237s\n",
      "[2188/3000] 1 tree, 31 leaves, max depth = 9, train loss: 0.03945, val loss: 0.12476, in 0.184s\n",
      "[2189/3000] 1 tree, 31 leaves, max depth = 15, train loss: 0.03942, val loss: 0.12478, in 0.242s\n",
      "[2190/3000] 1 tree, 31 leaves, max depth = 11, train loss: 0.03940, val loss: 0.12478, in 0.175s\n",
      "[2191/3000] 1 tree, 31 leaves, max depth = 10, train loss: 0.03939, val loss: 0.12478, in 0.164s\n",
      "[2192/3000] 1 tree, 31 leaves, max depth = 11, train loss: 0.03936, val loss: 0.12478, in 0.182s\n",
      "[2193/3000] 1 tree, 31 leaves, max depth = 8, train loss: 0.03934, val loss: 0.12477, in 0.178s\n",
      "[2194/3000] 1 tree, 31 leaves, max depth = 10, train loss: 0.03931, val loss: 0.12476, in 0.226s\n",
      "[2195/3000] 1 tree, 31 leaves, max depth = 11, train loss: 0.03929, val loss: 0.12475, in 0.200s\n",
      "[2196/3000] 1 tree, 31 leaves, max depth = 16, train loss: 0.03927, val loss: 0.12475, in 0.194s\n",
      "[2197/3000] 1 tree, 31 leaves, max depth = 12, train loss: 0.03922, val loss: 0.12469, in 0.240s\n",
      "[2198/3000] 1 tree, 31 leaves, max depth = 11, train loss: 0.03919, val loss: 0.12464, in 0.190s\n",
      "[2199/3000] 1 tree, 31 leaves, max depth = 10, train loss: 0.03917, val loss: 0.12462, in 0.359s\n",
      "[2200/3000] 1 tree, 31 leaves, max depth = 12, train loss: 0.03914, val loss: 0.12462, in 0.223s\n",
      "[2201/3000] 1 tree, 31 leaves, max depth = 10, train loss: 0.03912, val loss: 0.12463, in 0.266s\n",
      "[2202/3000] 1 tree, 31 leaves, max depth = 12, train loss: 0.03909, val loss: 0.12465, in 0.172s\n",
      "[2203/3000] 1 tree, 31 leaves, max depth = 11, train loss: 0.03906, val loss: 0.12465, in 0.284s\n",
      "[2204/3000] 1 tree, 31 leaves, max depth = 13, train loss: 0.03904, val loss: 0.12466, in 0.243s\n",
      "[2205/3000] 1 tree, 31 leaves, max depth = 12, train loss: 0.03901, val loss: 0.12466, in 0.209s\n",
      "[2206/3000] 1 tree, 31 leaves, max depth = 12, train loss: 0.03899, val loss: 0.12467, in 0.213s\n",
      "[2207/3000] 1 tree, 31 leaves, max depth = 15, train loss: 0.03896, val loss: 0.12468, in 0.224s\n",
      "[2208/3000] 1 tree, 31 leaves, max depth = 10, train loss: 0.03893, val loss: 0.12468, in 0.223s\n",
      "[2209/3000] 1 tree, 31 leaves, max depth = 8, train loss: 0.03892, val loss: 0.12468, in 0.169s\n",
      "[2210/3000] 1 tree, 31 leaves, max depth = 10, train loss: 0.03890, val loss: 0.12468, in 0.174s\n",
      "[2211/3000] 1 tree, 31 leaves, max depth = 10, train loss: 0.03888, val loss: 0.12467, in 0.196s\n",
      "[2212/3000] 1 tree, 31 leaves, max depth = 19, train loss: 0.03886, val loss: 0.12468, in 0.169s\n",
      "[2213/3000] 1 tree, 31 leaves, max depth = 14, train loss: 0.03884, val loss: 0.12469, in 0.177s\n",
      "[2214/3000] 1 tree, 31 leaves, max depth = 11, train loss: 0.03882, val loss: 0.12471, in 0.217s\n",
      "[2215/3000] 1 tree, 31 leaves, max depth = 8, train loss: 0.03881, val loss: 0.12470, in 0.197s\n",
      "[2216/3000] 1 tree, 31 leaves, max depth = 12, train loss: 0.03878, val loss: 0.12469, in 0.197s\n",
      "[2217/3000] 1 tree, 31 leaves, max depth = 11, train loss: 0.03876, val loss: 0.12470, in 0.197s\n",
      "[2218/3000] 1 tree, 31 leaves, max depth = 13, train loss: 0.03875, val loss: 0.12470, in 0.188s\n",
      "[2219/3000] 1 tree, 31 leaves, max depth = 11, train loss: 0.03873, val loss: 0.12471, in 0.213s\n",
      "[2220/3000] 1 tree, 31 leaves, max depth = 11, train loss: 0.03871, val loss: 0.12472, in 0.316s\n",
      "[2221/3000] 1 tree, 31 leaves, max depth = 13, train loss: 0.03868, val loss: 0.12472, in 0.226s\n",
      "[2222/3000] 1 tree, 31 leaves, max depth = 13, train loss: 0.03865, val loss: 0.12470, in 0.292s\n",
      "[2223/3000] 1 tree, 31 leaves, max depth = 10, train loss: 0.03863, val loss: 0.12470, in 0.261s\n",
      "[2224/3000] 1 tree, 31 leaves, max depth = 15, train loss: 0.03860, val loss: 0.12471, in 0.258s\n",
      "[2225/3000] 1 tree, 31 leaves, max depth = 11, train loss: 0.03856, val loss: 0.12468, in 0.293s\n",
      "[2226/3000] 1 tree, 31 leaves, max depth = 11, train loss: 0.03853, val loss: 0.12469, in 0.223s\n",
      "[2227/3000] 1 tree, 31 leaves, max depth = 10, train loss: 0.03851, val loss: 0.12470, in 0.213s\n",
      "[2228/3000] 1 tree, 31 leaves, max depth = 11, train loss: 0.03849, val loss: 0.12470, in 0.218s\n",
      "[2229/3000] 1 tree, 31 leaves, max depth = 12, train loss: 0.03847, val loss: 0.12469, in 0.179s\n",
      "[2230/3000] 1 tree, 31 leaves, max depth = 11, train loss: 0.03846, val loss: 0.12469, in 0.201s\n",
      "[2231/3000] 1 tree, 31 leaves, max depth = 10, train loss: 0.03844, val loss: 0.12469, in 0.170s\n",
      "[2232/3000] 1 tree, 31 leaves, max depth = 13, train loss: 0.03841, val loss: 0.12470, in 0.235s\n",
      "[2233/3000] 1 tree, 31 leaves, max depth = 14, train loss: 0.03839, val loss: 0.12472, in 0.169s\n",
      "[2234/3000] 1 tree, 31 leaves, max depth = 12, train loss: 0.03837, val loss: 0.12472, in 0.227s\n",
      "[2235/3000] 1 tree, 31 leaves, max depth = 12, train loss: 0.03835, val loss: 0.12473, in 0.199s\n",
      "[2236/3000] 1 tree, 31 leaves, max depth = 12, train loss: 0.03832, val loss: 0.12473, in 0.214s\n",
      "[2237/3000] 1 tree, 31 leaves, max depth = 8, train loss: 0.03830, val loss: 0.12473, in 0.187s\n",
      "[2238/3000] 1 tree, 31 leaves, max depth = 11, train loss: 0.03827, val loss: 0.12470, in 0.193s\n",
      "[2239/3000] 1 tree, 31 leaves, max depth = 10, train loss: 0.03825, val loss: 0.12468, in 0.190s\n",
      "[2240/3000] 1 tree, 31 leaves, max depth = 17, train loss: 0.03824, val loss: 0.12469, in 0.175s\n",
      "[2241/3000] 1 tree, 31 leaves, max depth = 10, train loss: 0.03822, val loss: 0.12470, in 0.333s\n",
      "[2242/3000] 1 tree, 31 leaves, max depth = 9, train loss: 0.03819, val loss: 0.12468, in 0.217s\n",
      "[2243/3000] 1 tree, 31 leaves, max depth = 11, train loss: 0.03818, val loss: 0.12468, in 0.212s\n",
      "[2244/3000] 1 tree, 31 leaves, max depth = 10, train loss: 0.03817, val loss: 0.12470, in 0.155s\n",
      "[2245/3000] 1 tree, 31 leaves, max depth = 8, train loss: 0.03815, val loss: 0.12471, in 0.205s\n",
      "[2246/3000] 1 tree, 31 leaves, max depth = 8, train loss: 0.03814, val loss: 0.12469, in 0.260s\n",
      "[2247/3000] 1 tree, 31 leaves, max depth = 10, train loss: 0.03811, val loss: 0.12472, in 0.234s\n",
      "[2248/3000] 1 tree, 31 leaves, max depth = 11, train loss: 0.03807, val loss: 0.12470, in 0.261s\n",
      "[2249/3000] 1 tree, 31 leaves, max depth = 10, train loss: 0.03804, val loss: 0.12470, in 0.244s\n",
      "[2250/3000] 1 tree, 31 leaves, max depth = 11, train loss: 0.03802, val loss: 0.12470, in 0.205s\n",
      "[2251/3000] 1 tree, 31 leaves, max depth = 10, train loss: 0.03801, val loss: 0.12470, in 0.205s\n",
      "[2252/3000] 1 tree, 31 leaves, max depth = 15, train loss: 0.03800, val loss: 0.12470, in 0.181s\n",
      "[2253/3000] 1 tree, 31 leaves, max depth = 12, train loss: 0.03797, val loss: 0.12468, in 0.279s\n",
      "[2254/3000] 1 tree, 31 leaves, max depth = 16, train loss: 0.03794, val loss: 0.12467, in 0.218s\n",
      "[2255/3000] 1 tree, 31 leaves, max depth = 12, train loss: 0.03792, val loss: 0.12467, in 0.183s\n",
      "[2256/3000] 1 tree, 31 leaves, max depth = 10, train loss: 0.03790, val loss: 0.12468, in 0.218s\n",
      "[2257/3000] 1 tree, 31 leaves, max depth = 12, train loss: 0.03787, val loss: 0.12467, in 0.208s\n",
      "[2258/3000] 1 tree, 31 leaves, max depth = 13, train loss: 0.03786, val loss: 0.12469, in 0.192s\n",
      "[2259/3000] 1 tree, 31 leaves, max depth = 13, train loss: 0.03784, val loss: 0.12469, in 0.177s\n",
      "[2260/3000] 1 tree, 31 leaves, max depth = 13, train loss: 0.03782, val loss: 0.12468, in 0.175s\n",
      "[2261/3000] 1 tree, 31 leaves, max depth = 11, train loss: 0.03779, val loss: 0.12468, in 0.208s\n",
      "[2262/3000] 1 tree, 31 leaves, max depth = 9, train loss: 0.03777, val loss: 0.12468, in 0.362s\n",
      "[2263/3000] 1 tree, 31 leaves, max depth = 13, train loss: 0.03775, val loss: 0.12467, in 0.224s\n",
      "[2264/3000] 1 tree, 31 leaves, max depth = 11, train loss: 0.03772, val loss: 0.12466, in 0.189s\n",
      "[2265/3000] 1 tree, 31 leaves, max depth = 9, train loss: 0.03770, val loss: 0.12465, in 0.182s\n",
      "[2266/3000] 1 tree, 31 leaves, max depth = 12, train loss: 0.03769, val loss: 0.12465, in 0.183s\n",
      "[2267/3000] 1 tree, 31 leaves, max depth = 9, train loss: 0.03768, val loss: 0.12464, in 0.194s\n",
      "[2268/3000] 1 tree, 31 leaves, max depth = 9, train loss: 0.03765, val loss: 0.12466, in 0.222s\n",
      "[2269/3000] 1 tree, 31 leaves, max depth = 13, train loss: 0.03762, val loss: 0.12466, in 0.226s\n",
      "[2270/3000] 1 tree, 31 leaves, max depth = 9, train loss: 0.03760, val loss: 0.12466, in 0.199s\n",
      "[2271/3000] 1 tree, 31 leaves, max depth = 10, train loss: 0.03757, val loss: 0.12467, in 0.232s\n",
      "[2272/3000] 1 tree, 31 leaves, max depth = 11, train loss: 0.03755, val loss: 0.12467, in 0.204s\n",
      "[2273/3000] 1 tree, 31 leaves, max depth = 12, train loss: 0.03753, val loss: 0.12468, in 0.160s\n",
      "[2274/3000] 1 tree, 31 leaves, max depth = 11, train loss: 0.03752, val loss: 0.12468, in 0.175s\n",
      "[2275/3000] 1 tree, 31 leaves, max depth = 13, train loss: 0.03751, val loss: 0.12468, in 0.177s\n",
      "[2276/3000] 1 tree, 31 leaves, max depth = 11, train loss: 0.03749, val loss: 0.12469, in 0.217s\n",
      "[2277/3000] 1 tree, 31 leaves, max depth = 9, train loss: 0.03746, val loss: 0.12469, in 0.226s\n",
      "[2278/3000] 1 tree, 31 leaves, max depth = 12, train loss: 0.03744, val loss: 0.12468, in 0.237s\n",
      "[2279/3000] 1 tree, 31 leaves, max depth = 10, train loss: 0.03743, val loss: 0.12467, in 0.190s\n",
      "[2280/3000] 1 tree, 31 leaves, max depth = 14, train loss: 0.03742, val loss: 0.12468, in 0.179s\n",
      "[2281/3000] 1 tree, 31 leaves, max depth = 10, train loss: 0.03738, val loss: 0.12468, in 0.255s\n",
      "[2282/3000] 1 tree, 31 leaves, max depth = 10, train loss: 0.03736, val loss: 0.12469, in 0.197s\n",
      "[2283/3000] 1 tree, 31 leaves, max depth = 11, train loss: 0.03734, val loss: 0.12469, in 0.355s\n",
      "[2284/3000] 1 tree, 31 leaves, max depth = 10, train loss: 0.03732, val loss: 0.12469, in 0.291s\n",
      "[2285/3000] 1 tree, 31 leaves, max depth = 12, train loss: 0.03730, val loss: 0.12468, in 0.271s\n",
      "[2286/3000] 1 tree, 31 leaves, max depth = 11, train loss: 0.03728, val loss: 0.12468, in 0.220s\n",
      "[2287/3000] 1 tree, 31 leaves, max depth = 11, train loss: 0.03725, val loss: 0.12470, in 0.231s\n",
      "[2288/3000] 1 tree, 31 leaves, max depth = 13, train loss: 0.03723, val loss: 0.12470, in 0.204s\n",
      "[2289/3000] 1 tree, 31 leaves, max depth = 10, train loss: 0.03721, val loss: 0.12470, in 0.203s\n",
      "[2290/3000] 1 tree, 31 leaves, max depth = 14, train loss: 0.03718, val loss: 0.12470, in 0.232s\n",
      "[2291/3000] 1 tree, 31 leaves, max depth = 12, train loss: 0.03716, val loss: 0.12470, in 0.192s\n",
      "[2292/3000] 1 tree, 31 leaves, max depth = 12, train loss: 0.03714, val loss: 0.12470, in 0.202s\n",
      "[2293/3000] 1 tree, 31 leaves, max depth = 10, train loss: 0.03711, val loss: 0.12471, in 0.231s\n",
      "[2294/3000] 1 tree, 31 leaves, max depth = 10, train loss: 0.03709, val loss: 0.12469, in 0.195s\n",
      "[2295/3000] 1 tree, 31 leaves, max depth = 12, train loss: 0.03706, val loss: 0.12468, in 0.230s\n",
      "[2296/3000] 1 tree, 31 leaves, max depth = 9, train loss: 0.03705, val loss: 0.12467, in 0.202s\n",
      "[2297/3000] 1 tree, 31 leaves, max depth = 12, train loss: 0.03701, val loss: 0.12464, in 0.208s\n",
      "[2298/3000] 1 tree, 31 leaves, max depth = 12, train loss: 0.03699, val loss: 0.12463, in 0.244s\n",
      "[2299/3000] 1 tree, 31 leaves, max depth = 11, train loss: 0.03697, val loss: 0.12465, in 0.211s\n",
      "Fit 2299 trees in 546.227 s, (71269 total leaves)\n",
      "Time spent computing histograms: 244.990s\n",
      "Time spent finding best splits:  23.036s\n",
      "Time spent applying splits:      27.625s\n",
      "Time spent predicting:           0.943s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "HistGradientBoostingClassifier(max_iter=3000, min_samples_leaf=50,\n",
       "                               n_iter_no_change=100, verbose=1)"
      ]
     },
     "execution_count": 123,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "histgbc = ensemble.HistGradientBoostingClassifier(max_iter=3000, learning_rate=0.1, n_iter_no_change=100,\n",
    "                                                  min_samples_leaf=50, \n",
    "                                                  warm_start=False, verbose=1)\n",
    "histgbc.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true,
     "source_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing results:\n",
      "accuracy = 0.9926413289010327\n",
      "adjusted balanced accuracy = 0.9852427800660792\n",
      "balanced accuracy = 0.9926213900330396\n",
      "average precision = 0.9905390910886096\n",
      "f1 score = 0.9932822649393064\n",
      "precision = 0.9937356604884143\n",
      "recall = 0.9928292829282929\n",
      "roc = 0.9926213900330395\n",
      "confusion matrix\n",
      " = [[136438   1043]\n",
      " [  1195 165455]]\n",
      "Testing results:\n",
      "accuracy = 0.948006983694848\n",
      "adjusted balanced accuracy = 0.8958615430887291\n",
      "balanced accuracy = 0.9479307715443646\n",
      "average precision = 0.9349724552981625\n",
      "f1 score = 0.9523163351154775\n",
      "precision = 0.9559224147324834\n",
      "recall = 0.9487373600821932\n",
      "roc = 0.9479307715443647\n",
      "confusion matrix\n",
      " = [[14491   809]\n",
      " [  948 17545]]\n"
     ]
    }
   ],
   "source": [
    "evaluate_classifier(histgbc, X_train, y_train)\n",
    "evaluate_classifier(histgbc, X_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true,
     "source_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 3 folds for each of 10 candidates, totalling 30 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done   2 tasks      | elapsed:   52.7s\n",
      "[Parallel(n_jobs=-1)]: Done  22 out of  30 | elapsed: 22.9min remaining:  8.3min\n",
      "[Parallel(n_jobs=-1)]: Done  30 out of  30 | elapsed: 34.6min finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RF grid search saved\n"
     ]
    }
   ],
   "source": [
    "rf_search = model_selection.RandomizedSearchCV(\n",
    "    ensemble.RandomForestClassifier(), \n",
    "    param_distributions={\n",
    "        'n_estimators': [10, 100, 300],\n",
    "        'criterion': ['gini', 'entropy'],\n",
    "        'class_weight': [{True: 2, False: 1}, {True: 3, False: 1}, 'balanced', None],\n",
    "        'max_depth': [None],\n",
    "        'min_samples_leaf': [20, 50],\n",
    "        'min_samples_split': [20, 50]\n",
    "    },\n",
    "    n_jobs=-1,\n",
    "    cv=3,\n",
    "    refit='f1',\n",
    "    scoring=['neg_log_loss', 'f1', 'roc_auc'],\n",
    "    verbose=5)\n",
    "rf_search.fit(X_train, y_train)\n",
    "\n",
    "with open(f'./model_gridsearch/{loss_type}/rf', 'wb') as f:\n",
    "    pickle.dump(rf_search, f)\n",
    "    print('RF grid search saved')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true,
     "source_hidden": true
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>mean_fit_time</th>\n",
       "      <th>mean_score_time</th>\n",
       "      <th>params</th>\n",
       "      <th>mean_test_neg_log_loss</th>\n",
       "      <th>rank_test_neg_log_loss</th>\n",
       "      <th>mean_test_f1</th>\n",
       "      <th>rank_test_f1</th>\n",
       "      <th>mean_test_roc_auc</th>\n",
       "      <th>rank_test_roc_auc</th>\n",
       "      <th>mean_test_balanced_accuracy</th>\n",
       "      <th>rank_test_balanced_accuracy</th>\n",
       "      <th>mean_test_accuracy</th>\n",
       "      <th>rank_test_accuracy</th>\n",
       "      <th>mean_test_recall</th>\n",
       "      <th>rank_test_recall</th>\n",
       "      <th>mean_test_precision</th>\n",
       "      <th>rank_test_precision</th>\n",
       "      <th>overall_rank</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>86</td>\n",
       "      <td>957.180677</td>\n",
       "      <td>37.894858</td>\n",
       "      <td>{'class_weight': None, 'criterion': 'entropy', 'max_depth': None, 'min_samples_leaf': 20, 'min_samples_split': 20, 'n_estimators': 300}</td>\n",
       "      <td>-0.256520</td>\n",
       "      <td>1</td>\n",
       "      <td>0.921146</td>\n",
       "      <td>1</td>\n",
       "      <td>0.972490</td>\n",
       "      <td>8</td>\n",
       "      <td>0.913810</td>\n",
       "      <td>5</td>\n",
       "      <td>0.914067</td>\n",
       "      <td>1</td>\n",
       "      <td>0.916504</td>\n",
       "      <td>49</td>\n",
       "      <td>0.925837</td>\n",
       "      <td>21</td>\n",
       "      <td>3.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>62</td>\n",
       "      <td>959.992151</td>\n",
       "      <td>37.904519</td>\n",
       "      <td>{'class_weight': 'balanced', 'criterion': 'entropy', 'max_depth': None, 'min_samples_leaf': 20, 'min_samples_split': 20, 'n_estimators': 300}</td>\n",
       "      <td>-0.258597</td>\n",
       "      <td>4</td>\n",
       "      <td>0.919503</td>\n",
       "      <td>6</td>\n",
       "      <td>0.972170</td>\n",
       "      <td>11</td>\n",
       "      <td>0.914732</td>\n",
       "      <td>1</td>\n",
       "      <td>0.913514</td>\n",
       "      <td>4</td>\n",
       "      <td>0.901950</td>\n",
       "      <td>69</td>\n",
       "      <td>0.937753</td>\n",
       "      <td>3</td>\n",
       "      <td>3.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>85</td>\n",
       "      <td>328.981438</td>\n",
       "      <td>13.230439</td>\n",
       "      <td>{'class_weight': None, 'criterion': 'entropy', 'max_depth': None, 'min_samples_leaf': 20, 'min_samples_split': 20, 'n_estimators': 100}</td>\n",
       "      <td>-0.257364</td>\n",
       "      <td>2</td>\n",
       "      <td>0.920973</td>\n",
       "      <td>2</td>\n",
       "      <td>0.972162</td>\n",
       "      <td>12</td>\n",
       "      <td>0.913637</td>\n",
       "      <td>6</td>\n",
       "      <td>0.913886</td>\n",
       "      <td>2</td>\n",
       "      <td>0.916251</td>\n",
       "      <td>51</td>\n",
       "      <td>0.925745</td>\n",
       "      <td>22</td>\n",
       "      <td>4.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    mean_fit_time  mean_score_time  \\\n",
       "86  957.180677     37.894858         \n",
       "62  959.992151     37.904519         \n",
       "85  328.981438     13.230439         \n",
       "\n",
       "                                                                                                                                           params  \\\n",
       "86  {'class_weight': None, 'criterion': 'entropy', 'max_depth': None, 'min_samples_leaf': 20, 'min_samples_split': 20, 'n_estimators': 300}         \n",
       "62  {'class_weight': 'balanced', 'criterion': 'entropy', 'max_depth': None, 'min_samples_leaf': 20, 'min_samples_split': 20, 'n_estimators': 300}   \n",
       "85  {'class_weight': None, 'criterion': 'entropy', 'max_depth': None, 'min_samples_leaf': 20, 'min_samples_split': 20, 'n_estimators': 100}         \n",
       "\n",
       "    mean_test_neg_log_loss  rank_test_neg_log_loss  mean_test_f1  \\\n",
       "86 -0.256520                1                       0.921146       \n",
       "62 -0.258597                4                       0.919503       \n",
       "85 -0.257364                2                       0.920973       \n",
       "\n",
       "    rank_test_f1  mean_test_roc_auc  rank_test_roc_auc  \\\n",
       "86  1             0.972490           8                   \n",
       "62  6             0.972170           11                  \n",
       "85  2             0.972162           12                  \n",
       "\n",
       "    mean_test_balanced_accuracy  rank_test_balanced_accuracy  \\\n",
       "86  0.913810                     5                             \n",
       "62  0.914732                     1                             \n",
       "85  0.913637                     6                             \n",
       "\n",
       "    mean_test_accuracy  rank_test_accuracy  mean_test_recall  \\\n",
       "86  0.914067            1                   0.916504           \n",
       "62  0.913514            4                   0.901950           \n",
       "85  0.913886            2                   0.916251           \n",
       "\n",
       "    rank_test_recall  mean_test_precision  rank_test_precision  overall_rank  \n",
       "86  49                0.925837             21                   3.0           \n",
       "62  69                0.937753             3                    3.5           \n",
       "85  51                0.925745             22                   4.0           "
      ]
     },
     "execution_count": 114,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "with open(f'./model_gridsearch/{loss_type}/rf', 'rb') as f:\n",
    "    rf_search = pickle.load(f)\n",
    "get_search_result(rf_search.cv_results_).iloc[:3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true,
     "source_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "[Parallel(n_jobs=1)]: Done 100 out of 100 | elapsed:  3.9min finished\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "RandomForestClassifier(criterion='entropy', min_samples_leaf=20,\n",
       "                       min_samples_split=20, verbose=1)"
      ]
     },
     "execution_count": 126,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rfc = ensemble.RandomForestClassifier(n_estimators=100, criterion='entropy', min_samples_split=20, min_samples_leaf=20,\n",
    "                                      class_weight=None, max_depth=None,\n",
    "                                      verbose=1)\n",
    "rfc.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true,
     "source_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing results:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "[Parallel(n_jobs=1)]: Done 100 out of 100 | elapsed:    7.9s finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy = 0.9416896008627861\n",
      "adjusted balanced accuracy = 0.8837886054082094\n",
      "balanced accuracy = 0.9418943027041047\n",
      "average precision = 0.9287566725707416\n",
      "f1 score = 0.9464158377548678\n",
      "precision = 0.9531666524655217\n",
      "recall = 0.9397599759975998\n",
      "roc = 0.9418943027041046\n",
      "confusion matrix\n",
      " = [[129786   7695]\n",
      " [ 10039 156611]]\n",
      "Testing results:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy = 0.9175272985529548\n",
      "adjusted balanced accuracy = 0.834601306482686\n",
      "balanced accuracy = 0.917300653241343\n",
      "average precision = 0.898244747826533\n",
      "f1 score = 0.924272477787137\n",
      "precision = 0.9288913162206445\n",
      "recall = 0.9196993456983723\n",
      "roc = 0.917300653241343\n",
      "confusion matrix\n",
      " = [[13998  1302]\n",
      " [ 1485 17008]]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Done 100 out of 100 | elapsed:    0.8s finished\n"
     ]
    }
   ],
   "source": [
    "evaluate_classifier(rfc, X_train, y_train)\n",
    "evaluate_classifier(rfc, X_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Generated Chemical Space Via Topological Indices & Predictors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make predictions based on predictor\n",
    "def predict_feasible_ids(clf, features):\n",
    "    '''\n",
    "    Outputs a list of ids for feasbile molecules in `features`\n",
    "    Input: \n",
    "        clf: classifier, giving out 1 / 0 for DG failure / not failure\n",
    "        data: dataframe with computed topological indices\n",
    "    '''\n",
    "    # Drop the indices decided upon during data processing    \n",
    "    X = features_scaler.transform(features[indices].values)\n",
    "    feasible = clf.predict(X)\n",
    "    print(np.count_nonzero(feasible), '/', len(feasible))\n",
    "    \n",
    "    return np.argwhere(feasible)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [],
   "source": [
    "def select_molecules_by(clf, database, actuals=None, size=5*6):\n",
    "    '''\n",
    "    Return a selection of chosen molecules as deemed feasible by `clf` in `database`.\n",
    "    Input:\n",
    "        clf: classifier, giving 1/0 output for DG failure / success\n",
    "        database: dictionary with entry\n",
    "                    `features`, for dataframe with computed topological indices \n",
    "                    `mols`, an RDKit molecule feeder object\n",
    "        actuals: actual feasibility values\n",
    "        size: number of molecules to select\n",
    "    '''\n",
    "    # select feasible molecules\n",
    "    t0 = time.time()\n",
    "    feasible_ids = predict_feasible_ids(clf, database['features']).flatten()\n",
    "    t1 = time.time()\n",
    "    print(f'Feasbile molecules chosen. Time taken = {t1-t0}')\n",
    "    \n",
    "    # actual 'correct' answers are supplied\n",
    "    y_pred = np.zeros(len(database['features'].index))\n",
    "    y_pred[feasible_ids] = 1\n",
    "    if actuals is not None:\n",
    "        for name, metric in classification_metrics.items():\n",
    "            loss = metric(y_pred, actuals)\n",
    "            print(f'{name} = {loss}')\n",
    "    \n",
    "    # Choose some random molecules and draw them\n",
    "    ids_selection = np.random.choice(feasible_ids, size=size, replace=False)\n",
    "    selected_molecules = [database['mols'][int(i)] for i in ids_selection]\n",
    "    \n",
    "    return selected_molecules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [],
   "source": [
    "nn_energy = nn_energy_search.best_estimator_\n",
    "hgb_energy = hgb_energy_search.best_estimator_\n",
    "\n",
    "nn_angle = nn_angle_search.best_estimator_\n",
    "hgb_angle = hgb_angle_search.best_estimator_\n",
    "\n",
    "nn_length = nn_length_search.best_estimator_\n",
    "hgb_length = hgb_length_search.best_estimator_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9988 / 56225\n",
      "Feasbile molecules chosen. Time taken = 0.6352953910827637\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<PIL.PngImagePlugin.PngImageFile image mode=RGB size=600x400 at 0x1D5658C8278>"
      ]
     },
     "execution_count": 99,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Molecules chosen by NN\n",
    "selected_molecules = select_molecules_by(nn_energy, chemspace['125_56k'], size=3*2)\n",
    "img = Draw.MolsToGridImage(\n",
    "        selected_molecules, \n",
    "        molsPerRow=3, \n",
    "        subImgSize=(200,200),\n",
    "        legends=[Chem.MolToSmiles(mol) for mol in selected_molecules])\n",
    "#img.save('./figures/nn_dg_mols_by_energy.pdf')\n",
    "img"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "50 / 216\n",
      "Feasbile molecules chosen. Time taken = 0.00600123405456543\n",
      "accuracy = 0.9537037037037037\n",
      "adjusted balanced accuracy = 0.8838554216867469\n",
      "balanced accuracy = 0.9419277108433735\n",
      "average precision = 0.8323646723646724\n",
      "f1 score = 0.9019607843137256\n",
      "precision = 0.8846153846153846\n",
      "recall = 0.92\n",
      "roc = 0.9419277108433735\n",
      "confusion matrix\n",
      " = [[160   6]\n",
      " [  4  46]]\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<PIL.PngImagePlugin.PngImageFile image mode=RGB size=1200x1800 at 0x1D566342BA8>"
      ]
     },
     "execution_count": 115,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Molecules chosen by NN\n",
    "selected_molecules = select_molecules_by(nn_energy, chemspace['c6h6'], actuals=y_c6h6['energy_loss'], size=50)\n",
    "img = Draw.MolsToGridImage(\n",
    "        selected_molecules, \n",
    "        molsPerRow=6,\n",
    "        subImgSize=(200,200),\n",
    "        legends=[Chem.MolToSmiles(mol) for mol in selected_molecules])\n",
    "img.save('./figures/c6h6_nn_energy.pdf')\n",
    "img"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "51 / 216\n",
      "Feasbile molecules chosen. Time taken = 0.0769956111907959\n",
      "accuracy = 0.9398148148148148\n",
      "adjusted balanced accuracy = 0.8399286987522281\n",
      "balanced accuracy = 0.919964349376114\n",
      "average precision = 0.791352438411262\n",
      "f1 score = 0.8737864077669903\n",
      "precision = 0.8653846153846154\n",
      "recall = 0.8823529411764706\n",
      "roc = 0.9199643493761139\n",
      "confusion matrix\n",
      " = [[158   7]\n",
      " [  6  45]]\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<PIL.PngImagePlugin.PngImageFile image mode=RGB size=1200x1800 at 0x1D5627D24A8>"
      ]
     },
     "execution_count": 116,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "###### Molecules chosen by NN\n",
    "selected_molecules = select_molecules_by(hgb_energy, chemspace['c6h6'], actuals=y_c6h6['energy_loss'], size=51)\n",
    "img = Draw.MolsToGridImage(\n",
    "        selected_molecules, \n",
    "        molsPerRow=6,\n",
    "        subImgSize=(200,200),\n",
    "        legends=[Chem.MolToSmiles(mol) for mol in selected_molecules])\n",
    "img.save('./figures/c6h6_hgb_energy.pdf')\n",
    "img"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Taking length_loss of >= 0.011 as 'impossible'\n",
      "Unconstructible molecules:\n",
      "66 / 216\n",
      "length_loss targets successfully computed. \n",
      "\n",
      "Taking angle_loss of >= 1.53 as 'impossible'\n",
      "Unconstructible molecules:\n",
      "199 / 216\n",
      "angle_loss targets successfully computed. \n",
      "\n",
      "Taking energy_loss of >= 10.557 as 'impossible'\n",
      "Unconstructible molecules:\n",
      "164 / 216\n",
      "energy_loss targets successfully computed. \n",
      "\n",
      "Features successfully computed \n",
      "\n"
     ]
    }
   ],
   "source": [
    "X_c6h6, y_c6h6 = prepare_training_data('c6h6', features_scaler=features_scaler)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing results:\n",
      "accuracy = 0.9537037037037037\n",
      "adjusted balanced accuracy = 0.8602251407129455\n",
      "balanced accuracy = 0.9301125703564728\n",
      "average precision = 0.8416239316239317\n",
      "f1 score = 0.9019607843137256\n",
      "precision = 0.92\n",
      "recall = 0.8846153846153846\n",
      "roc = 0.9301125703564728\n",
      "confusion matrix\n",
      " = [[160   4]\n",
      " [  6  46]]\n",
      "Testing results:\n",
      "accuracy = 0.9398148148148148\n",
      "adjusted balanced accuracy = 0.8287992495309568\n",
      "balanced accuracy = 0.9143996247654784\n",
      "average precision = 0.7959820680408917\n",
      "f1 score = 0.8737864077669903\n",
      "precision = 0.8823529411764706\n",
      "recall = 0.8653846153846154\n",
      "roc = 0.9143996247654784\n",
      "confusion matrix\n",
      " = [[158   6]\n",
      " [  7  45]]\n"
     ]
    }
   ],
   "source": [
    "evaluate_classifier(nn_energy, X_c6h6, y_c6h6['energy_loss'])\n",
    "evaluate_classifier(hgb_energy, X_c6h6, y_c6h6['energy_loss'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing results:\n",
      "accuracy = 0.9398148148148148\n",
      "adjusted balanced accuracy = 0.45048773278155485\n",
      "balanced accuracy = 0.7252438663907774\n",
      "average precision = 0.3553921568627451\n",
      "f1 score = 0.5517241379310345\n",
      "precision = 0.6666666666666666\n",
      "recall = 0.47058823529411764\n",
      "roc = 0.7252438663907774\n",
      "confusion matrix\n",
      " = [[195   4]\n",
      " [  9   8]]\n",
      "Testing results:\n",
      "accuracy = 0.9537037037037037\n",
      "adjusted balanced accuracy = 0.4117647058823528\n",
      "balanced accuracy = 0.7058823529411764\n",
      "average precision = 0.4580610021786492\n",
      "f1 score = 0.5833333333333334\n",
      "precision = 1.0\n",
      "recall = 0.4117647058823529\n",
      "roc = 0.7058823529411764\n",
      "confusion matrix\n",
      " = [[199   0]\n",
      " [ 10   7]]\n"
     ]
    }
   ],
   "source": [
    "evaluate_classifier(nn_angle, X_c6h6, y_c6h6['angle_loss'])\n",
    "evaluate_classifier(hgb_angle, X_c6h6, y_c6h6['angle_loss'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing results:\n",
      "accuracy = 0.9722222222222222\n",
      "adjusted balanced accuracy = 0.9090909090909092\n",
      "balanced accuracy = 0.9545454545454546\n",
      "average precision = 0.9615384615384616\n",
      "f1 score = 0.9803921568627451\n",
      "precision = 0.9615384615384616\n",
      "recall = 1.0\n",
      "roc = 0.9545454545454545\n",
      "confusion matrix\n",
      " = [[ 60   6]\n",
      " [  0 150]]\n",
      "Testing results:\n",
      "accuracy = 0.9675925925925926\n",
      "adjusted balanced accuracy = 0.9024242424242424\n",
      "balanced accuracy = 0.9512121212121212\n",
      "average precision = 0.9595113500597372\n",
      "f1 score = 0.9770491803278689\n",
      "precision = 0.9612903225806452\n",
      "recall = 0.9933333333333333\n",
      "roc = 0.9512121212121212\n",
      "confusion matrix\n",
      " = [[ 60   6]\n",
      " [  1 149]]\n"
     ]
    }
   ],
   "source": [
    "evaluate_classifier(nn_length, X_c6h6, y_c6h6['length_loss'])\n",
    "evaluate_classifier(hgb_length, X_c6h6, y_c6h6['length_loss'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
